<!DOCTYPE html>
<html>
<head>
<title>Paper collected by Wang</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css">


/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 20px;
  max-width: 960px;
  margin: 0 auto;
}

span#pid {
  color:red;
  
}
span#filename{
  font-style: oblique;
}

span#title{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: italic;
  font-size: 20px;
  border:1px solid #B50;
}
span#abs{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: oblique;
  font-size: 18px;
}
</style>
</head>
<body>

</p></br></br><div id='section'>Paperid: <span id='pid'>1, <a href='https://arxiv.org/pdf/2509.14619.pdf' target='_blank'>https://arxiv.org/pdf/2509.14619.pdf</a></span>   <span><a href='https://github.com/xiaobaoxia/LSTC-MDA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Feng Ding, Haisheng Fu, Soroush Oraki, Jie Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14619">LSTC-MDA: A Unified Framework for Long-Short Term Temporal Convolution and Mixed Data Augmentation in Skeleton-Based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeleton-based action recognition faces two longstanding challenges: the scarcity of labeled training samples and difficulty modeling short- and long-range temporal dependencies. To address these issues, we propose a unified framework, LSTC-MDA, which simultaneously improves temporal modeling and data diversity. We introduce a novel Long-Short Term Temporal Convolution (LSTC) module with parallel short- and long-term branches, these two feature branches are then aligned and fused adaptively using learned similarity weights to preserve critical long-range cues lost by conventional stride-2 temporal convolutions. We also extend Joint Mixing Data Augmentation (JMDA) with an Additive Mixup at the input level, diversifying training samples and restricting mixup operations to the same camera view to avoid distribution shifts. Ablation studies confirm each component contributes. LSTC-MDA achieves state-of-the-art results: 94.1% and 97.5% on NTU 60 (X-Sub and X-View), 90.4% and 92.0% on NTU 120 (X-Sub and X-Set),97.2% on NW-UCLA. Code: https://github.com/xiaobaoxia/LSTC-MDA.
<div id='section'>Paperid: <span id='pid'>2, <a href='https://arxiv.org/pdf/2508.12586.pdf' target='_blank'>https://arxiv.org/pdf/2508.12586.pdf</a></span>   <span><a href='https://github.com/wengwanjiang/FoundSkelModel' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongsong Wang, Wanjiang Weng, Junbo Wang, Fang Zhao, Guo-Sen Xie, Xin Geng, Liang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12586">Foundation Model for Skeleton-Based Human Action Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human action understanding serves as a foundational pillar in the field of intelligent motion perception. Skeletons serve as a modality- and device-agnostic representation for human modeling, and skeleton-based action understanding has potential applications in humanoid robot control and interaction. \RED{However, existing works often lack the scalability and generalization required to handle diverse action understanding tasks. There is no skeleton foundation model that can be adapted to a wide range of action understanding tasks}. This paper presents a Unified Skeleton-based Dense Representation Learning (USDRL) framework, which serves as a foundational model for skeleton-based human action understanding. USDRL consists of a Transformer-based Dense Spatio-Temporal Encoder (DSTE), Multi-Grained Feature Decorrelation (MG-FD), and Multi-Perspective Consistency Training (MPCT). The DSTE module adopts two parallel streams to learn temporal dynamic and spatial structure features. The MG-FD module collaboratively performs feature decorrelation across temporal, spatial, and instance domains to reduce dimensional redundancy and enhance information extraction. The MPCT module employs both multi-view and multi-modal self-supervised consistency training. The former enhances the learning of high-level semantics and mitigates the impact of low-level discrepancies, while the latter effectively facilitates the learning of informative multimodal features. We perform extensive experiments on 25 benchmarks across across 9 skeleton-based action understanding tasks, covering coarse prediction, dense prediction, and transferred prediction. Our approach significantly outperforms the current state-of-the-art methods. We hope that this work would broaden the scope of research in skeleton-based action understanding and encourage more attention to dense prediction tasks.
<div id='section'>Paperid: <span id='pid'>3, <a href='https://arxiv.org/pdf/2507.00792.pdf' target='_blank'>https://arxiv.org/pdf/2507.00792.pdf</a></span>   <span><a href='https://github.com/hvoss-techfak/JAX-IK' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hendric Voss, Stefan Kopp
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00792">JAX-IK: Real-Time Inverse Kinematics for Generating Multi-Constrained Movements of Virtual Human Characters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating accurate and realistic virtual human movements in real-time is of high importance for a variety of applications in computer graphics, interactive virtual environments, robotics, and biomechanics. This paper introduces a novel real-time inverse kinematics (IK) solver specifically designed for realistic human-like movement generation. Leveraging the automatic differentiation and just-in-time compilation of TensorFlow, the proposed solver efficiently handles complex articulated human skeletons with high degrees of freedom. By treating forward and inverse kinematics as differentiable operations, our method effectively addresses common challenges such as error accumulation and complicated joint limits in multi-constrained problems, which are critical for realistic human motion modeling. We demonstrate the solver's effectiveness on the SMPLX human skeleton model, evaluating its performance against widely used iterative-based IK algorithms, like Cyclic Coordinate Descent (CCD), FABRIK, and the nonlinear optimization algorithm IPOPT. Our experiments cover both simple end-effector tasks and sophisticated, multi-constrained problems with realistic joint limits. Results indicate that our IK solver achieves real-time performance, exhibiting rapid convergence, minimal computational overhead per iteration, and improved success rates compared to existing methods. The project code is available at https://github.com/hvoss-techfak/JAX-IK
<div id='section'>Paperid: <span id='pid'>4, <a href='https://arxiv.org/pdf/2507.00566.pdf' target='_blank'>https://arxiv.org/pdf/2507.00566.pdf</a></span>   <span><a href='https://github.com/kaai520/PGFA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Zhou, Shuhai Zhang, Zeng You, Jinwu Hu, Mingkui Tan, Fei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00566">Zero-Shot Skeleton-Based Action Recognition With Prototype-Guided Feature Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Zero-shot skeleton-based action recognition aims to classify unseen skeleton-based human actions without prior exposure to such categories during training. This task is extremely challenging due to the difficulty in generalizing from known to unknown actions. Previous studies typically use two-stage training: pre-training skeleton encoders on seen action categories using cross-entropy loss and then aligning pre-extracted skeleton and text features, enabling knowledge transfer to unseen classes through skeleton-text alignment and language models' generalization. However, their efficacy is hindered by 1) insufficient discrimination for skeleton features, as the fixed skeleton encoder fails to capture necessary alignment information for effective skeleton-text alignment; 2) the neglect of alignment bias between skeleton and unseen text features during testing. To this end, we propose a prototype-guided feature alignment paradigm for zero-shot skeleton-based action recognition, termed PGFA. Specifically, we develop an end-to-end cross-modal contrastive training framework to improve skeleton-text alignment, ensuring sufficient discrimination for skeleton features. Additionally, we introduce a prototype-guided text feature alignment strategy to mitigate the adverse impact of the distribution discrepancy during testing. We provide a theoretical analysis to support our prototype-guided text feature alignment strategy and empirically evaluate our overall PGFA on three well-known datasets. Compared with the top competitor SMIE method, our PGFA achieves absolute accuracy improvements of 22.96%, 12.53%, and 18.54% on the NTU-60, NTU-120, and PKU-MMD datasets, respectively.
<div id='section'>Paperid: <span id='pid'>5, <a href='https://arxiv.org/pdf/2506.07216.pdf' target='_blank'>https://arxiv.org/pdf/2506.07216.pdf</a></span>   <span><a href='https://github.com/NadaAbodeshish/Random-Cropping-augmentation-HGR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nada Aboudeshish, Dmitry Ignatov, Radu Timofte
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07216">AugmentGest: Can Random Data Cropping Augmentation Boost Gesture Recognition Performance?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data augmentation is a crucial technique in deep learning, particularly for tasks with limited dataset diversity, such as skeleton-based datasets. This paper proposes a comprehensive data augmentation framework that integrates geometric transformations, random cropping, rotation, zooming and intensity-based transformations, brightness and contrast adjustments to simulate real-world variations. Random cropping ensures the preservation of spatio-temporal integrity while addressing challenges such as viewpoint bias and occlusions. The augmentation pipeline generates three augmented versions for each sample in addition to the data set sample, thus quadrupling the data set size and enriching the diversity of gesture representations. The proposed augmentation strategy is evaluated on three models: multi-stream e2eET, FPPR point cloud-based hand gesture recognition (HGR), and DD-Network. Experiments are conducted on benchmark datasets including DHG14/28, SHREC'17, and JHMDB. The e2eET model, recognized as the state-of-the-art for hand gesture recognition on DHG14/28 and SHREC'17. The FPPR-PCD model, the second-best performing model on SHREC'17, excels in point cloud-based gesture recognition. DD-Net, a lightweight and efficient architecture for skeleton-based action recognition, is evaluated on SHREC'17 and the Human Motion Data Base (JHMDB). The results underline the effectiveness and versatility of the proposed augmentation strategy, significantly improving model generalization and robustness across diverse datasets and architectures. This framework not only establishes state-of-the-art results on all three evaluated models but also offers a scalable solution to advance HGR and action recognition applications in real-world scenarios. The framework is available at https://github.com/NadaAbodeshish/Random-Cropping-augmentation-HGR
<div id='section'>Paperid: <span id='pid'>6, <a href='https://arxiv.org/pdf/2506.01608.pdf' target='_blank'>https://arxiv.org/pdf/2506.01608.pdf</a></span>   <span><a href='https://github.com/amathislab/EPFL-Smart-Kitchen' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Andy Bonnetto, Haozhe Qi, Franklin Leong, Matea Tashkovska, Mahdi Rad, Solaiman Shokur, Friedhelm Hummel, Silvestro Micera, Marc Pollefeys, Alexander Mathis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01608">EPFL-Smart-Kitchen-30: Densely annotated cooking dataset with 3D kinematics to challenge video and language models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding behavior requires datasets that capture humans while carrying out complex tasks. The kitchen is an excellent environment for assessing human motor and cognitive function, as many complex actions are naturally exhibited in kitchens from chopping to cleaning. Here, we introduce the EPFL-Smart-Kitchen-30 dataset, collected in a noninvasive motion capture platform inside a kitchen environment. Nine static RGB-D cameras, inertial measurement units (IMUs) and one head-mounted HoloLens~2 headset were used to capture 3D hand, body, and eye movements. The EPFL-Smart-Kitchen-30 dataset is a multi-view action dataset with synchronized exocentric, egocentric, depth, IMUs, eye gaze, body and hand kinematics spanning 29.7 hours of 16 subjects cooking four different recipes. Action sequences were densely annotated with 33.78 action segments per minute. Leveraging this multi-modal dataset, we propose four benchmarks to advance behavior understanding and modeling through 1) a vision-language benchmark, 2) a semantic text-to-motion generation benchmark, 3) a multi-modal action recognition benchmark, 4) a pose-based action segmentation benchmark. We expect the EPFL-Smart-Kitchen-30 dataset to pave the way for better methods as well as insights to understand the nature of ecologically-valid human behavior. Code and data are available at https://github.com/amathislab/EPFL-Smart-Kitchen
<div id='section'>Paperid: <span id='pid'>7, <a href='https://arxiv.org/pdf/2504.11749.pdf' target='_blank'>https://arxiv.org/pdf/2504.11749.pdf</a></span>   <span><a href='https://github.com/zzysteve/SkeletonX' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zongye Zhang, Wenrui Cai, Qingjie Liu, Yunhong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.11749">SkeletonX: Data-Efficient Skeleton-based Action Recognition via Cross-sample Feature Aggregation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While current skeleton action recognition models demonstrate impressive performance on large-scale datasets, their adaptation to new application scenarios remains challenging. These challenges are particularly pronounced when facing new action categories, diverse performers, and varied skeleton layouts, leading to significant performance degeneration. Additionally, the high cost and difficulty of collecting skeleton data make large-scale data collection impractical. This paper studies one-shot and limited-scale learning settings to enable efficient adaptation with minimal data. Existing approaches often overlook the rich mutual information between labeled samples, resulting in sub-optimal performance in low-data scenarios. To boost the utility of labeled data, we identify the variability among performers and the commonality within each action as two key attributes. We present SkeletonX, a lightweight training pipeline that integrates seamlessly with existing GCN-based skeleton action recognizers, promoting effective training under limited labeled data. First, we propose a tailored sample pair construction strategy on two key attributes to form and aggregate sample pairs. Next, we develop a concise and effective feature aggregation module to process these pairs. Extensive experiments are conducted on NTU RGB+D, NTU RGB+D 120, and PKU-MMD with various GCN backbones, demonstrating that the pipeline effectively improves performance when trained from scratch with limited data. Moreover, it surpasses previous state-of-the-art methods in the one-shot setting, with only 1/10 of the parameters and much fewer FLOPs. The code and data are available at: https://github.com/zzysteve/SkeletonX
<div id='section'>Paperid: <span id='pid'>8, <a href='https://arxiv.org/pdf/2501.05711.pdf' target='_blank'>https://arxiv.org/pdf/2501.05711.pdf</a></span>   <span><a href='https://github.com/dominickrei/EgoExo4ADL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dominick Reilly, Manish Kumar Govind, Le Xue, Srijan Das
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.05711">From My View to Yours: Ego-Augmented Learning in Large Vision Language Models for Understanding Exocentric Daily Living Activities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Vision Language Models (LVLMs) have demonstrated impressive capabilities in video understanding, yet their adoption for Activities of Daily Living (ADL) remains limited by their inability to capture fine-grained interactions and spatial relationships. To address this, we aim to leverage the complementary nature of egocentric views to enhance LVLM's understanding of exocentric ADL videos. Consequently, we propose ego2exo knowledge distillation to learn ego-augmented exp representations. While effective, this approach requires paired ego-exo videos, which are impractical to collect at scale. To address this, we propose Skeleton-guided Synthetic Ego Generation (SK-EGO), which leverages human skeleton motion to generate synthetic ego views from exocentric videos. To enhance the ego representation of LVLMs trained on synthetic data, we develop a domain-agnostic bootstrapped ego2exo strategy that effectively transfers knowledge from real ego-exo pairs to synthetic ego-exo pairs, while mitigating domain misalignment. We find that the exo representations of our ego-augmented LVLMs successfully learn to extract ego-perspective cues, demonstrated through comprehensive evaluation on six ADL benchmarks and our proposed Ego-in-Exo PerceptionMCQ benchmark designed specifically to assess egocentric understanding from exocentric videos. Code, models, and data will be open-sourced at https://github.com/dominickrei/EgoExo4ADL.
<div id='section'>Paperid: <span id='pid'>9, <a href='https://arxiv.org/pdf/2411.18941.pdf' target='_blank'>https://arxiv.org/pdf/2411.18941.pdf</a></span>   <span><a href='https://github.com/firework8/ProtoGCN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongda Liu, Yunfan Liu, Min Ren, Hao Wang, Yunlong Wang, Zhenan Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18941">Revealing Key Details to See Differences: A Novel Prototypical Perspective for Skeleton-based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In skeleton-based action recognition, a key challenge is distinguishing between actions with similar trajectories of joints due to the lack of image-level details in skeletal representations. Recognizing that the differentiation of similar actions relies on subtle motion details in specific body parts, we direct our approach to focus on the fine-grained motion of local skeleton components. To this end, we introduce ProtoGCN, a Graph Convolutional Network (GCN)-based model that breaks down the dynamics of entire skeleton sequences into a combination of learnable prototypes representing core motion patterns of action units. By contrasting the reconstruction of prototypes, ProtoGCN can effectively identify and enhance the discriminative representation of similar actions. Without bells and whistles, ProtoGCN achieves state-of-the-art performance on multiple benchmark datasets, including NTU RGB+D, NTU RGB+D 120, Kinetics-Skeleton, and FineGYM, which demonstrates the effectiveness of the proposed method. The code is available at https://github.com/firework8/ProtoGCN.
<div id='section'>Paperid: <span id='pid'>10, <a href='https://arxiv.org/pdf/2410.20349.pdf' target='_blank'>https://arxiv.org/pdf/2410.20349.pdf</a></span>   <span><a href='https://github.com/LanglandsLin/IGM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lilang Lin, Lehong Wu, Jiahang Zhang, Jiaying Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20349">Idempotent Unsupervised Representation Learning for Skeleton-Based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative models, as a powerful technique for generation, also gradually become a critical tool for recognition tasks. However, in skeleton-based action recognition, the features obtained from existing pre-trained generative methods contain redundant information unrelated to recognition, which contradicts the nature of the skeleton's spatially sparse and temporally consistent properties, leading to undesirable performance. To address this challenge, we make efforts to bridge the gap in theory and methodology and propose a novel skeleton-based idempotent generative model (IGM) for unsupervised representation learning. More specifically, we first theoretically demonstrate the equivalence between generative models and maximum entropy coding, which demonstrates a potential route that makes the features of generative models more compact by introducing contrastive learning. To this end, we introduce the idempotency constraint to form a stronger consistency regularization in the feature space, to push the features only to maintain the critical information of motion semantics for the recognition task. Our extensive experiments on benchmark datasets, NTU RGB+D and PKUMMD, demonstrate the effectiveness of our proposed method. On the NTU 60 xsub dataset, we observe a performance improvement from 84.6$\%$ to 86.2$\%$. Furthermore, in zero-shot adaptation scenarios, our model demonstrates significant efficacy by achieving promising results in cases that were previously unrecognizable. Our project is available at \url{https://github.com/LanglandsLin/IGM}.
<div id='section'>Paperid: <span id='pid'>11, <a href='https://arxiv.org/pdf/2409.17951.pdf' target='_blank'>https://arxiv.org/pdf/2409.17951.pdf</a></span>   <span><a href='https://github.com/YinxPeng/HA-CM-main' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinpeng Yin, Wenming Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17951">Spatial Hierarchy and Temporal Attention Guided Cross Masking for Self-supervised Skeleton-based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In self-supervised skeleton-based action recognition, the mask reconstruction paradigm is gaining interest in enhancing model refinement and robustness through effective masking. However, previous works primarily relied on a single masking criterion, resulting in the model overfitting specific features and overlooking other effective information. In this paper, we introduce a hierarchy and attention guided cross-masking framework (HA-CM) that applies masking to skeleton sequences from both spatial and temporal perspectives. Specifically, in spatial graphs, we utilize hyperbolic space to maintain joint distinctions and effectively preserve the hierarchical structure of high-dimensional skeletons, employing joint hierarchy as the masking criterion. In temporal flows, we substitute traditional distance metrics with the global attention of joints for masking, addressing the convergence of distances in high-dimensional space and the lack of a global perspective. Additionally, we incorporate cross-contrast loss based on the cross-masking framework into the loss function to enhance the model's learning of instance-level features. HA-CM shows efficiency and universality on three public large-scale datasets, NTU-60, NTU-120, and PKU-MMD. The source code of our HA-CM is available at https://github.com/YinxPeng/HA-CM-main.
<div id='section'>Paperid: <span id='pid'>12, <a href='https://arxiv.org/pdf/2409.14336.pdf' target='_blank'>https://arxiv.org/pdf/2409.14336.pdf</a></span>   <span><a href='https://github.com/jidongkuang/DVTA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jidong Kuang, Hongsong Wang, Chaolei Han, Yang Zhang, Jie Gui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14336">Zero-Shot Skeleton-based Action Recognition with Dual Visual-Text Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Zero-shot action recognition, which addresses the issue of scalability and generalization in action recognition and allows the models to adapt to new and unseen actions dynamically, is an important research topic in computer vision communities. The key to zero-shot action recognition lies in aligning visual features with semantic vectors representing action categories. Most existing methods either directly project visual features onto the semantic space of text category or learn a shared embedding space between the two modalities. However, a direct projection cannot accurately align the two modalities, and learning robust and discriminative embedding space between visual and text representations is often difficult. To address these issues, we introduce Dual Visual-Text Alignment (DVTA) for skeleton-based zero-shot action recognition. The DVTA consists of two alignment modules--Direct Alignment (DA) and Augmented Alignment (AA)--along with a designed Semantic Description Enhancement (SDE). The DA module maps the skeleton features to the semantic space through a specially designed visual projector, followed by the SDE, which is based on cross-attention to enhance the connection between skeleton and text, thereby reducing the gap between modalities. The AA module further strengthens the learning of the embedding space by utilizing deep metric learning to learn the similarity between skeleton and text. Our approach achieves state-of-the-art performances on several popular zero-shot skeleton-based action recognition benchmarks. The code is available at: https://github.com/jidongkuang/DVTA.
<div id='section'>Paperid: <span id='pid'>13, <a href='https://arxiv.org/pdf/2409.05749.pdf' target='_blank'>https://arxiv.org/pdf/2409.05749.pdf</a></span>   <span><a href='https://github.com/SafwenNaimi/Representation-Learning-for-Skeleton-Action-Recognition-with-Convolutional-Transformers-and-BYOL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Safwen Naimi, Wassim Bouachir, Guillaume-Alexandre Bilodeau
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.05749">ReL-SAR: Representation Learning for Skeleton Action Recognition with Convolutional Transformers and BYOL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To extract robust and generalizable skeleton action recognition features, large amounts of well-curated data are typically required, which is a challenging task hindered by annotation and computation costs. Therefore, unsupervised representation learning is of prime importance to leverage unlabeled skeleton data. In this work, we investigate unsupervised representation learning for skeleton action recognition. For this purpose, we designed a lightweight convolutional transformer framework, named ReL-SAR, exploiting the complementarity of convolutional and attention layers for jointly modeling spatial and temporal cues in skeleton sequences. We also use a Selection-Permutation strategy for skeleton joints to ensure more informative descriptions from skeletal data. Finally, we capitalize on Bootstrap Your Own Latent (BYOL) to learn robust representations from unlabeled skeleton sequence data. We achieved very competitive results on limited-size datasets: MCAD, IXMAS, JHMDB, and NW-UCLA, showing the effectiveness of our proposed method against state-of-the-art methods in terms of both performance and computational efficiency. To ensure reproducibility and reusability, the source code including all implementation parameters is provided at: https://github.com/SafwenNaimi/Representation-Learning-for-Skeleton-Action-Recognition-with-Convolutional-Transformers-and-BYOL
<div id='section'>Paperid: <span id='pid'>14, <a href='https://arxiv.org/pdf/2409.02483.pdf' target='_blank'>https://arxiv.org/pdf/2409.02483.pdf</a></span>   <span><a href='https://github.com/yunfengdiao/Skeleton-Robustness-Benchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunfeng Diao, Baiqi Wu, Ruixuan Zhang, Ajian Liu, Xiaoshuai Hao, Xingxing Wei, Meng Wang, He Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.02483">TASAR: Transfer-based Attack on Skeletal Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeletal sequence data, as a widely employed representation of human actions, are crucial in Human Activity Recognition (HAR). Recently, adversarial attacks have been proposed in this area, which exposes potential security concerns, and more importantly provides a good tool for model robustness test. Within this research, transfer-based attack is an important tool as it mimics the real-world scenario where an attacker has no knowledge of the target model, but is under-explored in Skeleton-based HAR (S-HAR). Consequently, existing S-HAR attacks exhibit weak adversarial transferability and the reason remains largely unknown. In this paper, we investigate this phenomenon via the characterization of the loss function. We find that one prominent indicator of poor transferability is the low smoothness of the loss function. Led by this observation, we improve the transferability by properly smoothening the loss when computing the adversarial examples. This leads to the first Transfer-based Attack on Skeletal Action Recognition, TASAR. TASAR explores the smoothened model posterior of pre-trained surrogates, which is achieved by a new post-train Dual Bayesian optimization strategy. Furthermore, unlike existing transfer-based methods which overlook the temporal coherence within sequences, TASAR incorporates motion dynamics into the Bayesian attack, effectively disrupting the spatial-temporal coherence of S-HARs. For exhaustive evaluation, we build the first large-scale robust S-HAR benchmark, comprising 7 S-HAR models, 10 attack methods, 3 S-HAR datasets and 2 defense models. Extensive results demonstrate the superiority of TASAR. Our benchmark enables easy comparisons for future studies, with the code available in the https://github.com/yunfengdiao/Skeleton-Robustness-Benchmark.
<div id='section'>Paperid: <span id='pid'>15, <a href='https://arxiv.org/pdf/2409.00349.pdf' target='_blank'>https://arxiv.org/pdf/2409.00349.pdf</a></span>   <span><a href='https://github.com/ipl-uw/ToddlerAct' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hsiang-Wei Huang, Jiacheng Sun, Cheng-Yen Yang, Zhongyu Jiang, Li-Yu Huang, Jenq-Neng Hwang, Yu-Ching Yeh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.00349">ToddlerAct: A Toddler Action Recognition Dataset for Gross Motor Development Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Assessing gross motor development in toddlers is crucial for understanding their physical development and identifying potential developmental delays or disorders. However, existing datasets for action recognition primarily focus on adults, lacking the diversity and specificity required for accurate assessment in toddlers. In this paper, we present ToddlerAct, a toddler gross motor action recognition dataset, aiming to facilitate research in early childhood development. The dataset consists of video recordings capturing a variety of gross motor activities commonly observed in toddlers aged under three years old. We describe the data collection process, annotation methodology, and dataset characteristics. Furthermore, we benchmarked multiple state-of-the-art methods including image-based and skeleton-based action recognition methods on our datasets. Our findings highlight the importance of domain-specific datasets for accurate assessment of gross motor development in toddlers and lay the foundation for future research in this critical area. Our dataset will be available at https://github.com/ipl-uw/ToddlerAct.
<div id='section'>Paperid: <span id='pid'>16, <a href='https://arxiv.org/pdf/2407.15706.pdf' target='_blank'>https://arxiv.org/pdf/2407.15706.pdf</a></span>   <span><a href='https://github.com/liujf69/MMCL-Action' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinfu Liu, Chen Chen, Mengyuan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.15706">Multi-Modality Co-Learning for Efficient Skeleton-based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeleton-based action recognition has garnered significant attention due to the utilization of concise and resilient skeletons. Nevertheless, the absence of detailed body information in skeletons restricts performance, while other multimodal methods require substantial inference resources and are inefficient when using multimodal data during both training and inference stages. To address this and fully harness the complementary multimodal features, we propose a novel multi-modality co-learning (MMCL) framework by leveraging the multimodal large language models (LLMs) as auxiliary networks for efficient skeleton-based action recognition, which engages in multi-modality co-learning during the training stage and keeps efficiency by employing only concise skeletons in inference. Our MMCL framework primarily consists of two modules. First, the Feature Alignment Module (FAM) extracts rich RGB features from video frames and aligns them with global skeleton features via contrastive learning. Second, the Feature Refinement Module (FRM) uses RGB images with temporal information and text instruction to generate instructive features based on the powerful generalization of multimodal LLMs. These instructive text features will further refine the classification scores and the refined scores will enhance the model's robustness and generalization in a manner similar to soft labels. Extensive experiments on NTU RGB+D, NTU RGB+D 120 and Northwestern-UCLA benchmarks consistently verify the effectiveness of our MMCL, which outperforms the existing skeleton-based action recognition methods. Meanwhile, experiments on UTD-MHAD and SYSU-Action datasets demonstrate the commendable generalization of our MMCL in zero-shot and domain-adaptive action recognition. Our code is publicly available at: https://github.com/liujf69/MMCL-Action.
<div id='section'>Paperid: <span id='pid'>17, <a href='https://arxiv.org/pdf/2407.13460.pdf' target='_blank'>https://arxiv.org/pdf/2407.13460.pdf</a></span>   <span><a href='https://github.com/pha123661/SA-DVAE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sheng-Wei Li, Zi-Xiang Wei, Wei-Jie Chen, Yi-Hsin Yu, Chih-Yuan Yang, Jane Yung-jen Hsu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.13460">SA-DVAE: Improving Zero-Shot Skeleton-Based Action Recognition by Disentangled Variational Autoencoders</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing zero-shot skeleton-based action recognition methods utilize projection networks to learn a shared latent space of skeleton features and semantic embeddings. The inherent imbalance in action recognition datasets, characterized by variable skeleton sequences yet constant class labels, presents significant challenges for alignment. To address the imbalance, we propose SA-DVAE -- Semantic Alignment via Disentangled Variational Autoencoders, a method that first adopts feature disentanglement to separate skeleton features into two independent parts -- one is semantic-related and another is irrelevant -- to better align skeleton and semantic features. We implement this idea via a pair of modality-specific variational autoencoders coupled with a total correction penalty. We conduct experiments on three benchmark datasets: NTU RGB+D, NTU RGB+D 120 and PKU-MMD, and our experimental results show that SA-DAVE produces improved performance over existing methods. The code is available at https://github.com/pha123661/SA-DVAE.
<div id='section'>Paperid: <span id='pid'>18, <a href='https://arxiv.org/pdf/2407.01397.pdf' target='_blank'>https://arxiv.org/pdf/2407.01397.pdf</a></span>   <span><a href='https://github.com/Sperimental3/CHARON' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Matteo Mosconi, Andriy Sorokin, Aniello Panariello, Angelo Porrello, Jacopo Bonato, Marco Cotogni, Luigi Sabetta, Simone Calderara, Rita Cucchiara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.01397">Mask and Compress: Efficient Skeleton-based Action Recognition in Continual Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The use of skeletal data allows deep learning models to perform action recognition efficiently and effectively. Herein, we believe that exploring this problem within the context of Continual Learning is crucial. While numerous studies focus on skeleton-based action recognition from a traditional offline perspective, only a handful venture into online approaches. In this respect, we introduce CHARON (Continual Human Action Recognition On skeletoNs), which maintains consistent performance while operating within an efficient framework. Through techniques like uniform sampling, interpolation, and a memory-efficient training stage based on masking, we achieve improved recognition accuracy while minimizing computational overhead. Our experiments on Split NTU-60 and the proposed Split NTU-120 datasets demonstrate that CHARON sets a new benchmark in this domain. The code is available at https://github.com/Sperimental3/CHARON.
<div id='section'>Paperid: <span id='pid'>19, <a href='https://arxiv.org/pdf/2407.00696.pdf' target='_blank'>https://arxiv.org/pdf/2407.00696.pdf</a></span>   <span><a href='https://github.com/wangjs96/Graph-in-Graph-Neural-Network' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiongshu Wang, Jing Yang, Jiankang Deng, Hatice Gunes, Siyang Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.00696">Graph in Graph Neural Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing Graph Neural Networks (GNNs) are limited to process graphs each of whose vertices is represented by a vector or a single value, limited their representing capability to describe complex objects. In this paper, we propose the first GNN (called Graph in Graph Neural (GIG) Network) which can process graph-style data (called GIG sample) whose vertices are further represented by graphs. Given a set of graphs or a data sample whose components can be represented by a set of graphs (called multi-graph data sample), our GIG network starts with a GIG sample generation (GSG) module which encodes the input as a \textbf{GIG sample}, where each GIG vertex includes a graph. Then, a set of GIG hidden layers are stacked, with each consisting of: (1) a GIG vertex-level updating (GVU) module that individually updates the graph in every GIG vertex based on its internal information; and (2) a global-level GIG sample updating (GGU) module that updates graphs in all GIG vertices based on their relationships, making the updated GIG vertices become global context-aware. This way, both internal cues within the graph contained in each GIG vertex and the relationships among GIG vertices could be utilized for down-stream tasks. Experimental results demonstrate that our GIG network generalizes well for not only various generic graph analysis tasks but also real-world multi-graph data analysis (e.g., human skeleton video-based action recognition), which achieved the new state-of-the-art results on 13 out of 14 evaluated datasets. Our code is publicly available at https://github.com/wangjs96/Graph-in-Graph-Neural-Network.
<div id='section'>Paperid: <span id='pid'>20, <a href='https://arxiv.org/pdf/2406.18011.pdf' target='_blank'>https://arxiv.org/pdf/2406.18011.pdf</a></span>   <span><a href='https://github.com/YijieYang23/SkeleT-GCN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yijie Yang, Jinlu Zhang, Jiaxu Zhang, Zhigang Tu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.18011">Expressive Keypoints for Skeleton-based Action Recognition via Skeleton Transformation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the realm of skeleton-based action recognition, the traditional methods which rely on coarse body keypoints fall short of capturing subtle human actions. In this work, we propose Expressive Keypoints that incorporates hand and foot details to form a fine-grained skeletal representation, improving the discriminative ability for existing models in discerning intricate actions. To efficiently model Expressive Keypoints, the Skeleton Transformation strategy is presented to gradually downsample the keypoints and prioritize prominent joints by allocating the importance weights. Additionally, a plug-and-play Instance Pooling module is exploited to extend our approach to multi-person scenarios without surging computation costs. Extensive experimental results over seven datasets present the superiority of our method compared to the state-of-the-art for skeleton-based human action recognition. Code is available at https://github.com/YijieYang23/SkeleT-GCN.
<div id='section'>Paperid: <span id='pid'>21, <a href='https://arxiv.org/pdf/2406.13327.pdf' target='_blank'>https://arxiv.org/pdf/2406.13327.pdf</a></span>   <span><a href='https://github.com/azzh1/PURLS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Anqi Zhu, Qiuhong Ke, Mingming Gong, James Bailey
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.13327">Part-aware Unified Representation of Language and Skeleton for Zero-shot Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While remarkable progress has been made on supervised skeleton-based action recognition, the challenge of zero-shot recognition remains relatively unexplored. In this paper, we argue that relying solely on aligning label-level semantics and global skeleton features is insufficient to effectively transfer locally consistent visual knowledge from seen to unseen classes. To address this limitation, we introduce Part-aware Unified Representation between Language and Skeleton (PURLS) to explore visual-semantic alignment at both local and global scales. PURLS introduces a new prompting module and a novel partitioning module to generate aligned textual and visual representations across different levels. The former leverages a pre-trained GPT-3 to infer refined descriptions of the global and local (body-part-based and temporal-interval-based) movements from the original action labels. The latter employs an adaptive sampling strategy to group visual features from all body joint movements that are semantically relevant to a given description. Our approach is evaluated on various skeleton/language backbones and three large-scale datasets, i.e., NTU-RGB+D 60, NTU-RGB+D 120, and a newly curated dataset Kinetics-skeleton 200. The results showcase the universality and superior performance of PURLS, surpassing prior skeleton-based solutions and standard baselines from other domains. The source codes can be accessed at https://github.com/azzh1/PURLS.
<div id='section'>Paperid: <span id='pid'>22, <a href='https://arxiv.org/pdf/2405.20633.pdf' target='_blank'>https://arxiv.org/pdf/2405.20633.pdf</a></span>   <span><a href='https://github.com/YilliaJing/Skeleton-OOD.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jing Xu, Anqi Zhu, Jingyu Lin, Qiuhong Ke, Cunjian Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.20633">Skeleton-OOD: An End-to-End Skeleton-Based Model for Robust Out-of-Distribution Human Action Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human action recognition is crucial in computer vision systems. However, in real-world scenarios, human actions often fall outside the distribution of training data, requiring a model to both recognize in-distribution (ID) actions and reject out-of-distribution (OOD) ones. Despite its importance, there has been limited research on OOD detection in human actions. Existing works on OOD detection mainly focus on image data with RGB structure, and many methods are post-hoc in nature. While these methods are convenient and computationally efficient, they often lack sufficient accuracy, fail to consider the exposure of OOD samples, and ignore the application in skeleton structure data. To address these challenges, we propose a novel end-to-end skeleton-based model called Skeleton-OOD, which is committed to improving the effectiveness of OOD tasks while ensuring the accuracy of ID recognition. Through extensive experiments conducted on NTU-RGB+D 60, NTU-RGB+D 120, and Kinetics-400 datasets, Skeleton-OOD demonstrates the superior performance of our proposed approach compared to state-of-the-art methods. Our findings underscore the effectiveness of classic OOD detection techniques in the context of skeleton-based action recognition tasks, offering promising avenues for future research in this field. Code is available at https://github.com/YilliaJing/Skeleton-OOD.git.
<div id='section'>Paperid: <span id='pid'>23, <a href='https://arxiv.org/pdf/2405.20606.pdf' target='_blank'>https://arxiv.org/pdf/2405.20606.pdf</a></span>   <span><a href='https://github.com/cseeyangchen/C2VL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Chen, Tian He, Junfeng Fu, Ling Wang, Jingcai Guo, Ting Hu, Hong Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.20606">Vision-Language Meets the Skeleton: Progressively Distillation with Cross-Modal Knowledge for 3D Action Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeleton-based action representation learning aims to interpret and understand human behaviors by encoding the skeleton sequences, which can be categorized into two primary training paradigms: supervised learning and self-supervised learning. However, the former one-hot classification requires labor-intensive predefined action categories annotations, while the latter involves skeleton transformations (e.g., cropping) in the pretext tasks that may impair the skeleton structure. To address these challenges, we introduce a novel skeleton-based training framework (C$^2$VL) based on Cross-modal Contrastive learning that uses the progressive distillation to learn task-agnostic human skeleton action representation from the Vision-Language knowledge prompts. Specifically, we establish the vision-language action concept space through vision-language knowledge prompts generated by pre-trained large multimodal models (LMMs), which enrich the fine-grained details that the skeleton action space lacks. Moreover, we propose the intra-modal self-similarity and inter-modal cross-consistency softened targets in the cross-modal representation learning process to progressively control and guide the degree of pulling vision-language knowledge prompts and corresponding skeletons closer. These soft instance discrimination and self-knowledge distillation strategies contribute to the learning of better skeleton-based action representations from the noisy skeleton-vision-language pairs. During the inference phase, our method requires only the skeleton data as the input for action recognition and no longer for vision-language prompts. Extensive experiments on NTU RGB+D 60, NTU RGB+D 120, and PKU-MMD datasets demonstrate that our method outperforms the previous methods and achieves state-of-the-art results. Code is available at: https://github.com/cseeyangchen/C2VL.
<div id='section'>Paperid: <span id='pid'>24, <a href='https://arxiv.org/pdf/2405.19833.pdf' target='_blank'>https://arxiv.org/pdf/2405.19833.pdf</a></span>   <span><a href='https://github.com/MartaYang/KITRO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fengyuan Yang, Kerui Gu, Angela Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19833">KITRO: Refining Human Mesh by 2D Clues and Kinematic-tree Rotation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>2D keypoints are commonly used as an additional cue to refine estimated 3D human meshes. Current methods optimize the pose and shape parameters with a reprojection loss on the provided 2D keypoints. Such an approach, while simple and intuitive, has limited effectiveness because the optimal solution is hard to find in ambiguous parameter space and may sacrifice depth. Additionally, divergent gradients from distal joints complicate and deviate the refinement of proximal joints in the kinematic chain. To address these, we introduce Kinematic-Tree Rotation (KITRO), a novel mesh refinement strategy that explicitly models depth and human kinematic-tree structure. KITRO treats refinement from a bone-wise perspective. Unlike previous methods which perform gradient-based optimizations, our method calculates bone directions in closed form. By accounting for the 2D pose, bone length, and parent joint's depth, the calculation results in two possible directions for each child joint. We then use a decision tree to trace binary choices for all bones along the human skeleton's kinematic-tree to select the most probable hypothesis. Our experiments across various datasets and baseline models demonstrate that KITRO significantly improves 3D joint estimation accuracy and achieves an ideal 2D fit simultaneously. Our code available at: https://github.com/MartaYang/KITRO.
<div id='section'>Paperid: <span id='pid'>25, <a href='https://arxiv.org/pdf/2404.15719.pdf' target='_blank'>https://arxiv.org/pdf/2404.15719.pdf</a></span>   <span><a href='https://github.com/liujf69/ICMEW2024-Track10' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinfu Liu, Baiqiao Yin, Jiaying Lin, Jiajun Wen, Yue Li, Mengyuan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.15719">HDBN: A Novel Hybrid Dual-branch Network for Robust Skeleton-based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeleton-based action recognition has gained considerable traction thanks to its utilization of succinct and robust skeletal representations. Nonetheless, current methodologies often lean towards utilizing a solitary backbone to model skeleton modality, which can be limited by inherent flaws in the network backbone. To address this and fully leverage the complementary characteristics of various network architectures, we propose a novel Hybrid Dual-Branch Network (HDBN) for robust skeleton-based action recognition, which benefits from the graph convolutional network's proficiency in handling graph-structured data and the powerful modeling capabilities of Transformers for global information. In detail, our proposed HDBN is divided into two trunk branches: MixGCN and MixFormer. The two branches utilize GCNs and Transformers to model both 2D and 3D skeletal modalities respectively. Our proposed HDBN emerged as one of the top solutions in the Multi-Modal Video Reasoning and Analyzing Competition (MMVRAC) of 2024 ICME Grand Challenge, achieving accuracies of 47.95% and 75.36% on two benchmarks of the UAV-Human dataset by outperforming most existing methods. Our code will be publicly available at: https://github.com/liujf69/ICMEW2024-Track10.
<div id='section'>Paperid: <span id='pid'>26, <a href='https://arxiv.org/pdf/2403.15212.pdf' target='_blank'>https://arxiv.org/pdf/2403.15212.pdf</a></span>   <span><a href='https://github.com/DeepIntoStreams/GCN-DevLSTM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Jiang, Weixin Yang, Xin Zhang, Hao Ni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15212">GCN-DevLSTM: Path Development for Skeleton-Based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeleton-based action recognition (SAR) in videos is an important but challenging task in computer vision. The recent state-of-the-art (SOTA) models for SAR are primarily based on graph convolutional neural networks (GCNs), which are powerful in extracting the spatial information of skeleton data. However, it is yet clear that such GCN-based models can effectively capture the temporal dynamics of human action sequences. To this end, we propose the G-Dev layer, which exploits the path development -- a principled and parsimonious representation for sequential data by leveraging the Lie group structure. By integrating the G-Dev layer, the hybrid G-DevLSTM module enhances the traditional LSTM to reduce the time dimension while retaining high-frequency information. It can be conveniently applied to any temporal graph data, complementing existing advanced GCN-based models. Our empirical studies on the NTU60, NTU120 and Chalearn2013 datasets demonstrate that our proposed GCN-DevLSTM network consistently improves the strong GCN baseline models and achieves SOTA results with superior robustness in SAR tasks. The code is available at https://github.com/DeepIntoStreams/GCN-DevLSTM.
<div id='section'>Paperid: <span id='pid'>27, <a href='https://arxiv.org/pdf/2403.09975.pdf' target='_blank'>https://arxiv.org/pdf/2403.09975.pdf</a></span>   <span><a href='https://github.com/xuyizdby/NoiseEraSAR' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/xuyizdby/NoiseEraSAR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Xu, Kunyu Peng, Di Wen, Ruiping Liu, Junwei Zheng, Yufan Chen, Jiaming Zhang, Alina Roitberg, Kailun Yang, Rainer Stiefelhagen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.09975">Skeleton-Based Human Action Recognition with Noisy Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding human actions from body poses is critical for assistive robots sharing space with humans in order to make informed and safe decisions about the next interaction. However, precise temporal localization and annotation of activity sequences is time-consuming and the resulting labels are often noisy. If not effectively addressed, label noise negatively affects the model's training, resulting in lower recognition quality. Despite its importance, addressing label noise for skeleton-based action recognition has been overlooked so far. In this study, we bridge this gap by implementing a framework that augments well-established skeleton-based human action recognition methods with label-denoising strategies from various research areas to serve as the initial benchmark. Observations reveal that these baselines yield only marginal performance when dealing with sparse skeleton data. Consequently, we introduce a novel methodology, NoiseEraSAR, which integrates global sample selection, co-teaching, and Cross-Modal Mixture-of-Experts (CM-MOE) strategies, aimed at mitigating the adverse impacts of label noise. Our proposed approach demonstrates better performance on the established benchmark, setting new state-of-the-art standards. The source code for this study is accessible at https://github.com/xuyizdby/NoiseEraSAR.
<div id='section'>Paperid: <span id='pid'>28, <a href='https://arxiv.org/pdf/2403.04444.pdf' target='_blank'>https://arxiv.org/pdf/2403.04444.pdf</a></span>   <span><a href='https://github.com/Andyen512/DDHPose' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingyuan Cai, Xuecai Hu, Saihui Hou, Li Yao, Yongzhen Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.04444">Disentangled Diffusion-Based 3D Human Pose Estimation with Hierarchical Spatial and Temporal Denoiser</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, diffusion-based methods for monocular 3D human pose estimation have achieved state-of-the-art (SOTA) performance by directly regressing the 3D joint coordinates from the 2D pose sequence. Although some methods decompose the task into bone length and bone direction prediction based on the human anatomical skeleton to explicitly incorporate more human body prior constraints, the performance of these methods is significantly lower than that of the SOTA diffusion-based methods. This can be attributed to the tree structure of the human skeleton. Direct application of the disentangled method could amplify the accumulation of hierarchical errors, propagating through each hierarchy. Meanwhile, the hierarchical information has not been fully explored by the previous methods. To address these problems, a Disentangled Diffusion-based 3D Human Pose Estimation method with Hierarchical Spatial and Temporal Denoiser is proposed, termed DDHPose. In our approach: (1) We disentangle the 3D pose and diffuse the bone length and bone direction during the forward process of the diffusion model to effectively model the human pose prior. A disentanglement loss is proposed to supervise diffusion model learning. (2) For the reverse process, we propose Hierarchical Spatial and Temporal Denoiser (HSTDenoiser) to improve the hierarchical modeling of each joint. Our HSTDenoiser comprises two components: the Hierarchical-Related Spatial Transformer (HRST) and the Hierarchical-Related Temporal Transformer (HRTT). HRST exploits joint spatial information and the influence of the parent joint on each joint for spatial modeling, while HRTT utilizes information from both the joint and its hierarchical adjacent joints to explore the hierarchical temporal correlations among joints. Code and models are available at https://github.com/Andyen512/DDHPose
<div id='section'>Paperid: <span id='pid'>29, <a href='https://arxiv.org/pdf/2312.15144.pdf' target='_blank'>https://arxiv.org/pdf/2312.15144.pdf</a></span>   <span><a href='https://github.com/zhshj0110/CSRE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaojie Zhang, Jianqin Yin, Yonghao Dang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.15144">A Generically Contrastive Spatiotemporal Representation Enhancement for 3D Skeleton Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeleton-based action recognition is a central task in computer vision and human-robot interaction. However, most previous methods suffer from overlooking the explicit exploitation of the latent data distributions (i.e., the intra-class variations and inter-class relations), thereby leading to confusion about ambiguous samples and sub-optimum solutions of the skeleton encoders. To mitigate this, we propose a Contrastive Spatiotemporal Representation Enhancement (CSRE) framework to obtain more discriminative representations from the sequences, which can be incorporated into various previous skeleton encoders and can be removed when testing. Specifically, we decompose the representation into spatial- and temporal-specific features to explore fine-grained motion patterns along the corresponding dimensions. Furthermore, to explicitly exploit the latent data distributions, we employ the attentive features to contrastive learning, which models the cross-sequence semantic relations by pulling together the features from the positive pairs and pushing away the negative pairs. Extensive experiments show that CSRE with five various skeleton encoders (HCN, 2S-AGCN, CTR-GCN, Hyperformer, and BlockGCN) achieves solid improvements on five benchmarks. The code will be released at https://github.com/zhshj0110/CSRE.
<div id='section'>Paperid: <span id='pid'>30, <a href='https://arxiv.org/pdf/2312.07051.pdf' target='_blank'>https://arxiv.org/pdf/2312.07051.pdf</a></span>   <span><a href='https://github.com/Charrrrrlie/Mask-as-Supervision' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchen Yang, Yu Qiao, Xiao Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.07051">Mask as Supervision: Leveraging Unified Mask Information for Unsupervised 3D Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic estimation of 3D human pose from monocular RGB images is a challenging and unsolved problem in computer vision. In a supervised manner, approaches heavily rely on laborious annotations and present hampered generalization ability due to the limited diversity of 3D pose datasets. To address these challenges, we propose a unified framework that leverages mask as supervision for unsupervised 3D pose estimation. With general unsupervised segmentation algorithms, the proposed model employs skeleton and physique representations that exploit accurate pose information from coarse to fine. Compared with previous unsupervised approaches, we organize the human skeleton in a fully unsupervised way which enables the processing of annotation-free data and provides ready-to-use estimation results. Comprehensive experiments demonstrate our state-of-the-art pose estimation performance on Human3.6M and MPI-INF-3DHP datasets. Further experiments on in-the-wild datasets also illustrate the capability to access more data to boost our model. Code will be available at https://github.com/Charrrrrlie/Mask-as-Supervision.
<div id='section'>Paperid: <span id='pid'>31, <a href='https://arxiv.org/pdf/2312.06330.pdf' target='_blank'>https://arxiv.org/pdf/2312.06330.pdf</a></span>   <span><a href='https://github.com/KPeng9510/OS-SAR' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/KPeng9510/OS-SAR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kunyu Peng, Cheng Yin, Junwei Zheng, Ruiping Liu, David Schneider, Jiaming Zhang, Kailun Yang, M. Saquib Sarfraz, Rainer Stiefelhagen, Alina Roitberg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.06330">Navigating Open Set Scenarios for Skeleton-based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In real-world scenarios, human actions often fall outside the distribution of training data, making it crucial for models to recognize known actions and reject unknown ones. However, using pure skeleton data in such open-set conditions poses challenges due to the lack of visual background cues and the distinct sparse structure of body pose sequences. In this paper, we tackle the unexplored Open-Set Skeleton-based Action Recognition (OS-SAR) task and formalize the benchmark on three skeleton-based datasets. We assess the performance of seven established open-set approaches on our task and identify their limits and critical generalization issues when dealing with skeleton information. To address these challenges, we propose a distance-based cross-modality ensemble method that leverages the cross-modal alignment of skeleton joints, bones, and velocities to achieve superior open-set recognition performance. We refer to the key idea as CrossMax - an approach that utilizes a novel cross-modality mean max discrepancy suppression mechanism to align latent spaces during training and a cross-modality distance-based logits refinement method during testing. CrossMax outperforms existing approaches and consistently yields state-of-the-art results across all datasets and backbones. The benchmark, code, and models will be released at https://github.com/KPeng9510/OS-SAR.
<div id='section'>Paperid: <span id='pid'>32, <a href='https://arxiv.org/pdf/2312.03288.pdf' target='_blank'>https://arxiv.org/pdf/2312.03288.pdf</a></span>   <span><a href='https://github.com/maclong01/STEP-CATFormer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nguyen Huu Bao Long
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.03288">STEP CATFormer: Spatial-Temporal Effective Body-Part Cross Attention Transformer for Skeleton-based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph convolutional networks (GCNs) have been widely used and achieved remarkable results in skeleton-based action recognition. We think the key to skeleton-based action recognition is a skeleton hanging in frames, so we focus on how the Graph Convolutional Convolution networks learn different topologies and effectively aggregate joint features in the global temporal and local temporal. In this work, we propose three Channel-wise Tolopogy Graph Convolution based on Channel-wise Topology Refinement Graph Convolution (CTR-GCN). Combining CTR-GCN with two joint cross-attention modules can capture the upper-lower body part and hand-foot relationship skeleton features. After that, to capture features of human skeletons changing in frames we design the Temporal Attention Transformers to extract skeletons effectively. The Temporal Attention Transformers can learn the temporal features of human skeleton sequences. Finally, we fuse the temporal features output scale with MLP and classification. We develop a powerful graph convolutional network named Spatial Temporal Effective Body-part Cross Attention Transformer which notably high-performance on the NTU RGB+D, NTU RGB+D 120 datasets. Our code and models are available at https://github.com/maclong01/STEP-CATFormer
<div id='section'>Paperid: <span id='pid'>33, <a href='https://arxiv.org/pdf/2312.01697.pdf' target='_blank'>https://arxiv.org/pdf/2312.01697.pdf</a></span>   <span><a href='https://github.com/OpenGVLab/Hulk' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yizhou Wang, Yixuan Wu, Weizhen He, Xun Guo, Feng Zhu, Lei Bai, Rui Zhao, Jian Wu, Tong He, Wanli Ouyang, Shixiang Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.01697">Hulk: A Universal Knowledge Translator for Human-Centric Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-centric perception tasks, e.g., pedestrian detection, skeleton-based action recognition, and pose estimation, have wide industrial applications, such as metaverse and sports analysis. There is a recent surge to develop human-centric foundation models that can benefit a broad range of human-centric perception tasks. While many human-centric foundation models have achieved success, they did not explore 3D and vision-language tasks for human-centric and required task-specific finetuning. These limitations restrict their application to more downstream tasks and situations. To tackle these problems, we present Hulk, the first multimodal human-centric generalist model, capable of addressing 2D vision, 3D vision, skeleton-based, and vision-language tasks without task-specific finetuning. The key to achieving this is condensing various task-specific heads into two general heads, one for discrete representations, \emph{e.g.,} languages, and the other for continuous representations, \emph{e.g.,} location coordinates. The outputs of two heads can be further stacked into four distinct input and output modalities. This uniform representation enables Hulk to treat diverse human-centric tasks as modality translation, integrating knowledge across a wide range of tasks. Comprehensive evaluations of Hulk on 12 benchmarks covering 8 human-centric tasks demonstrate the superiority of our proposed method, achieving state-of-the-art performance in 11 benchmarks. The code will be available on https://github.com/OpenGVLab/Hulk.
<div id='section'>Paperid: <span id='pid'>34, <a href='https://arxiv.org/pdf/2311.14775.pdf' target='_blank'>https://arxiv.org/pdf/2311.14775.pdf</a></span>   <span><a href='https://github.com/xuyankun/VSViG/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yankun Xu, Junzhe Wang, Yun-Hsuan Chen, Jie Yang, Wenjie Ming, Shuang Wang, Mohamad Sawan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.14775">VSViG: Real-time Video-based Seizure Detection via Skeleton-based Spatiotemporal ViG</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>An accurate and efficient epileptic seizure onset detection can significantly benefit patients. Traditional diagnostic methods, primarily relying on electroencephalograms (EEGs), often result in cumbersome and non-portable solutions, making continuous patient monitoring challenging. The video-based seizure detection system is expected to free patients from the constraints of scalp or implanted EEG devices and enable remote monitoring in residential settings. Previous video-based methods neither enable all-day monitoring nor provide short detection latency due to insufficient resources and ineffective patient action recognition techniques. Additionally, skeleton-based action recognition approaches remain limitations in identifying subtle seizure-related actions. To address these challenges, we propose a novel Video-based Seizure detection model via a skeleton-based spatiotemporal Vision Graph neural network (VSViG) for its efficient, accurate and timely purpose in real-time scenarios. Our experimental results indicate VSViG outperforms previous state-of-the-art action recognition models on our collected patients' video data with higher accuracy (5.9% error), lower FLOPs (0.4G), and smaller model size (1.4M). Furthermore, by integrating a decision-making rule that combines output probabilities and an accumulative function, we achieve a 5.1 s detection latency after EEG onset, a 13.1 s detection advance before clinical onset, and a zero false detection rate. The project homepage is available at: https://github.com/xuyankun/VSViG/
<div id='section'>Paperid: <span id='pid'>35, <a href='https://arxiv.org/pdf/2311.13444.pdf' target='_blank'>https://arxiv.org/pdf/2311.13444.pdf</a></span>   <span><a href='https://github.com/ShiqiYu/OpenGait' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chao Fan, Jingzhe Ma, Dongyang Jin, Chuanfu Shen, Shiqi Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.13444">SkeletonGait: Gait Recognition Using Skeleton Maps</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The choice of the representations is essential for deep gait recognition methods. The binary silhouettes and skeletal coordinates are two dominant representations in recent literature, achieving remarkable advances in many scenarios. However, inherent challenges remain, in which silhouettes are not always guaranteed in unconstrained scenes, and structural cues have not been fully utilized from skeletons. In this paper, we introduce a novel skeletal gait representation named skeleton map, together with SkeletonGait, a skeleton-based method to exploit structural information from human skeleton maps. Specifically, the skeleton map represents the coordinates of human joints as a heatmap with Gaussian approximation, exhibiting a silhouette-like image devoid of exact body structure. Beyond achieving state-of-the-art performances over five popular gait datasets, more importantly, SkeletonGait uncovers novel insights about how important structural features are in describing gait and when they play a role. Furthermore, we propose a multi-branch architecture, named SkeletonGait++, to make use of complementary features from both skeletons and silhouettes. Experiments indicate that SkeletonGait++ outperforms existing state-of-the-art methods by a significant margin in various scenarios. For instance, it achieves an impressive rank-1 accuracy of over 85% on the challenging GREW dataset. All the source code is available at https://github.com/ShiqiYu/OpenGait.
<div id='section'>Paperid: <span id='pid'>36, <a href='https://arxiv.org/pdf/2311.03106.pdf' target='_blank'>https://arxiv.org/pdf/2311.03106.pdf</a></span>   <span><a href='https://github.com/HuiGuanLab/UmURL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengkai Sun, Daizong Liu, Jianfeng Dong, Xiaoye Qu, Junyu Gao, Xun Yang, Xun Wang, Meng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.03106">Unified Multi-modal Unsupervised Representation Learning for Skeleton-based Action Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsupervised pre-training has shown great success in skeleton-based action understanding recently. Existing works typically train separate modality-specific models, then integrate the multi-modal information for action understanding by a late-fusion strategy. Although these approaches have achieved significant performance, they suffer from the complex yet redundant multi-stream model designs, each of which is also limited to the fixed input skeleton modality. To alleviate these issues, in this paper, we propose a Unified Multimodal Unsupervised Representation Learning framework, called UmURL, which exploits an efficient early-fusion strategy to jointly encode the multi-modal features in a single-stream manner. Specifically, instead of designing separate modality-specific optimization processes for uni-modal unsupervised learning, we feed different modality inputs into the same stream with an early-fusion strategy to learn their multi-modal features for reducing model complexity. To ensure that the fused multi-modal features do not exhibit modality bias, i.e., being dominated by a certain modality input, we further propose both intra- and inter-modal consistency learning to guarantee that the multi-modal features contain the complete semantics of each modal via feature decomposition and distinct alignment. In this manner, our framework is able to learn the unified representations of uni-modal or multi-modal skeleton input, which is flexible to different kinds of modality input for robust action understanding in practical cases. Extensive experiments conducted on three large-scale datasets, i.e., NTU-60, NTU-120, and PKU-MMD II, demonstrate that UmURL is highly efficient, possessing the approximate complexity with the uni-modal methods, while achieving new state-of-the-art performance across various downstream task scenarios in skeleton-based action representation learning.
<div id='section'>Paperid: <span id='pid'>37, <a href='https://arxiv.org/pdf/2310.10547.pdf' target='_blank'>https://arxiv.org/pdf/2310.10547.pdf</a></span>   <span><a href='https://github.com/stnoah1/infogcn2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Seunggeun Chi, Hyung-gun Chi, Qixing Huang, Karthik Ramani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.10547">InfoGCN++: Learning Representation by Predicting the Future for Online Human Skeleton-based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeleton-based action recognition has made significant advancements recently, with models like InfoGCN showcasing remarkable accuracy. However, these models exhibit a key limitation: they necessitate complete action observation prior to classification, which constrains their applicability in real-time situations such as surveillance and robotic systems. To overcome this barrier, we introduce InfoGCN++, an innovative extension of InfoGCN, explicitly developed for online skeleton-based action recognition. InfoGCN++ augments the abilities of the original InfoGCN model by allowing real-time categorization of action types, independent of the observation sequence's length. It transcends conventional approaches by learning from current and anticipated future movements, thereby creating a more thorough representation of the entire sequence. Our approach to prediction is managed as an extrapolation issue, grounded on observed actions. To enable this, InfoGCN++ incorporates Neural Ordinary Differential Equations, a concept that lets it effectively model the continuous evolution of hidden states. Following rigorous evaluations on three skeleton-based action recognition benchmarks, InfoGCN++ demonstrates exceptional performance in online action recognition. It consistently equals or exceeds existing techniques, highlighting its significant potential to reshape the landscape of real-time action recognition applications. Consequently, this work represents a major leap forward from InfoGCN, pushing the limits of what's possible in online, skeleton-based action recognition. The code for InfoGCN++ is publicly available at https://github.com/stnoah1/infogcn2 for further exploration and validation.
<div id='section'>Paperid: <span id='pid'>38, <a href='https://arxiv.org/pdf/2309.12029.pdf' target='_blank'>https://arxiv.org/pdf/2309.12029.pdf</a></span>   <span><a href='https://github.com/cyfml/OPSTL' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/cyfml/OPSTL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifei Chen, Kunyu Peng, Alina Roitberg, David Schneider, Jiaming Zhang, Junwei Zheng, Yufan Chen, Ruiping Liu, Kailun Yang, Rainer Stiefelhagen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.12029">Exploring Self-supervised Skeleton-based Action Recognition in Occluded Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To integrate action recognition into autonomous robotic systems, it is essential to address challenges such as person occlusions-a common yet often overlooked scenario in existing self-supervised skeleton-based action recognition methods. In this work, we propose IosPSTL, a simple and effective self-supervised learning framework designed to handle occlusions. IosPSTL combines a cluster-agnostic KNN imputer with an Occluded Partial Spatio-Temporal Learning (OPSTL) strategy. First, we pre-train the model on occluded skeleton sequences. Then, we introduce a cluster-agnostic KNN imputer that performs semantic grouping using k-means clustering on sequence embeddings. It imputes missing skeleton data by applying K-Nearest Neighbors in the latent space, leveraging nearby sample representations to restore occluded joints. This imputation generates more complete skeleton sequences, which significantly benefits downstream self-supervised models. To further enhance learning, the OPSTL module incorporates Adaptive Spatial Masking (ASM) to make better use of intact, high-quality skeleton sequences during training. Our method achieves state-of-the-art performance on the occluded versions of the NTU-60 and NTU-120 datasets, demonstrating its robustness and effectiveness under challenging conditions. Code is available at https://github.com/cyfml/OPSTL.
<div id='section'>Paperid: <span id='pid'>39, <a href='https://arxiv.org/pdf/2309.12009.pdf' target='_blank'>https://arxiv.org/pdf/2309.12009.pdf</a></span>   <span><a href='https://github.com/desehuileng0o0/IKEM' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/desehuileng0o0/IKEM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiping Wei, Kunyu Peng, Alina Roitberg, Jiaming Zhang, Junwei Zheng, Ruiping Liu, Yufan Chen, Kailun Yang, Rainer Stiefelhagen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.12009">Elevating Skeleton-Based Action Recognition with Efficient Multi-Modality Self-Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-supervised representation learning for human action recognition has developed rapidly in recent years. Most of the existing works are based on skeleton data while using a multi-modality setup. These works overlooked the differences in performance among modalities, which led to the propagation of erroneous knowledge between modalities while only three fundamental modalities, i.e., joints, bones, and motions are used, hence no additional modalities are explored.
  In this work, we first propose an Implicit Knowledge Exchange Module (IKEM) which alleviates the propagation of erroneous knowledge between low-performance modalities. Then, we further propose three new modalities to enrich the complementary information between modalities. Finally, to maintain efficiency when introducing new modalities, we propose a novel teacher-student framework to distill the knowledge from the secondary modalities into the mandatory modalities considering the relationship constrained by anchors, positives, and negatives, named relational cross-modality knowledge distillation. The experimental results demonstrate the effectiveness of our approach, unlocking the efficient use of skeleton-based multi-modality data. Source code will be made publicly available at https://github.com/desehuileng0o0/IKEM.
<div id='section'>Paperid: <span id='pid'>40, <a href='https://arxiv.org/pdf/2309.09592.pdf' target='_blank'>https://arxiv.org/pdf/2309.09592.pdf</a></span>   <span><a href='https://github.com/EHZ9NIWI7/MSF-GZSSAR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ming-Zhe Li, Zhen Jia, Zhang Zhang, Zhanyu Ma, Liang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.09592">Multi-Semantic Fusion Model for Generalized Zero-Shot Skeleton-Based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalized zero-shot skeleton-based action recognition (GZSSAR) is a new challenging problem in computer vision community, which requires models to recognize actions without any training samples. Previous studies only utilize the action labels of verb phrases as the semantic prototypes for learning the mapping from skeleton-based actions to a shared semantic space. However, the limited semantic information of action labels restricts the generalization ability of skeleton features for recognizing unseen actions. In order to solve this dilemma, we propose a multi-semantic fusion (MSF) model for improving the performance of GZSSAR, where two kinds of class-level textual descriptions (i.e., action descriptions and motion descriptions), are collected as auxiliary semantic information to enhance the learning efficacy of generalizable skeleton features. Specially, a pre-trained language encoder takes the action descriptions, motion descriptions and original class labels as inputs to obtain rich semantic features for each action class, while a skeleton encoder is implemented to extract skeleton features. Then, a variational autoencoder (VAE) based generative module is performed to learn a cross-modal alignment between skeleton and semantic features. Finally, a classification module is built to recognize the action categories of input samples, where a seen-unseen classification gate is adopted to predict whether the sample comes from seen action classes or not in GZSSAR. The superior performance in comparisons with previous models validates the effectiveness of the proposed MSF model on GZSSAR.
<div id='section'>Paperid: <span id='pid'>41, <a href='https://arxiv.org/pdf/2308.16018.pdf' target='_blank'>https://arxiv.org/pdf/2308.16018.pdf</a></span>   <span><a href='https://github.com/BUPTSJZhang/SiT?MLP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaojie Zhang, Jianqin Yin, Yonghao Dang, Jiajun Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.16018">SiT-MLP: A Simple MLP with Point-wise Topology Feature Learning for Skeleton-based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph convolution networks (GCNs) have achieved remarkable performance in skeleton-based action recognition. However, previous GCN-based methods rely on elaborate human priors excessively and construct complex feature aggregation mechanisms, which limits the generalizability and effectiveness of networks. To solve these problems, we propose a novel Spatial Topology Gating Unit (STGU), an MLP-based variant without extra priors, to capture the co-occurrence topology features that encode the spatial dependency across all joints. In STGU, to learn the point-wise topology features, a new gate-based feature interaction mechanism is introduced to activate the features point-to-point by the attention map generated from the input sample. Based on the STGU, we propose the first MLP-based model, SiT-MLP, for skeleton-based action recognition in this work. Compared with previous methods on three large-scale datasets, SiT-MLP achieves competitive performance. In addition, SiT-MLP reduces the parameters significantly with favorable results. The code will be available at https://github.com/BUPTSJZhang/SiT?MLP.
<div id='section'>Paperid: <span id='pid'>42, <a href='https://arxiv.org/pdf/2308.14024.pdf' target='_blank'>https://arxiv.org/pdf/2308.14024.pdf</a></span>   <span><a href='https://github.com/firework8/BRL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongda Liu, Yunlong Wang, Min Ren, Junxing Hu, Zhengquan Luo, Guangqi Hou, Zhenan Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.14024">Balanced Representation Learning for Long-tailed Skeleton-based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeleton-based action recognition has recently made significant progress. However, data imbalance is still a great challenge in real-world scenarios. The performance of current action recognition algorithms declines sharply when training data suffers from heavy class imbalance. The imbalanced data actually degrades the representations learned by these methods and becomes the bottleneck for action recognition. How to learn unbiased representations from imbalanced action data is the key to long-tailed action recognition. In this paper, we propose a novel balanced representation learning method to address the long-tailed problem in action recognition. Firstly, a spatial-temporal action exploration strategy is presented to expand the sample space effectively, generating more valuable samples in a rebalanced manner. Secondly, we design a detached action-aware learning schedule to further mitigate the bias in the representation space. The schedule detaches the representation learning of tail classes from training and proposes an action-aware loss to impose more effective constraints. Additionally, a skip-modal representation is proposed to provide complementary structural information. The proposed method is validated on four skeleton datasets, NTU RGB+D 60, NTU RGB+D 120, NW-UCLA, and Kinetics. It not only achieves consistently large improvement compared to the state-of-the-art (SOTA) methods, but also demonstrates a superior generalization capacity through extensive experiments. Our code is available at https://github.com/firework8/BRL.
<div id='section'>Paperid: <span id='pid'>43, <a href='https://arxiv.org/pdf/2308.10557.pdf' target='_blank'>https://arxiv.org/pdf/2308.10557.pdf</a></span>   <span><a href='https://github.com/KathPra/LSHR_LSHT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Katharina Prasse, Steffen Jung, Yuxuan Zhou, Margret Keuper
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.10557">Local Spherical Harmonics Improve Skeleton-Based Hand Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hand action recognition is essential. Communication, human-robot interactions, and gesture control are dependent on it. Skeleton-based action recognition traditionally includes hands, which belong to the classes which remain challenging to correctly recognize to date. We propose a method specifically designed for hand action recognition which uses relative angular embeddings and local Spherical Harmonics to create novel hand representations. The use of Spherical Harmonics creates rotation-invariant representations which make hand action recognition even more robust against inter-subject differences and viewpoint changes. We conduct extensive experiments on the hand joints in the First-Person Hand Action Benchmark with RGB-D Videos and 3D Hand Pose Annotations, and on the NTU RGB+D 120 dataset, demonstrating the benefit of using Local Spherical Harmonics Representations. Our code is available at https://github.com/KathPra/LSHR_LSHT.
<div id='section'>Paperid: <span id='pid'>44, <a href='https://arxiv.org/pdf/2308.07571.pdf' target='_blank'>https://arxiv.org/pdf/2308.07571.pdf</a></span>   <span><a href='https://github.com/OSVAI/Ske2Grid' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongqi Cai, Yangyuxuan Kang, Anbang Yao, Yurong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.07571">Ske2Grid: Skeleton-to-Grid Representation Learning for Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents Ske2Grid, a new representation learning framework for improved skeleton-based action recognition. In Ske2Grid, we define a regular convolution operation upon a novel grid representation of human skeleton, which is a compact image-like grid patch constructed and learned through three novel designs. Specifically, we propose a graph-node index transform (GIT) to construct a regular grid patch through assigning the nodes in the skeleton graph one by one to the desired grid cells. To ensure that GIT is a bijection and enrich the expressiveness of the grid representation, an up-sampling transform (UPT) is learned to interpolate the skeleton graph nodes for filling the grid patch to the full. To resolve the problem when the one-step UPT is aggressive and further exploit the representation capability of the grid patch with increasing spatial size, a progressive learning strategy (PLS) is proposed which decouples the UPT into multiple steps and aligns them to multiple paired GITs through a compact cascaded design learned progressively. We construct networks upon prevailing graph convolution networks and conduct experiments on six mainstream skeleton-based action recognition datasets. Experiments show that our Ske2Grid significantly outperforms existing GCN-based solutions under different benchmark settings, without bells and whistles. Code and models are available at https://github.com/OSVAI/Ske2Grid
<div id='section'>Paperid: <span id='pid'>45, <a href='https://arxiv.org/pdf/2308.03950.pdf' target='_blank'>https://arxiv.org/pdf/2308.03950.pdf</a></span>   <span><a href='https://github.com/YujieOuO/SMIE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujie Zhou, Wenwen Qiang, Anyi Rao, Ning Lin, Bing Su, Jiaqi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.03950">Zero-shot Skeleton-based Action Recognition via Mutual Information Estimation and Maximization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Zero-shot skeleton-based action recognition aims to recognize actions of unseen categories after training on data of seen categories. The key is to build the connection between visual and semantic space from seen to unseen classes. Previous studies have primarily focused on encoding sequences into a singular feature vector, with subsequent mapping the features to an identical anchor point within the embedded space. Their performance is hindered by 1) the ignorance of the global visual/semantic distribution alignment, which results in a limitation to capture the true interdependence between the two spaces. 2) the negligence of temporal information since the frame-wise features with rich action clues are directly pooled into a single feature vector. We propose a new zero-shot skeleton-based action recognition method via mutual information (MI) estimation and maximization. Specifically, 1) we maximize the MI between visual and semantic space for distribution alignment; 2) we leverage the temporal information for estimating the MI by encouraging MI to increase as more frames are observed. Extensive experiments on three large-scale skeleton action datasets confirm the effectiveness of our method. Code: https://github.com/YujieOuO/SMIE.
<div id='section'>Paperid: <span id='pid'>46, <a href='https://arxiv.org/pdf/2307.08476.pdf' target='_blank'>https://arxiv.org/pdf/2307.08476.pdf</a></span>   <span><a href='https://github.com/HongYan1123/SkeletonMAE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hong Yan, Yang Liu, Yushen Wei, Zhen Li, Guanbin Li, Liang Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.08476">SkeletonMAE: Graph-based Masked Autoencoder for Skeleton Sequence Pre-training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeleton sequence representation learning has shown great advantages for action recognition due to its promising ability to model human joints and topology. However, the current methods usually require sufficient labeled data for training computationally expensive models, which is labor-intensive and time-consuming. Moreover, these methods ignore how to utilize the fine-grained dependencies among different skeleton joints to pre-train an efficient skeleton sequence learning model that can generalize well across different datasets. In this paper, we propose an efficient skeleton sequence learning framework, named Skeleton Sequence Learning (SSL). To comprehensively capture the human pose and obtain discriminative skeleton sequence representation, we build an asymmetric graph-based encoder-decoder pre-training architecture named SkeletonMAE, which embeds skeleton joint sequence into Graph Convolutional Network (GCN) and reconstructs the masked skeleton joints and edges based on the prior human topology knowledge. Then, the pre-trained SkeletonMAE encoder is integrated with the Spatial-Temporal Representation Learning (STRL) module to build the SSL framework. Extensive experimental results show that our SSL generalizes well across different datasets and outperforms the state-of-the-art self-supervised skeleton-based action recognition methods on FineGym, Diving48, NTU 60 and NTU 120 datasets. Additionally, we obtain comparable performance to some fully supervised methods. The code is avaliable at https://github.com/HongYan1123/SkeletonMAE.
<div id='section'>Paperid: <span id='pid'>47, <a href='https://arxiv.org/pdf/2307.07791.pdf' target='_blank'>https://arxiv.org/pdf/2307.07791.pdf</a></span>   <span><a href='https://github.com/Levigty/CMCS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengyuan Liu, Hong Liu, Tianyu Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.07791">Cross-Model Cross-Stream Learning for Self-Supervised Human Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Considering the instance-level discriminative ability, contrastive learning methods, including MoCo and SimCLR, have been adapted from the original image representation learning task to solve the self-supervised skeleton-based action recognition task. These methods usually use multiple data streams (i.e., joint, motion, and bone) for ensemble learning, meanwhile, how to construct a discriminative feature space within a single stream and effectively aggregate the information from multiple streams remains an open problem. To this end, this paper first applies a new contrastive learning method called BYOL to learn from skeleton data, and then formulate SkeletonBYOL as a simple yet effective baseline for self-supervised skeleton-based action recognition. Inspired by SkeletonBYOL, this paper further presents a Cross-Model and Cross-Stream (CMCS) framework. This framework combines Cross-Model Adversarial Learning (CMAL) and Cross-Stream Collaborative Learning (CSCL). Specifically, CMAL learns single-stream representation by cross-model adversarial loss to obtain more discriminative features. To aggregate and interact with multi-stream information, CSCL is designed by generating similarity pseudo label of ensemble learning as supervision and guiding feature generation for individual streams. Extensive experiments on three datasets verify the complementary properties between CMAL and CSCL and also verify that the proposed method can achieve better results than state-of-the-art methods using various evaluation protocols.
<div id='section'>Paperid: <span id='pid'>48, <a href='https://arxiv.org/pdf/2304.14045.pdf' target='_blank'>https://arxiv.org/pdf/2304.14045.pdf</a></span>   <span><a href='https://github.com/xiu-cs/IGANet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ti Wang, Hong Liu, Runwei Ding, Wenhao Li, Yingxuan You, Xia Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.14045">Interweaved Graph and Attention Network for 3D Human Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite substantial progress in 3D human pose estimation from a single-view image, prior works rarely explore global and local correlations, leading to insufficient learning of human skeleton representations. To address this issue, we propose a novel Interweaved Graph and Attention Network (IGANet) that allows bidirectional communications between graph convolutional networks (GCNs) and attentions. Specifically, we introduce an IGA module, where attentions are provided with local information from GCNs and GCNs are injected with global information from attentions. Additionally, we design a simple yet effective U-shaped multi-layer perceptron (uMLP), which can capture multi-granularity information for body joints. Extensive experiments on two popular benchmark datasets (i.e. Human3.6M and MPI-INF-3DHP) are conducted to evaluate our proposed method.The results show that IGANet achieves state-of-the-art performance on both datasets. Code is available at https://github.com/xiu-cs/IGANet.
<div id='section'>Paperid: <span id='pid'>49, <a href='https://arxiv.org/pdf/2304.04023.pdf' target='_blank'>https://arxiv.org/pdf/2304.04023.pdf</a></span>   <span><a href='https://github.com/1xbq1/A2MC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Binqian Xu, Xiangbo Shu, Jiachao Zhang, Rui Yan, Guo-Sen Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.04023">Attack-Augmentation Mixing-Contrastive Skeletal Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Contrastive learning, relying on effective positive and negative sample pairs, is beneficial to learn informative skeleton representations in unsupervised skeleton-based action recognition. To achieve these positive and negative pairs, existing weak/strong data augmentation methods have to randomly change the appearance of skeletons for indirectly pursuing semantic perturbations. However, such approaches have two limitations: i) solely perturbing appearance cannot well capture the intrinsic semantic information of skeletons, and ii) randomly perturbation may change the original positive/negative pairs to soft positive/negative ones. To address the above dilemma, we start the first attempt to explore an attack-based augmentation scheme that additionally brings in direct semantic perturbation, for constructing hard positive pairs and further assisting in constructing hard negative pairs. In particular, we propose a novel Attack-Augmentation Mixing-Contrastive skeletal representation learning (A$^2$MC) to contrast hard positive features and hard negative features for learning more robust skeleton representations. In A$^2$MC, Attack-Augmentation (Att-Aug) is designed to collaboratively perform targeted and untargeted perturbations of skeletons via attack and augmentation respectively, for generating high-quality hard positive features. Meanwhile, Positive-Negative Mixer (PNM) is presented to mix hard positive features and negative features for generating hard negative features, which are adopted for updating the mixed memory banks. Extensive experiments on three public datasets demonstrate that A$^2$MC is competitive with the state-of-the-art methods. The code will be accessible on A$^2$MC (https://github.com/1xbq1/A2MC).
<div id='section'>Paperid: <span id='pid'>50, <a href='https://arxiv.org/pdf/2304.00387.pdf' target='_blank'>https://arxiv.org/pdf/2304.00387.pdf</a></span>   <span><a href='https://github.com/anshulbshah/HaLP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Anshul Shah, Aniket Roy, Ketul Shah, Shlok Kumar Mishra, David Jacobs, Anoop Cherian, Rama Chellappa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.00387">HaLP: Hallucinating Latent Positives for Skeleton-based Self-Supervised Learning of Actions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Supervised learning of skeleton sequence encoders for action recognition has received significant attention in recent times. However, learning such encoders without labels continues to be a challenging problem. While prior works have shown promising results by applying contrastive learning to pose sequences, the quality of the learned representations is often observed to be closely tied to data augmentations that are used to craft the positives. However, augmenting pose sequences is a difficult task as the geometric constraints among the skeleton joints need to be enforced to make the augmentations realistic for that action. In this work, we propose a new contrastive learning approach to train models for skeleton-based action recognition without labels. Our key contribution is a simple module, HaLP - to Hallucinate Latent Positives for contrastive learning. Specifically, HaLP explores the latent space of poses in suitable directions to generate new positives. To this end, we present a novel optimization formulation to solve for the synthetic positives with an explicit control on their hardness. We propose approximations to the objective, making them solvable in closed form with minimal overhead. We show via experiments that using these generated positives within a standard contrastive learning framework leads to consistent improvements across benchmarks such as NTU-60, NTU-120, and PKU-II on tasks like linear evaluation, transfer learning, and kNN evaluation. Our code will be made available at https://github.com/anshulbshah/HaLP.
<div id='section'>Paperid: <span id='pid'>51, <a href='https://arxiv.org/pdf/2303.10876.pdf' target='_blank'>https://arxiv.org/pdf/2303.10876.pdf</a></span>   <span><a href='https://github.com/MediaBrain-SJTU/EqMotion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenxin Xu, Robby T. Tan, Yuhong Tan, Siheng Chen, Yu Guang Wang, Xinchao Wang, Yanfeng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.10876">EqMotion: Equivariant Multi-agent Motion Prediction with Invariant Interaction Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning to predict agent motions with relationship reasoning is important for many applications. In motion prediction tasks, maintaining motion equivariance under Euclidean geometric transformations and invariance of agent interaction is a critical and fundamental principle. However, such equivariance and invariance properties are overlooked by most existing methods. To fill this gap, we propose EqMotion, an efficient equivariant motion prediction model with invariant interaction reasoning. To achieve motion equivariance, we propose an equivariant geometric feature learning module to learn a Euclidean transformable feature through dedicated designs of equivariant operations. To reason agent's interactions, we propose an invariant interaction reasoning module to achieve a more stable interaction modeling. To further promote more comprehensive motion features, we propose an invariant pattern feature learning module to learn an invariant pattern feature, which cooperates with the equivariant geometric feature to enhance network expressiveness. We conduct experiments for the proposed model on four distinct scenarios: particle dynamics, molecule dynamics, human skeleton motion prediction and pedestrian trajectory prediction. Experimental results show that our method is not only generally applicable, but also achieves state-of-the-art prediction performances on all the four tasks, improving by 24.0/30.1/8.6/9.2%. Code is available at https://github.com/MediaBrain-SJTU/EqMotion.
<div id='section'>Paperid: <span id='pid'>52, <a href='https://arxiv.org/pdf/2303.06242.pdf' target='_blank'>https://arxiv.org/pdf/2303.06242.pdf</a></span>   <span><a href='https://github.com/paolomandica/HYSP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Luca Franco, Paolo Mandica, Bharti Munjal, Fabio Galasso
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.06242">HYperbolic Self-Paced Learning for Self-Supervised Skeleton-based Action Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-paced learning has been beneficial for tasks where some initial knowledge is available, such as weakly supervised learning and domain adaptation, to select and order the training sample sequence, from easy to complex. However its applicability remains unexplored in unsupervised learning, whereby the knowledge of the task matures during training. We propose a novel HYperbolic Self-Paced model (HYSP) for learning skeleton-based action representations. HYSP adopts self-supervision: it uses data augmentations to generate two views of the same sample, and it learns by matching one (named online) to the other (the target). We propose to use hyperbolic uncertainty to determine the algorithmic learning pace, under the assumption that less uncertain samples should be more strongly driving the training, with a larger weight and pace. Hyperbolic uncertainty is a by-product of the adopted hyperbolic neural networks, it matures during training and it comes with no extra cost, compared to the established Euclidean SSL framework counterparts. When tested on three established skeleton-based action recognition datasets, HYSP outperforms the state-of-the-art on PKU-MMD I, as well as on 2 out of 3 downstream tasks on NTU-60 and NTU-120. Additionally, HYSP only uses positive pairs and bypasses therefore the complex and computationally-demanding mining procedures required for the negatives in contrastive techniques. Code is available at https://github.com/paolomandica/HYSP.
<div id='section'>Paperid: <span id='pid'>53, <a href='https://arxiv.org/pdf/2303.03729.pdf' target='_blank'>https://arxiv.org/pdf/2303.03729.pdf</a></span>   <span><a href='https://github.com/zhysora/FR-Head' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huanyu Zhou, Qingjie Liu, Yunhong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.03729">Learning Discriminative Representations for Skeleton Based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human action recognition aims at classifying the category of human action from a segment of a video. Recently, people have dived into designing GCN-based models to extract features from skeletons for performing this task, because skeleton representations are much more efficient and robust than other modalities such as RGB frames. However, when employing the skeleton data, some important clues like related items are also discarded. It results in some ambiguous actions that are hard to be distinguished and tend to be misclassified. To alleviate this problem, we propose an auxiliary feature refinement head (FR Head), which consists of spatial-temporal decoupling and contrastive feature refinement, to obtain discriminative representations of skeletons. Ambiguous samples are dynamically discovered and calibrated in the feature space. Furthermore, FR Head could be imposed on different stages of GCNs to build a multi-level refinement for stronger supervision. Extensive experiments are conducted on NTU RGB+D, NTU RGB+D 120, and NW-UCLA datasets. Our proposed models obtain competitive results from state-of-the-art methods and can help to discriminate those ambiguous samples. Codes are available at https://github.com/zhysora/FR-Head.
<div id='section'>Paperid: <span id='pid'>54, <a href='https://arxiv.org/pdf/2302.12967.pdf' target='_blank'>https://arxiv.org/pdf/2302.12967.pdf</a></span>   <span><a href='https://github.com/aikuniverse/TCTE-Net' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinzhao Luo, Lu Zhou, Guibo Zhu, Guojing Ge, Beiying Yang, Jinqiao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.12967">Temporal-Channel Topology Enhanced Network for Skeleton-Based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeleton-based action recognition has become popular in recent years due to its efficiency and robustness. Most current methods adopt graph convolutional network (GCN) for topology modeling, but GCN-based methods are limited in long-distance correlation modeling and generalizability. In contrast, the potential of convolutional neural network (CNN) for topology modeling has not been fully explored. In this paper, we propose a novel CNN architecture, Temporal-Channel Topology Enhanced Network (TCTE-Net), to learn spatial and temporal topologies for skeleton-based action recognition. The TCTE-Net consists of two modules: the Temporal-Channel Focus module, which learns a temporal-channel focus matrix to identify the most critical feature representations, and the Dynamic Channel Topology Attention module, which dynamically learns spatial topological features, and fuses them with an attention mechanism to model long-distance channel-wise topology. We conduct experiments on NTU RGB+D, NTU RGB+D 120, and FineGym datasets. TCTE-Net shows state-of-the-art performance compared to CNN-based methods and achieves superior performance compared to GCN-based methods. The code is available at https://github.com/aikuniverse/TCTE-Net.
<div id='section'>Paperid: <span id='pid'>55, <a href='https://arxiv.org/pdf/2302.09018.pdf' target='_blank'>https://arxiv.org/pdf/2302.09018.pdf</a></span>   <span><a href='https://github.com/YujieOuO/PSTL.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujie Zhou, Haodong Duan, Anyi Rao, Bing Su, Jiaqi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.09018">Self-supervised Action Representation Learning from Partial Spatio-Temporal Skeleton Sequences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-supervised learning has demonstrated remarkable capability in representation learning for skeleton-based action recognition. Existing methods mainly focus on applying global data augmentation to generate different views of the skeleton sequence for contrastive learning. However, due to the rich action clues in the skeleton sequences, existing methods may only take a global perspective to learn to discriminate different skeletons without thoroughly leveraging the local relationship between different skeleton joints and video frames, which is essential for real-world applications. In this work, we propose a Partial Spatio-Temporal Learning (PSTL) framework to exploit the local relationship from a partial skeleton sequences built by a unique spatio-temporal masking strategy. Specifically, we construct a negative-sample-free triplet steam structure that is composed of an anchor stream without any masking, a spatial masking stream with Central Spatial Masking (CSM), and a temporal masking stream with Motion Attention Temporal Masking (MATM). The feature cross-correlation matrix is measured between the anchor stream and the other two masking streams, respectively. (1) Central Spatial Masking discards selected joints from the feature calculation process, where the joints with a higher degree of centrality have a higher possibility of being selected. (2) Motion Attention Temporal Masking leverages the motion of action and remove frames that move faster with a higher possibility. Our method achieves SOTA performance on NTU-60, NTU-120 and PKU-MMD under various downstream tasks. A practical evaluation is performed where some skeleton joints are lost in downstream tasks. In contrast to previous methods that suffer from large performance drops, our PSTL can still achieve remarkable results, validating the robustness of our method. Code: https://github.com/YujieOuO/PSTL.git.
<div id='section'>Paperid: <span id='pid'>56, <a href='https://arxiv.org/pdf/2302.02327.pdf' target='_blank'>https://arxiv.org/pdf/2302.02327.pdf</a></span>   <span><a href='https://github.com/1xbq1/PSP-Learning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Binqian Xu, Xiangbo Shu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.02327">Pyramid Self-attention Polymerization Learning for Semi-supervised Skeleton-based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most semi-supervised skeleton-based action recognition approaches aim to learn the skeleton action representations only at the joint level, but neglect the crucial motion characteristics at the coarser-grained body (e.g., limb, trunk) level that provide rich additional semantic information, though the number of labeled data is limited. In this work, we propose a novel Pyramid Self-attention Polymerization Learning (dubbed as PSP Learning) framework to jointly learn body-level, part-level, and joint-level action representations of joint and motion data containing abundant and complementary semantic information via contrastive learning covering coarse-to-fine granularity. Specifically, to complement semantic information from coarse to fine granularity in skeleton actions, we design a new Pyramid Polymerizing Attention (PPA) mechanism that firstly calculates the body-level attention map, part-level attention map, and joint-level attention map, as well as polymerizes these attention maps in a level-by-level way (i.e., from body level to part level, and further to joint level). Moreover, we present a new Coarse-to-fine Contrastive Loss (CCL) including body-level contrast loss, part-level contrast loss, and joint-level contrast loss to jointly measure the similarity between the body/part/joint-level contrasting features of joint and motion data. Finally, extensive experiments are conducted on the NTU RGB+D and North-Western UCLA datasets to demonstrate the competitive performance of the proposed PSP Learning in the semi-supervised skeleton-based action recognition task. The source codes of PSP Learning are publicly available at https://github.com/1xbq1/PSP-Learning.
<div id='section'>Paperid: <span id='pid'>57, <a href='https://arxiv.org/pdf/2301.10900.pdf' target='_blank'>https://arxiv.org/pdf/2301.10900.pdf</a></span>   <span><a href='https://github.com/OliverHxh/SkeletonGCL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaohu Huang, Hao Zhou, Jian Wang, Haocheng Feng, Junyu Han, Errui Ding, Jingdong Wang, Xinggang Wang, Wenyu Liu, Bin Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.10900">Graph Contrastive Learning for Skeleton-based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the field of skeleton-based action recognition, current top-performing graph convolutional networks (GCNs) exploit intra-sequence context to construct adaptive graphs for feature aggregation. However, we argue that such context is still \textit{local} since the rich cross-sequence relations have not been explicitly investigated. In this paper, we propose a graph contrastive learning framework for skeleton-based action recognition (\textit{SkeletonGCL}) to explore the \textit{global} context across all sequences. In specific, SkeletonGCL associates graph learning across sequences by enforcing graphs to be class-discriminative, \emph{i.e.,} intra-class compact and inter-class dispersed, which improves the GCN capacity to distinguish various action patterns. Besides, two memory banks are designed to enrich cross-sequence context from two complementary levels, \emph{i.e.,} instance and semantic levels, enabling graph contrastive learning in multiple context scales. Consequently, SkeletonGCL establishes a new training paradigm, and it can be seamlessly incorporated into current GCNs. Without loss of generality, we combine SkeletonGCL with three GCNs (2S-ACGN, CTR-GCN, and InfoGCN), and achieve consistent improvements on NTU60, NTU120, and NW-UCLA benchmarks. The source code will be available at \url{https://github.com/OliverHxh/SkeletonGCL}.
<div id='section'>Paperid: <span id='pid'>58, <a href='https://arxiv.org/pdf/2210.04216.pdf' target='_blank'>https://arxiv.org/pdf/2210.04216.pdf</a></span>   <span><a href='https://github.com/erikervalid/AMPose' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongxin Lin, Yunwei Chiu, Peiyuan Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.04216">AMPose: Alternately Mixed Global-Local Attention Model for 3D Human Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The graph convolutional networks (GCNs) have been applied to model the physically connected and non-local relations among human joints for 3D human pose estimation (HPE). In addition, the purely Transformer-based models recently show promising results in video-based 3D HPE. However, the single-frame method still needs to model the physically connected relations among joints because the feature representations transformed only by global relations via the Transformer neglect information on the human skeleton. To deal with this problem, we propose a novel method in which the Transformer encoder and GCN blocks are alternately stacked, namely AMPose, to combine the global and physically connected relations among joints towards HPE. In the AMPose, the Transformer encoder is applied to connect each joint with all the other joints, while GCNs are applied to capture information on physically connected relations. The effectiveness of our proposed method is evaluated on the Human3.6M dataset. Our model also shows better generalization ability by testing on the MPI-INF-3DHP dataset. Code can be retrieved at https://github.com/erikervalid/AMPose.
<div id='section'>Paperid: <span id='pid'>59, <a href='https://arxiv.org/pdf/2209.02986.pdf' target='_blank'>https://arxiv.org/pdf/2209.02986.pdf</a></span>   <span><a href='https://github.com/ideal-idea/SAP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruijie Hou, Yanran Li, Ningyu Zhang, Yulin Zhou, Xiaosong Yang, Zhao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.02986">Shifting Perspective to See Difference: A Novel Multi-View Method for Skeleton based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeleton-based human action recognition is a longstanding challenge due to its complex dynamics. Some fine-grain details of the dynamics play a vital role in classification. The existing work largely focuses on designing incremental neural networks with more complicated adjacent matrices to capture the details of joints relationships. However, they still have difficulties distinguishing actions that have broadly similar motion patterns but belong to different categories. Interestingly, we found that the subtle differences in motion patterns can be significantly amplified and become easy for audience to distinct through specified view directions, where this property haven't been fully explored before. Drastically different from previous work, we boost the performance by proposing a conceptually simple yet effective Multi-view strategy that recognizes actions from a collection of dynamic view features. Specifically, we design a novel Skeleton-Anchor Proposal (SAP) module which contains a Multi-head structure to learn a set of views. For feature learning of different views, we introduce a novel Angle Representation to transform the actions under different views and feed the transformations into the baseline model. Our module can work seamlessly with the existing action classification model. Incorporated with baseline models, our SAP module exhibits clear performance gains on many challenging benchmarks. Moreover, comprehensive experiments show that our model consistently beats down the state-of-the-art and remains effective and robust especially when dealing with corrupted data. Related code will be available on https://github.com/ideal-idea/SAP .
<div id='section'>Paperid: <span id='pid'>60, <a href='https://arxiv.org/pdf/2208.05318.pdf' target='_blank'>https://arxiv.org/pdf/2208.05318.pdf</a></span>   <span><a href='https://github.com/MartinXM/GAP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wangmeng Xiang, Chao Li, Yuxuan Zhou, Biao Wang, Lei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.05318">Generative Action Description Prompts for Skeleton-based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeleton-based action recognition has recently received considerable attention. Current approaches to skeleton-based action recognition are typically formulated as one-hot classification tasks and do not fully exploit the semantic relations between actions. For example, "make victory sign" and "thumb up" are two actions of hand gestures, whose major difference lies in the movement of hands. This information is agnostic from the categorical one-hot encoding of action classes but could be unveiled from the action description. Therefore, utilizing action description in training could potentially benefit representation learning. In this work, we propose a Generative Action-description Prompts (GAP) approach for skeleton-based action recognition. More specifically, we employ a pre-trained large-scale language model as the knowledge engine to automatically generate text descriptions for body parts movements of actions, and propose a multi-modal training scheme by utilizing the text encoder to generate feature vectors for different body parts and supervise the skeleton encoder for action representation learning. Experiments show that our proposed GAP method achieves noticeable improvements over various baseline models without extra computation cost at inference. GAP achieves new state-of-the-arts on popular skeleton-based action recognition benchmarks, including NTU RGB+D, NTU RGB+D 120 and NW-UCLA. The source code is available at https://github.com/MartinXM/GAP.
<div id='section'>Paperid: <span id='pid'>61, <a href='https://arxiv.org/pdf/2202.11423.pdf' target='_blank'>https://arxiv.org/pdf/2202.11423.pdf</a></span>   <span><a href='https://github.com/KPeng9510/Trans4SOAR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kunyu Peng, Alina Roitberg, Kailun Yang, Jiaming Zhang, Rainer Stiefelhagen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2202.11423">Delving Deep into One-Shot Skeleton-based Action Recognition with Diverse Occlusions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Occlusions are universal disruptions constantly present in the real world. Especially for sparse representations, such as human skeletons, a few occluded points might destroy the geometrical and temporal continuity critically affecting the results. Yet, the research of data-scarce recognition from skeleton sequences, such as one-shot action recognition, does not explicitly consider occlusions despite their everyday pervasiveness. In this work, we explicitly tackle body occlusions for Skeleton-based One-shot Action Recognition (SOAR). We mainly consider two occlusion variants: 1) random occlusions and 2) more realistic occlusions caused by diverse everyday objects, which we generate by projecting the existing IKEA 3D furniture models into the camera coordinate system of the 3D skeletons with different geometric parameters. We leverage the proposed pipeline to blend out portions of skeleton sequences of the three popular action recognition datasets and formalize the first benchmark for SOAR from partially occluded body poses. Another key property of our benchmark are the more realistic occlusions generated by everyday objects, as even in standard recognition from 3D skeletons, only randomly missing joints were considered. We re-evaluate existing state-of-the-art frameworks for SOAR in the light of this new task and further introduce Trans4SOAR - a new transformer-based model which leverages three data streams and mixed attention fusion mechanism to alleviate the adverse effects caused by occlusions. While our experiments demonstrate a clear decline in accuracy with missing skeleton portions, this effect is smaller with Trans4SOAR, which outperforms other architectures on all datasets. Although we specifically focus on occlusions, Trans4SOAR additionally yields state-of-the-art in the standard SOAR without occlusion, surpassing the best published approach by 2.85% on NTU-120.
<div id='section'>Paperid: <span id='pid'>62, <a href='https://arxiv.org/pdf/2111.15129.pdf' target='_blank'>https://arxiv.org/pdf/2111.15129.pdf</a></span>   <span><a href='https://github.com/ml-postech/Skeleton-anonymization/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Saemi Moon, Myeonghyeon Kim, Zhenyue Qin, Yang Liu, Dongwoo Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2111.15129">Anonymization for Skeleton Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeleton-based action recognition attracts practitioners and researchers due to the lightweight, compact nature of datasets. Compared with RGB-video-based action recognition, skeleton-based action recognition is a safer way to protect the privacy of subjects while having competitive recognition performance. However, due to improvements in skeleton recognition algorithms as well as motion and depth sensors, more details of motion characteristics can be preserved in the skeleton dataset, leading to potential privacy leakage. We first train classifiers to categorize private information from skeleton trajectories to investigate the potential privacy leakage from skeleton datasets. Our preliminary experiments show that the gender classifier achieves 87% accuracy on average, and the re-identification classifier achieves 80% accuracy on average with three baseline models: Shift-GCN, MS-G3D, and 2s-AGCN. We propose an anonymization framework based on adversarial learning to protect potential privacy leakage from the skeleton dataset. Experimental results show that an anonymized dataset can reduce the risk of privacy leakage while having marginal effects on action recognition performance even with simple anonymizer architectures. The code used in our experiments is available at https://github.com/ml-postech/Skeleton-anonymization/
<div id='section'>Paperid: <span id='pid'>63, <a href='https://arxiv.org/pdf/2111.11927.pdf' target='_blank'>https://arxiv.org/pdf/2111.11927.pdf</a></span>   <span><a href='https://github.com/qingshi9974/BMVC2021-Hierarchical-Graph-Networks-for-3D-Human-Pose-Estimation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Han Li, Bowen Shi, Wenrui Dai, Yabo Chen, Botao Wang, Yu Sun, Min Guo, Chenlin Li, Junni Zou, Hongkai Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2111.11927">Hierarchical Graph Networks for 3D Human Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent 2D-to-3D human pose estimation works tend to utilize the graph structure formed by the topology of the human skeleton. However, we argue that this skeletal topology is too sparse to reflect the body structure and suffer from serious 2D-to-3D ambiguity problem. To overcome these weaknesses, we propose a novel graph convolution network architecture, Hierarchical Graph Networks (HGN). It is based on denser graph topology generated by our multi-scale graph structure building strategy, thus providing more delicate geometric information. The proposed architecture contains three sparse-to-fine representation subnetworks organized in parallel, in which multi-scale graph-structured features are processed and exchange information through a novel feature fusion strategy, leading to rich hierarchical representations. We also introduce a 3D coarse mesh constraint to further boost detail-related feature learning. Extensive experiments demonstrate that our HGN achieves the state-of-the art performance with reduced network parameters. Code is released at https://github.com/qingshi9974/BMVC2021-Hierarchical-Graph-Networks-for-3D-Human-Pose-Estimation.
<div id='section'>Paperid: <span id='pid'>64, <a href='https://arxiv.org/pdf/2203.08133.pdf' target='_blank'>https://arxiv.org/pdf/2203.08133.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sida Peng, Zhen Xu, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Hujun Bao, Xiaowei Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.08133">Animatable Implicit Neural Representations for Creating Realistic Avatars from Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the challenge of reconstructing an animatable human model from a multi-view video. Some recent works have proposed to decompose a non-rigidly deforming scene into a canonical neural radiance field and a set of deformation fields that map observation-space points to the canonical space, thereby enabling them to learn the dynamic scene from images. However, they represent the deformation field as translational vector field or SE(3) field, which makes the optimization highly under-constrained. Moreover, these representations cannot be explicitly controlled by input motions. Instead, we introduce a pose-driven deformation field based on the linear blend skinning algorithm, which combines the blend weight field and the 3D human skeleton to produce observation-to-canonical correspondences. Since 3D human skeletons are more observable, they can regularize the learning of the deformation field. Moreover, the pose-driven deformation field can be controlled by input skeletal motions to generate new deformation fields to animate the canonical human model. Experiments show that our approach significantly outperforms recent human modeling methods. The code is available at https://zju3dv.github.io/animatable_nerf/.
<div id='section'>Paperid: <span id='pid'>65, <a href='https://arxiv.org/pdf/2508.14889.pdf' target='_blank'>https://arxiv.org/pdf/2508.14889.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mert Kiray, Alvaro Ritter, Nassir Navab, Benjamin Busam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14889">MS-CLR: Multi-Skeleton Contrastive Learning for Human Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Contrastive learning has gained significant attention in skeleton-based action recognition for its ability to learn robust representations from unlabeled data. However, existing methods rely on a single skeleton convention, which limits their ability to generalize across datasets with diverse joint structures and anatomical coverage. We propose Multi-Skeleton Contrastive Learning (MS-CLR), a general self-supervised framework that aligns pose representations across multiple skeleton conventions extracted from the same sequence. This encourages the model to learn structural invariances and capture diverse anatomical cues, resulting in more expressive and generalizable features. To support this, we adapt the ST-GCN architecture to handle skeletons with varying joint layouts and scales through a unified representation scheme. Experiments on the NTU RGB+D 60 and 120 datasets demonstrate that MS-CLR consistently improves performance over strong single-skeleton contrastive learning baselines. A multi-skeleton ensemble further boosts performance, setting new state-of-the-art results on both datasets.
<div id='section'>Paperid: <span id='pid'>66, <a href='https://arxiv.org/pdf/2312.05830.pdf' target='_blank'>https://arxiv.org/pdf/2312.05830.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunheng Li, Zhongyu Li, Shanghua Gao, Qilong Wang, Qibin Hou, Ming-Ming Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.05830">A Decoupled Spatio-Temporal Framework for Skeleton-based Action Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effectively modeling discriminative spatio-temporal information is essential for segmenting activities in long action sequences. However, we observe that existing methods are limited in weak spatio-temporal modeling capability due to two forms of decoupled modeling: (i) cascaded interaction couples spatial and temporal modeling, which over-smooths motion modeling over the long sequence, and (ii) joint-shared temporal modeling adopts shared weights to model each joint, ignoring the distinct motion patterns of different joints. We propose a Decoupled Spatio-Temporal Framework (DeST) to address the above issues. Firstly, we decouple the cascaded spatio-temporal interaction to avoid stacking multiple spatio-temporal blocks, while achieving sufficient spatio-temporal interaction. Specifically, DeST performs once unified spatial modeling and divides the spatial features into different groups of subfeatures, which then adaptively interact with temporal features from different layers. Since the different sub-features contain distinct spatial semantics, the model could learn the optimal interaction pattern at each layer. Meanwhile, inspired by the fact that different joints move at different speeds, we propose joint-decoupled temporal modeling, which employs independent trainable weights to capture distinctive temporal features of each joint. On four large-scale benchmarks of different scenes, DeST significantly outperforms current state-of-the-art methods with less computational complexity.
<div id='section'>Paperid: <span id='pid'>67, <a href='https://arxiv.org/pdf/2404.07188.pdf' target='_blank'>https://arxiv.org/pdf/2404.07188.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bingyi Zhang, Rajgopal Kannan, Carl Busart, Viktor Prasanna
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.07188">GCV-Turbo: End-to-end Acceleration of GNN-based Computer Vision Tasks on FPGA</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph neural networks (GNNs) have recently empowered various novel computer vision (CV) tasks. In GNN-based CV tasks, a combination of CNN layers and GNN layers or only GNN layers are employed. This paper introduces GCV-Turbo, a domain-specific accelerator on FPGA for end-to-end acceleration of GNN-based CV tasks. GCV-Turbo consists of two key components: (1) a \emph{novel} hardware architecture optimized for the computation kernels in both CNNs and GNNs using the same set of computation resources. (2) a PyTorch-compatible compiler that takes a user-defined model as input, performs end-to-end optimization for the computation graph of a given GNN-based CV task, and produces optimized code for hardware execution. The hardware architecture and the compiler work synergistically to support a variety of GNN-based CV tasks. We implement GCV-Turbo on a state-of-the-art FPGA and evaluate its performance across six representative GNN-based CV tasks with diverse input data modalities (e.g., image, human skeleton, point cloud). Compared with state-of-the-art CPU (GPU) implementations, GCV-Turbo achieves an average latency reduction of $68.4\times$ ($4.1\times$) on these six GNN-based CV tasks. Moreover, GCV-Turbo supports the execution of the standalone CNNs or GNNs, achieving performance comparable to that of state-of-the-art CNN (GNN) accelerators for widely used CNN-only (GNN-only) models.
<div id='section'>Paperid: <span id='pid'>68, <a href='https://arxiv.org/pdf/2404.09927.pdf' target='_blank'>https://arxiv.org/pdf/2404.09927.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Bi, Cheng Qian, Zhicheng Zhang, Nassir Navab, Zhongliang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.09927">Autonomous Path Planning for Intercostal Robotic Ultrasound Imaging Using Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ultrasound (US) has been widely used in daily clinical practice for screening internal organs and guiding interventions. However, due to the acoustic shadow cast by the subcutaneous rib cage, the US examination for thoracic application is still challenging. To fully cover and reconstruct the region of interest in US for diagnosis, an intercostal scanning path is necessary. To tackle this challenge, we present a reinforcement learning (RL) approach for planning scanning paths between ribs to monitor changes in lesions on internal organs, such as the liver and heart, which are covered by rib cages. Structured anatomical information of the human skeleton is crucial for planning these intercostal paths. To obtain such anatomical insight, an RL agent is trained in a virtual environment constructed using computational tomography (CT) templates with randomly initialized tumors of various shapes and locations. In addition, task-specific state representation and reward functions are introduced to ensure the convergence of the training process while minimizing the effects of acoustic attenuation and shadows during scanning. To validate the effectiveness of the proposed approach, experiments have been carried out on unseen CTs with randomly defined single or multiple scanning targets. The results demonstrate the efficiency of the proposed RL framework in planning non-shadowed US scanning trajectories in areas with limited acoustic access.
<div id='section'>Paperid: <span id='pid'>69, <a href='https://arxiv.org/pdf/2407.19981.pdf' target='_blank'>https://arxiv.org/pdf/2407.19981.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chao Liu, Xin Liu, Zitong Yu, Yonghong Hou, Huanjing Yue, Jingyu Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.19981">Adversarial Robustness in RGB-Skeleton Action Recognition: Leveraging Attention Modality Reweighter</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks (DNNs) have been applied in many computer vision tasks and achieved state-of-the-art (SOTA) performance. However, misclassification will occur when DNNs predict adversarial examples which are created by adding human-imperceptible adversarial noise to natural examples. This limits the application of DNN in security-critical fields. In order to enhance the robustness of models, previous research has primarily focused on the unimodal domain, such as image recognition and video understanding. Although multi-modal learning has achieved advanced performance in various tasks, such as action recognition, research on the robustness of RGB-skeleton action recognition models is scarce. In this paper, we systematically investigate how to improve the robustness of RGB-skeleton action recognition models. We initially conducted empirical analysis on the robustness of different modalities and observed that the skeleton modality is more robust than the RGB modality. Motivated by this observation, we propose the \formatword{A}ttention-based \formatword{M}odality \formatword{R}eweighter (\formatword{AMR}), which utilizes an attention layer to re-weight the two modalities, enabling the model to learn more robust features. Our AMR is plug-and-play, allowing easy integration with multimodal models. To demonstrate the effectiveness of AMR, we conducted extensive experiments on various datasets. For example, compared to the SOTA methods, AMR exhibits a 43.77\% improvement against PGD20 attacks on the NTU-RGB+D 60 dataset. Furthermore, it effectively balances the differences in robustness between different modalities.
<div id='section'>Paperid: <span id='pid'>70, <a href='https://arxiv.org/pdf/2304.12281.pdf' target='_blank'>https://arxiv.org/pdf/2304.12281.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jia-Wei Liu, Yan-Pei Cao, Tianyuan Yang, Eric Zhongcong Xu, Jussi Keppo, Ying Shan, Xiaohu Qie, Mike Zheng Shou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.12281">HOSNeRF: Dynamic Human-Object-Scene Neural Radiance Fields from a Single Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce HOSNeRF, a novel 360Â° free-viewpoint rendering method that reconstructs neural radiance fields for dynamic human-object-scene from a single monocular in-the-wild video. Our method enables pausing the video at any frame and rendering all scene details (dynamic humans, objects, and backgrounds) from arbitrary viewpoints. The first challenge in this task is the complex object motions in human-object interactions, which we tackle by introducing the new object bones into the conventional human skeleton hierarchy to effectively estimate large object deformations in our dynamic human-object model. The second challenge is that humans interact with different objects at different times, for which we introduce two new learnable object state embeddings that can be used as conditions for learning our human-object representation and scene representation, respectively. Extensive experiments show that HOSNeRF significantly outperforms SOTA approaches on two challenging datasets by a large margin of 40% ~ 50% in terms of LPIPS. The code, data, and compelling examples of 360Â° free-viewpoint renderings from single videos will be released in https://showlab.github.io/HOSNeRF.
<div id='section'>Paperid: <span id='pid'>71, <a href='https://arxiv.org/pdf/2302.07408.pdf' target='_blank'>https://arxiv.org/pdf/2302.07408.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Han Li, Bowen Shi, Wenrui Dai, Hongwei Zheng, Botao Wang, Yu Sun, Min Guo, Chenlin Li, Junni Zou, Hongkai Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.07408">Pose-Oriented Transformer with Uncertainty-Guided Refinement for 2D-to-3D Human Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There has been a recent surge of interest in introducing transformers to 3D human pose estimation (HPE) due to their powerful capabilities in modeling long-term dependencies. However, existing transformer-based methods treat body joints as equally important inputs and ignore the prior knowledge of human skeleton topology in the self-attention mechanism. To tackle this issue, in this paper, we propose a Pose-Oriented Transformer (POT) with uncertainty guided refinement for 3D HPE. Specifically, we first develop novel pose-oriented self-attention mechanism and distance-related position embedding for POT to explicitly exploit the human skeleton topology. The pose-oriented self-attention mechanism explicitly models the topological interactions between body joints, whereas the distance-related position embedding encodes the distance of joints to the root joint to distinguish groups of joints with different difficulties in regression. Furthermore, we present an Uncertainty-Guided Refinement Network (UGRN) to refine pose predictions from POT, especially for the difficult joints, by considering the estimated uncertainty of each joint with uncertainty-guided sampling strategy and self-attention mechanism. Extensive experiments demonstrate that our method significantly outperforms the state-of-the-art methods with reduced model parameters on 3D HPE benchmarks such as Human3.6M and MPI-INF-3DHP
<div id='section'>Paperid: <span id='pid'>72, <a href='https://arxiv.org/pdf/2212.04761.pdf' target='_blank'>https://arxiv.org/pdf/2212.04761.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jungho Lee, Minhyeok Lee, Suhwan Cho, Sungmin Woo, Sungjun Jang, Sangyoun Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.04761">Leveraging Spatio-Temporal Dependency for Skeleton-Based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeleton-based action recognition has attracted considerable attention due to its compact representation of the human body's skeletal sructure. Many recent methods have achieved remarkable performance using graph convolutional networks (GCNs) and convolutional neural networks (CNNs), which extract spatial and temporal features, respectively. Although spatial and temporal dependencies in the human skeleton have been explored separately, spatio-temporal dependency is rarely considered. In this paper, we propose the Spatio-Temporal Curve Network (STC-Net) to effectively leverage the spatio-temporal dependency of the human skeleton. Our proposed network consists of two novel elements: 1) The Spatio-Temporal Curve (STC) module; and 2) Dilated Kernels for Graph Convolution (DK-GC). The STC module dynamically adjusts the receptive field by identifying meaningful node connections between every adjacent frame and generating spatio-temporal curves based on the identified node connections, providing an adaptive spatio-temporal coverage. In addition, we propose DK-GC to consider long-range dependencies, which results in a large receptive field without any additional parameters by applying an extended kernel to the given adjacency matrices of the graph. Our STC-Net combines these two modules and achieves state-of-the-art performance on four skeleton-based action recognition benchmarks.
<div id='section'>Paperid: <span id='pid'>73, <a href='https://arxiv.org/pdf/2208.10741.pdf' target='_blank'>https://arxiv.org/pdf/2208.10741.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jungho Lee, Minhyeok Lee, Dogyoon Lee, Sangyoun Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.10741">Hierarchically Decomposed Graph Convolutional Networks for Skeleton-Based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph convolutional networks (GCNs) are the most commonly used methods for skeleton-based action recognition and have achieved remarkable performance. Generating adjacency matrices with semantically meaningful edges is particularly important for this task, but extracting such edges is challenging problem. To solve this, we propose a hierarchically decomposed graph convolutional network (HD-GCN) architecture with a novel hierarchically decomposed graph (HD-Graph). The proposed HD-GCN effectively decomposes every joint node into several sets to extract major structurally adjacent and distant edges, and uses them to construct an HD-Graph containing those edges in the same semantic spaces of a human skeleton. In addition, we introduce an attention-guided hierarchy aggregation (A-HA) module to highlight the dominant hierarchical edge sets of the HD-Graph. Furthermore, we apply a new six-way ensemble method, which uses only joint and bone stream without any motion stream. The proposed model is evaluated and achieves state-of-the-art performance on four large, popular datasets. Finally, we demonstrate the effectiveness of our model with various comparative experiments.
<div id='section'>Paperid: <span id='pid'>74, <a href='https://arxiv.org/pdf/2404.16359.pdf' target='_blank'>https://arxiv.org/pdf/2404.16359.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cong Wu, Xiao-Jun Wu, Tianyang Xu, Josef Kittler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.16359">An Improved Graph Pooling Network for Skeleton-Based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pooling is a crucial operation in computer vision, yet the unique structure of skeletons hinders the application of existing pooling strategies to skeleton graph modelling. In this paper, we propose an Improved Graph Pooling Network, referred to as IGPN. The main innovations include: Our method incorporates a region-awareness pooling strategy based on structural partitioning. The correlation matrix of the original feature is used to adaptively adjust the weight of information in different regions of the newly generated features, resulting in more flexible and effective processing. To prevent the irreversible loss of discriminative information, we propose a cross fusion module and an information supplement module to provide block-level and input-level information respectively. As a plug-and-play structure, the proposed operation can be seamlessly combined with existing GCN-based models. We conducted extensive evaluations on several challenging benchmarks, and the experimental results indicate the effectiveness of our proposed solutions. For example, in the cross-subject evaluation of the NTU-RGB+D 60 dataset, IGPN achieves a significant improvement in accuracy compared to the baseline while reducing Flops by nearly 70%; a heavier version has also been introduced to further boost accuracy.
<div id='section'>Paperid: <span id='pid'>75, <a href='https://arxiv.org/pdf/2309.05834.pdf' target='_blank'>https://arxiv.org/pdf/2309.05834.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cong Wu, Xiao-Jun Wu, Josef Kittler, Tianyang Xu, Sara Atito, Muhammad Awais, Zhenhua Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.05834">SCD-Net: Spatiotemporal Clues Disentanglement Network for Self-supervised Skeleton-based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Contrastive learning has achieved great success in skeleton-based action recognition. However, most existing approaches encode the skeleton sequences as entangled spatiotemporal representations and confine the contrasts to the same level of representation. Instead, this paper introduces a novel contrastive learning framework, namely Spatiotemporal Clues Disentanglement Network (SCD-Net). Specifically, we integrate the decoupling module with a feature extractor to derive explicit clues from spatial and temporal domains respectively. As for the training of SCD-Net, with a constructed global anchor, we encourage the interaction between the anchor and extracted clues. Further, we propose a new masking strategy with structural constraints to strengthen the contextual associations, leveraging the latest development from masked image modelling into the proposed SCD-Net. We conduct extensive evaluations on the NTU-RGB+D (60&120) and PKU-MMD (I&II) datasets, covering various downstream tasks such as action recognition, action retrieval, transfer learning, and semi-supervised learning. The experimental results demonstrate the effectiveness of our method, which outperforms the existing state-of-the-art (SOTA) approaches significantly.
<div id='section'>Paperid: <span id='pid'>76, <a href='https://arxiv.org/pdf/2509.07335.pdf' target='_blank'>https://arxiv.org/pdf/2509.07335.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haiqing Ren, Zhongkai Luo, Heng Fan, Xiaohui Yuan, Guanchen Wang, Libo Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07335">G3CN: Gaussian Topology Refinement Gated Graph Convolutional Network for Skeleton-Based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph Convolutional Networks (GCNs) have proven to be highly effective for skeleton-based action recognition, primarily due to their ability to leverage graph topology for feature aggregation, a key factor in extracting meaningful representations. However, despite their success, GCNs often struggle to effectively distinguish between ambiguous actions, revealing limitations in the representation of learned topological and spatial features. To address this challenge, we propose a novel approach, Gaussian Topology Refinement Gated Graph Convolution (G$^{3}$CN), to address the challenge of distinguishing ambiguous actions in skeleton-based action recognition. G$^{3}$CN incorporates a Gaussian filter to refine the skeleton topology graph, improving the representation of ambiguous actions. Additionally, Gated Recurrent Units (GRUs) are integrated into the GCN framework to enhance information propagation between skeleton points. Our method shows strong generalization across various GCN backbones. Extensive experiments on NTU RGB+D, NTU RGB+D 120, and NW-UCLA benchmarks demonstrate that G$^{3}$CN effectively improves action recognition, particularly for ambiguous samples.
<div id='section'>Paperid: <span id='pid'>77, <a href='https://arxiv.org/pdf/2407.08572.pdf' target='_blank'>https://arxiv.org/pdf/2407.08572.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunfeng Diao, Baiqi Wu, Ruixuan Zhang, Xun Yang, Meng Wang, He Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08572">Boosting Adversarial Transferability for Skeleton-based Action Recognition via Exploring the Model Posterior Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeletal motion plays a pivotal role in human activity recognition (HAR). Recently, attack methods have been proposed to identify the universal vulnerability of skeleton-based HAR(S-HAR). However, the research of adversarial transferability on S-HAR is largely missing. More importantly, existing attacks all struggle in transfer across unknown S-HAR models. We observed that the key reason is that the loss landscape of the action recognizers is rugged and sharp. Given the established correlation in prior studies~\cite{qin2022boosting,wu2020towards} between loss landscape and adversarial transferability, we assume and empirically validate that smoothing the loss landscape could potentially improve adversarial transferability on S-HAR. This is achieved by proposing a new post-train Dual Bayesian strategy, which can effectively explore the model posterior space for a collection of surrogates without the need for re-training. Furthermore, to craft adversarial examples along the motion manifold, we incorporate the attack gradient with information of the motion dynamics in a Bayesian manner. Evaluated on benchmark datasets, e.g. HDM05 and NTU 60, the average transfer success rate can reach as high as 35.9\% and 45.5\% respectively. In comparison, current state-of-the-art skeletal attacks achieve only 3.6\% and 9.8\%. The high adversarial transferability remains consistent across various surrogate, victim, and even defense models. Through a comprehensive analysis of the results, we provide insights on what surrogates are more likely to exhibit transferability, to shed light on future research.
<div id='section'>Paperid: <span id='pid'>78, <a href='https://arxiv.org/pdf/2406.00639.pdf' target='_blank'>https://arxiv.org/pdf/2406.00639.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haojun Xu, Yan Gao, Jie Li, Xinbo Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00639">An Information Compensation Framework for Zero-Shot Skeleton-based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Zero-shot human skeleton-based action recognition aims to construct a model that can recognize actions outside the categories seen during training. Previous research has focused on aligning sequences' visual and semantic spatial distributions. However, these methods extract semantic features simply. They ignore that proper prompt design for rich and fine-grained action cues can provide robust representation space clustering. In order to alleviate the problem of insufficient information available for skeleton sequences, we design an information compensation learning framework from an information-theoretic perspective to improve zero-shot action recognition accuracy with a multi-granularity semantic interaction mechanism. Inspired by ensemble learning, we propose a multi-level alignment (MLA) approach to compensate information for action classes. MLA aligns multi-granularity embeddings with visual embedding through a multi-head scoring mechanism to distinguish semantically similar action names and visually similar actions. Furthermore, we introduce a new loss function sampling method to obtain a tight and robust representation. Finally, these multi-granularity semantic embeddings are synthesized to form a proper decision surface for classification. Significant action recognition performance is achieved when evaluated on the challenging NTU RGB+D, NTU RGB+D 120, and PKU-MMD benchmarks and validate that multi-granularity semantic features facilitate the differentiation of action clusters with similar visual features.
<div id='section'>Paperid: <span id='pid'>79, <a href='https://arxiv.org/pdf/2405.07444.pdf' target='_blank'>https://arxiv.org/pdf/2405.07444.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Clinton Mo, Kun Hu, Chengjiang Long, Dong Yuan, Zhiyong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.07444">Motion Keyframe Interpolation for Any Human Skeleton via Temporally Consistent Point Cloud Sampling and Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the character animation field, modern supervised keyframe interpolation models have demonstrated exceptional performance in constructing natural human motions from sparse pose definitions. As supervised models, large motion datasets are necessary to facilitate the learning process; however, since motion is represented with fixed hierarchical skeletons, such datasets are incompatible for skeletons outside the datasets' native configurations. Consequently, the expected availability of a motion dataset for desired skeletons severely hinders the feasibility of learned interpolation in practice. To combat this limitation, we propose Point Cloud-based Motion Representation Learning (PC-MRL), an unsupervised approach to enabling cross-compatibility between skeletons for motion interpolation learning. PC-MRL consists of a skeleton obfuscation strategy using temporal point cloud sampling, and an unsupervised skeleton reconstruction method from point clouds. We devise a temporal point-wise K-nearest neighbors loss for unsupervised learning. Moreover, we propose First-frame Offset Quaternion (FOQ) and Rest Pose Augmentation (RPA) strategies to overcome necessary limitations of our unsupervised point cloud-to-skeletal motion process. Comprehensive experiments demonstrate the effectiveness of PC-MRL in motion interpolation for desired skeletons without supervision from native datasets.
<div id='section'>Paperid: <span id='pid'>80, <a href='https://arxiv.org/pdf/2305.12398.pdf' target='_blank'>https://arxiv.org/pdf/2305.12398.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haojun Xu, Yan Gao, Zheng Hui, Jie Li, Xinbo Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.12398">Language Knowledge-Assisted Representation Learning for Skeleton-Based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>How humans understand and recognize the actions of others is a complex neuroscientific problem that involves a combination of cognitive mechanisms and neural networks. Research has shown that humans have brain areas that recognize actions that process top-down attentional information, such as the temporoparietal association area. Also, humans have brain regions dedicated to understanding the minds of others and analyzing their intentions, such as the medial prefrontal cortex of the temporal lobe. Skeleton-based action recognition creates mappings for the complex connections between the human skeleton movement patterns and behaviors. Although existing studies encoded meaningful node relationships and synthesized action representations for classification with good results, few of them considered incorporating a priori knowledge to aid potential representation learning for better performance. LA-GCN proposes a graph convolution network using large-scale language models (LLM) knowledge assistance. First, the LLM knowledge is mapped into a priori global relationship (GPR) topology and a priori category relationship (CPR) topology between nodes. The GPR guides the generation of new "bone" representations, aiming to emphasize essential node information from the data level. The CPR mapping simulates category prior knowledge in human brain regions, encoded by the PC-AC module and used to add additional supervision-forcing the model to learn class-distinguishable features. In addition, to improve information transfer efficiency in topology modeling, we propose multi-hop attention graph convolution. It aggregates each node's k-order neighbor simultaneously to speed up model convergence. LA-GCN reaches state-of-the-art on NTU RGB+D, NTU RGB+D 120, and NW-UCLA datasets.
<div id='section'>Paperid: <span id='pid'>81, <a href='https://arxiv.org/pdf/2411.11288.pdf' target='_blank'>https://arxiv.org/pdf/2411.11288.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Chen, Jingcai Guo, Song Guo, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11288">Neuron: Learning Context-Aware Evolving Representations for Zero-Shot Skeleton Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Zero-shot skeleton action recognition is a non-trivial task that requires robust unseen generalization with prior knowledge from only seen classes and shared semantics. Existing methods typically build the skeleton-semantics interactions by uncontrollable mappings and conspicuous representations, thereby can hardly capture the intricate and fine-grained relationship for effective cross-modal transferability. To address these issues, we propose a novel dyNamically Evolving dUal skeleton-semantic syneRgistic framework with the guidance of cOntext-aware side informatioN (dubbed Neuron), to explore more fine-grained cross-modal correspondence from micro to macro perspectives at both spatial and temporal levels, respectively. Concretely, 1) we first construct the spatial-temporal evolving micro-prototypes and integrate dynamic context-aware side information to capture the intricate and synergistic skeleton-semantic correlations step-by-step, progressively refining cross-model alignment; and 2) we introduce the spatial compression and temporal memory mechanisms to guide the growth of spatial-temporal micro-prototypes, enabling them to absorb structure-related spatial representations and regularity-dependent temporal patterns. Notably, such processes are analogous to the learning and growth of neurons, equipping the framework with the capacity to generalize to novel unseen action categories. Extensive experiments on various benchmark datasets demonstrated the superiority of the proposed method.
<div id='section'>Paperid: <span id='pid'>82, <a href='https://arxiv.org/pdf/2306.09615.pdf' target='_blank'>https://arxiv.org/pdf/2306.09615.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yaqi Zhang, Yan Lu, Bin Liu, Zhiwei Zhao, Qi Chu, Nenghai Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.09615">EVOPOSE: A Recursive Transformer For 3D Human Pose Estimation With Kinematic Structure Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transformer is popular in recent 3D human pose estimation, which utilizes long-term modeling to lift 2D keypoints into the 3D space. However, current transformer-based methods do not fully exploit the prior knowledge of the human skeleton provided by the kinematic structure. In this paper, we propose a novel transformer-based model EvoPose to introduce the human body prior knowledge for 3D human pose estimation effectively. Specifically, a Structural Priors Representation (SPR) module represents human priors as structural features carrying rich body patterns, e.g. joint relationships. The structural features are interacted with 2D pose sequences and help the model to achieve more informative spatiotemporal features. Moreover, a Recursive Refinement (RR) module is applied to refine the 3D pose outputs by utilizing estimated results and further injects human priors simultaneously. Extensive experiments demonstrate the effectiveness of EvoPose which achieves a new state of the art on two most popular benchmarks, Human3.6M and MPI-INF-3DHP.
<div id='section'>Paperid: <span id='pid'>83, <a href='https://arxiv.org/pdf/2508.12948.pdf' target='_blank'>https://arxiv.org/pdf/2508.12948.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Wei, Shaojie Zhang, Yonghao Dang, Jianqin Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12948">MaskSem: Semantic-Guided Masking for Learning 3D Hybrid High-Order Motion Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human action recognition is a crucial task for intelligent robotics, particularly within the context of human-robot collaboration research. In self-supervised skeleton-based action recognition, the mask-based reconstruction paradigm learns the spatial structure and motion patterns of the skeleton by masking joints and reconstructing the target from unlabeled data. However, existing methods focus on a limited set of joints and low-order motion patterns, limiting the model's ability to understand complex motion patterns. To address this issue, we introduce MaskSem, a novel semantic-guided masking method for learning 3D hybrid high-order motion representations. This novel framework leverages Grad-CAM based on relative motion to guide the masking of joints, which can be represented as the most semantically rich temporal orgions. The semantic-guided masking process can encourage the model to explore more discriminative features. Furthermore, we propose using hybrid high-order motion as the reconstruction target, enabling the model to learn multi-order motion patterns. Specifically, low-order motion velocity and high-order motion acceleration are used together as the reconstruction target. This approach offers a more comprehensive description of the dynamic motion process, enhancing the model's understanding of motion patterns. Experiments on the NTU60, NTU120, and PKU-MMD datasets show that MaskSem, combined with a vanilla transformer, improves skeleton-based action recognition, making it more suitable for applications in human-robot interaction.
<div id='section'>Paperid: <span id='pid'>84, <a href='https://arxiv.org/pdf/2305.11468.pdf' target='_blank'>https://arxiv.org/pdf/2305.11468.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Zhou, Zhi-Qi Cheng, Jun-Yan He, Bin Luo, Yifeng Geng, Xuansong Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.11468">Overcoming Topology Agnosticism: Enhancing Skeleton-Based Action Recognition through Redefined Skeletal Topology Awareness</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph Convolutional Networks (GCNs) have long defined the state-of-the-art in skeleton-based action recognition, leveraging their ability to unravel the complex dynamics of human joint topology through the graph's adjacency matrix. However, an inherent flaw has come to light in these cutting-edge models: they tend to optimize the adjacency matrix jointly with the model weights. This process, while seemingly efficient, causes a gradual decay of bone connectivity data, culminating in a model indifferent to the very topology it sought to map. As a remedy, we propose a threefold strategy: (1) We forge an innovative pathway that encodes bone connectivity by harnessing the power of graph distances. This approach preserves the vital topological nuances often lost in conventional GCNs. (2) We highlight an oft-overlooked feature - the temporal mean of a skeletal sequence, which, despite its modest guise, carries highly action-specific information. (3) Our investigation revealed strong variations in joint-to-joint relationships across different actions. This finding exposes the limitations of a single adjacency matrix in capturing the variations of relational configurations emblematic of human movement, which we remedy by proposing an efficient refinement to Graph Convolutions (GC) - the BlockGC. This evolution slashes parameters by a substantial margin (above 40%), while elevating performance beyond original GCNs. Our full model, the BlockGCN, establishes new standards in skeleton-based action recognition for small model sizes. Its high accuracy, notably on the large-scale NTU RGB+D 120 dataset, stand as compelling proof of the efficacy of BlockGCN.
<div id='section'>Paperid: <span id='pid'>85, <a href='https://arxiv.org/pdf/2211.09590.pdf' target='_blank'>https://arxiv.org/pdf/2211.09590.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Zhou, Zhi-Qi Cheng, Chao Li, Yanwen Fang, Yifeng Geng, Xuansong Xie, Margret Keuper
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.09590">Hypergraph Transformer for Skeleton-based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeleton-based action recognition aims to recognize human actions given human joint coordinates with skeletal interconnections. By defining a graph with joints as vertices and their natural connections as edges, previous works successfully adopted Graph Convolutional networks (GCNs) to model joint co-occurrences and achieved superior performance. More recently, a limitation of GCNs is identified, i.e., the topology is fixed after training. To relax such a restriction, Self-Attention (SA) mechanism has been adopted to make the topology of GCNs adaptive to the input, resulting in the state-of-the-art hybrid models. Concurrently, attempts with plain Transformers have also been made, but they still lag behind state-of-the-art GCN-based methods due to the lack of structural prior. Unlike hybrid models, we propose a more elegant solution to incorporate the bone connectivity into Transformer via a graph distance embedding. Our embedding retains the information of skeletal structure during training, whereas GCNs merely use it for initialization. More importantly, we reveal an underlying issue of graph models in general, i.e., pairwise aggregation essentially ignores the high-order kinematic dependencies between body joints. To fill this gap, we propose a new self-attention (SA) mechanism on hypergraph, termed Hypergraph Self-Attention (HyperSA), to incorporate intrinsic higher-order relations into the model. We name the resulting model Hyperformer, and it beats state-of-the-art graph models w.r.t. accuracy and efficiency on NTU RGB+D, NTU RGB+D 120, and Northwestern-UCLA datasets.
<div id='section'>Paperid: <span id='pid'>86, <a href='https://arxiv.org/pdf/2205.02071.pdf' target='_blank'>https://arxiv.org/pdf/2205.02071.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Liu, Jiyao Yang, Madhawa Perera, Pan Ji, Dongwoo Kim, Min Xu, Tianyang Wang, Saeed Anwar, Tom Gedeon, Lei Wang, Zhenyue Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2205.02071">Representation-Centric Survey of Skeletal Action Recognition and the ANUBIS Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D skeleton-based human action recognition has emerged as a powerful alternative to traditional RGB and depth-based approaches, offering robustness to environmental variations, computational efficiency, and enhanced privacy. Despite remarkable progress, current research remains fragmented across diverse input representations and lacks evaluation under scenarios that reflect modern real-world challenges. This paper presents a representation-centric survey of skeleton-based action recognition, systematically categorizing state-of-the-art methods by their input feature types: joint coordinates, bone vectors, motion flows, and extended representations, and analyzing how these choices influence spatial-temporal modeling strategies. Building on the insights from this review, we introduce ANUBIS, a large-scale, challenging skeleton action dataset designed to address critical gaps in existing benchmarks. ANUBIS incorporates multi-view recordings with back-view perspectives, complex multi-person interactions, fine-grained and violent actions, and contemporary social behaviors. We benchmark a diverse set of state-of-the-art models on ANUBIS and conduct an in-depth analysis of how different feature types affect recognition performance across 102 action categories. Our results show strong action-feature dependencies, highlight the limitations of naïve multi-representational fusion, and point toward the need for task-aware, semantically aligned integration strategies. This work offers both a comprehensive foundation and a practical benchmarking resource, aiming to guide the next generation of robust, generalizable skeleton-based action recognition systems for complex real-world scenarios. The dataset website, benchmarking framework, and download link are available at https://yliu1082.github.io/ANUBIS/.
<div id='section'>Paperid: <span id='pid'>87, <a href='https://arxiv.org/pdf/2506.00915.pdf' target='_blank'>https://arxiv.org/pdf/2506.00915.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengyuan Liu, Hong Liu, Qianshuo Hu, Bin Ren, Junsong Yuan, Jiaying Lin, Jiajun Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00915">3D Skeleton-Based Action Recognition: A Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the inherent advantages of skeleton representation, 3D skeleton-based action recognition has become a prominent topic in the field of computer vision. However, previous reviews have predominantly adopted a model-oriented perspective, often neglecting the fundamental steps involved in skeleton-based action recognition. This oversight tends to ignore key components of skeleton-based action recognition beyond model design and has hindered deeper, more intrinsic understanding of the task. To bridge this gap, our review aims to address these limitations by presenting a comprehensive, task-oriented framework for understanding skeleton-based action recognition. We begin by decomposing the task into a series of sub-tasks, placing particular emphasis on preprocessing steps such as modality derivation and data augmentation. The subsequent discussion delves into critical sub-tasks, including feature extraction and spatio-temporal modeling techniques. Beyond foundational action recognition networks, recently advanced frameworks such as hybrid architectures, Mamba models, large language models (LLMs), and generative models have also been highlighted. Finally, a comprehensive overview of public 3D skeleton datasets is presented, accompanied by an analysis of state-of-the-art algorithms evaluated on these benchmarks. By integrating task-oriented discussions, comprehensive examinations of sub-tasks, and an emphasis on the latest advancements, our review provides a fundamental and accessible structured roadmap for understanding and advancing the field of 3D skeleton-based action recognition.
<div id='section'>Paperid: <span id='pid'>88, <a href='https://arxiv.org/pdf/2002.05907.pdf' target='_blank'>https://arxiv.org/pdf/2002.05907.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bin Ren, Mengyuan Liu, Runwei Ding, Hong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2002.05907">A Survey on 3D Skeleton-Based Action Recognition Using Learning Method</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D skeleton-based action recognition (3D SAR) has gained significant attention within the computer vision community, owing to the inherent advantages offered by skeleton data. As a result, a plethora of impressive works, including those based on conventional handcrafted features and learned feature extraction methods, have been conducted over the years. However, prior surveys on action recognition have primarily focused on video or RGB data-dominated approaches, with limited coverage of reviews related to skeleton data. Furthermore, despite the extensive application of deep learning methods in this field, there has been a notable absence of research that provides an introductory or comprehensive review from the perspective of deep learning architectures. To address these limitations, this survey first underscores the importance of action recognition and emphasizes the significance of 3D skeleton data as a valuable modality. Subsequently, we provide a comprehensive introduction to mainstream action recognition techniques based on four fundamental deep architectures, i.e., Recurrent Neural Networks (RNNs), Convolutional Neural Networks (CNNs), Graph Convolutional Network (GCN), and Transformers. All methods with the corresponding architectures are then presented in a data-driven manner with detailed discussion. Finally, we offer insights into the current largest 3D skeleton dataset, NTU-RGB+D, and its new edition, NTU-RGB+D 120, along with an overview of several top-performing algorithms on these datasets. To the best of our knowledge, this research represents the first comprehensive discussion of deep learning-based action recognition using 3D skeleton data.
<div id='section'>Paperid: <span id='pid'>89, <a href='https://arxiv.org/pdf/2509.23888.pdf' target='_blank'>https://arxiv.org/pdf/2509.23888.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tatsuro Banno, Takehiko Ohkawa, Ruicong Liu, Ryosuke Furuta, Yoichi Sato
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23888">AssemblyHands-X: Modeling 3D Hand-Body Coordination for Understanding Bimanual Human Activities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Bimanual human activities inherently involve coordinated movements of both hands and body. However, the impact of this coordination in activity understanding has not been systematically evaluated due to the lack of suitable datasets. Such evaluation demands kinematic-level annotations (e.g., 3D pose) for the hands and body, yet existing 3D activity datasets typically annotate either hand or body pose. Another line of work employs marker-based motion capture to provide full-body pose, but the physical markers introduce visual artifacts, thereby limiting models' generalization to natural, markerless videos. To address these limitations, we present AssemblyHands-X, the first markerless 3D hand-body benchmark for bimanual activities, designed to study the effect of hand-body coordination for action recognition. We begin by constructing a pipeline for 3D pose annotation from synchronized multi-view videos. Our approach combines multi-view triangulation with SMPL-X mesh fitting, yielding reliable 3D registration of hands and upper body. We then validate different input representations (e.g., video, hand pose, body pose, or hand-body pose) across recent action recognition models based on graph convolution or spatio-temporal attention. Our extensive experiments show that pose-based action inference is more efficient and accurate than video baselines. Moreover, joint modeling of hand and body cues improves action recognition over using hands or upper body alone, highlighting the importance of modeling interdependent hand-body dynamics for a holistic understanding of bimanual activities.
<div id='section'>Paperid: <span id='pid'>90, <a href='https://arxiv.org/pdf/2411.16768.pdf' target='_blank'>https://arxiv.org/pdf/2411.16768.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wangze Xu, Yifan Zhan, Zhihang Zhong, Xiao Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16768">Sequential Gaussian Avatars with Hierarchical Motion Context</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The emergence of neural rendering has significantly advanced the rendering quality of 3D human avatars, with the recently popular 3DGS technique enabling real-time performance. However, SMPL-driven 3DGS human avatars still struggle to capture fine appearance details due to the complex mapping from pose to appearance during fitting. In this paper, we propose SeqAvatar, which excavates the explicit 3DGS representation to better model human avatars based on a hierarchical motion context. Specifically, we utilize a coarse-to-fine motion conditions that incorporate both the overall human skeleton and fine-grained vertex motions for non-rigid deformation. To enhance the robustness of the proposed motion conditions, we adopt a spatio-temporal multi-scale sampling strategy to hierarchically integrate more motion clues to model human avatars. Extensive experiments demonstrate that our method significantly outperforms 3DGS-based approaches and renders human avatars orders of magnitude faster than the latest NeRF-based models that incorporate temporal context, all while delivering performance that is at least comparable or even superior. Project page: https://zezeaaa.github.io/projects/SeqAvatar/
<div id='section'>Paperid: <span id='pid'>91, <a href='https://arxiv.org/pdf/2407.12322.pdf' target='_blank'>https://arxiv.org/pdf/2407.12322.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenhan Wu, Ce Zheng, Zihao Yang, Chen Chen, Srijan Das, Aidong Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.12322">Frequency Guidance Matters: Skeletal Action Recognition by Frequency-Aware Mixed Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, transformers have demonstrated great potential for modeling long-term dependencies from skeleton sequences and thereby gained ever-increasing attention in skeleton action recognition. However, the existing transformer-based approaches heavily rely on the naive attention mechanism for capturing the spatiotemporal features, which falls short in learning discriminative representations that exhibit similar motion patterns. To address this challenge, we introduce the Frequency-aware Mixed Transformer (FreqMixFormer), specifically designed for recognizing similar skeletal actions with subtle discriminative motions. First, we introduce a frequency-aware attention module to unweave skeleton frequency representations by embedding joint features into frequency attention maps, aiming to distinguish the discriminative movements based on their frequency coefficients. Subsequently, we develop a mixed transformer architecture to incorporate spatial features with frequency features to model the comprehensive frequency-spatial patterns. Additionally, a temporal transformer is proposed to extract the global correlations across frames. Extensive experiments show that FreqMiXFormer outperforms SOTA on 3 popular skeleton action recognition datasets, including NTU RGB+D, NTU RGB+D 120, and NW-UCLA datasets.
<div id='section'>Paperid: <span id='pid'>92, <a href='https://arxiv.org/pdf/2305.17939.pdf' target='_blank'>https://arxiv.org/pdf/2305.17939.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nariki Tanaka, Hiroshi Kera, Kazuhiko Kawamoto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.17939">Fourier Analysis on Robustness of Graph Convolutional Neural Networks for Skeleton-based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Using Fourier analysis, we explore the robustness and vulnerability of graph convolutional neural networks (GCNs) for skeleton-based action recognition. We adopt a joint Fourier transform (JFT), a combination of the graph Fourier transform (GFT) and the discrete Fourier transform (DFT), to examine the robustness of adversarially-trained GCNs against adversarial attacks and common corruptions. Experimental results with the NTU RGB+D dataset reveal that adversarial training does not introduce a robustness trade-off between adversarial attacks and low-frequency perturbations, which typically occurs during image classification based on convolutional neural networks. This finding indicates that adversarial training is a practical approach to enhancing robustness against adversarial attacks and common corruptions in skeleton-based action recognition. Furthermore, we find that the Fourier approach cannot explain vulnerability against skeletal part occlusion corruption, which highlights its limitations. These findings extend our understanding of the robustness of GCNs, potentially guiding the development of more robust learning methods for skeleton-based action recognition.
<div id='section'>Paperid: <span id='pid'>93, <a href='https://arxiv.org/pdf/2305.00666.pdf' target='_blank'>https://arxiv.org/pdf/2305.00666.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yilei Hua, Wenhan Wu, Ce Zheng, Aidong Lu, Mengyuan Liu, Chen Chen, Shiqian Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.00666">Part Aware Contrastive Learning for Self-Supervised Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, remarkable results have been achieved in self-supervised action recognition using skeleton sequences with contrastive learning. It has been observed that the semantic distinction of human action features is often represented by local body parts, such as legs or hands, which are advantageous for skeleton-based action recognition. This paper proposes an attention-based contrastive learning framework for skeleton representation learning, called SkeAttnCLR, which integrates local similarity and global features for skeleton-based action representations. To achieve this, a multi-head attention mask module is employed to learn the soft attention mask features from the skeletons, suppressing non-salient local features while accentuating local salient features, thereby bringing similar local features closer in the feature space. Additionally, ample contrastive pairs are generated by expanding contrastive pairs based on salient and non-salient features with global features, which guide the network to learn the semantic representations of the entire skeleton. Therefore, with the attention mask mechanism, SkeAttnCLR learns local features under different data augmentation views. The experiment results demonstrate that the inclusion of local feature similarity significantly enhances skeleton-based action representation. Our proposed SkeAttnCLR outperforms state-of-the-art methods on NTURGB+D, NTU120-RGB+D, and PKU-MMD datasets.
<div id='section'>Paperid: <span id='pid'>94, <a href='https://arxiv.org/pdf/2209.02399.pdf' target='_blank'>https://arxiv.org/pdf/2209.02399.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenhan Wu, Yilei Hua, Ce Zheng, Shiqian Wu, Chen Chen, Aidong Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.02399">SkeletonMAE: Spatial-Temporal Masked Autoencoders for Self-supervised Skeleton Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fully supervised skeleton-based action recognition has achieved great progress with the blooming of deep learning techniques. However, these methods require sufficient labeled data which is not easy to obtain. In contrast, self-supervised skeleton-based action recognition has attracted more attention. With utilizing the unlabeled data, more generalizable features can be learned to alleviate the overfitting problem and reduce the demand of massive labeled training data. Inspired by the MAE, we propose a spatial-temporal masked autoencoder framework for self-supervised 3D skeleton-based action recognition (SkeletonMAE). Following MAE's masking and reconstruction pipeline, we utilize a skeleton-based encoder-decoder transformer architecture to reconstruct the masked skeleton sequences. A novel masking strategy, named Spatial-Temporal Masking, is introduced in terms of both joint-level and frame-level for the skeleton sequence. This pre-training strategy makes the encoder output generalizable skeleton features with spatial and temporal dependencies. Given the unmasked skeleton sequence, the encoder is fine-tuned for the action recognition task. Extensive experiments show that our SkeletonMAE achieves remarkable performance and outperforms the state-of-the-art methods on both NTU RGB+D and NTU RGB+D 120 datasets.
<div id='section'>Paperid: <span id='pid'>95, <a href='https://arxiv.org/pdf/2501.16843.pdf' target='_blank'>https://arxiv.org/pdf/2501.16843.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxin Cao, Kai Ye, Derui Wang, Minhui Xue, Hao Ge, Chenxiong Qian, Jin Song Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.16843">Bones of Contention: Exploring Query-Efficient Attacks Against Skeleton Recognition Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeleton action recognition models have secured more attention than video-based ones in various applications due to privacy preservation and lower storage requirements. Skeleton data are typically transmitted to cloud servers for action recognition, with results returned to clients via Apps/APIs. However, the vulnerability of skeletal models against adversarial perturbations gradually reveals the unreliability of these systems. Existing black-box attacks all operate in a decision-based manner, resulting in numerous queries that hinder efficiency and feasibility in real-world applications. Moreover, all attacks off the shelf focus on only restricted perturbations, while ignoring model weaknesses when encountered with non-semantic perturbations. In this paper, we propose two query-effIcient Skeletal Adversarial AttaCks, ISAAC-K and ISAAC-N. As a black-box attack, ISAAC-K utilizes Grad-CAM in a surrogate model to extract key joints where minor sparse perturbations are then added to fool the classifier. To guarantee natural adversarial motions, we introduce constraints of both bone length and temporal consistency. ISAAC-K finds stronger adversarial examples on $\ell_\infty$ norm, which can encompass those on other norms. Exhaustive experiments substantiate that ISAAC-K can uplift the attack efficiency of the perturbations under 10 skeletal models. Additionally, as a byproduct, ISAAC-N fools the classifier by replacing skeletons unrelated to the action. We surprisingly find that skeletal models are vulnerable to large perturbations where the part-wise non-semantic joints are just replaced, leading to a query-free no-box attack without any prior knowledge. Based on that, four adaptive defenses are eventually proposed to improve the robustness of skeleton recognition models.
<div id='section'>Paperid: <span id='pid'>96, <a href='https://arxiv.org/pdf/2006.12075.pdf' target='_blank'>https://arxiv.org/pdf/2006.12075.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingyi Shi, Kfir Aberman, Andreas Aristidou, Taku Komura, Dani Lischinski, Daniel Cohen-Or, Baoquan Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2006.12075">MotioNet: 3D Human Motion Reconstruction from Monocular Video with Skeleton Consistency</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce MotioNet, a deep neural network that directly reconstructs the motion of a 3D human skeleton from monocular video.While previous methods rely on either rigging or inverse kinematics (IK) to associate a consistent skeleton with temporally coherent joint rotations, our method is the first data-driven approach that directly outputs a kinematic skeleton, which is a complete, commonly used, motion representation. At the crux of our approach lies a deep neural network with embedded kinematic priors, which decomposes sequences of 2D joint positions into two separate attributes: a single, symmetric, skeleton, encoded by bone lengths, and a sequence of 3D joint rotations associated with global root positions and foot contact labels. These attributes are fed into an integrated forward kinematics (FK) layer that outputs 3D positions, which are compared to a ground truth. In addition, an adversarial loss is applied to the velocities of the recovered rotations, to ensure that they lie on the manifold of natural joint rotations. The key advantage of our approach is that it learns to infer natural joint rotations directly from the training data, rather than assuming an underlying model, or inferring them from joint positions using a data-agnostic IK solver. We show that enforcing a single consistent skeleton along with temporally coherent joint rotations constrains the solution space, leading to a more robust handling of self-occlusions and depth ambiguities.
<div id='section'>Paperid: <span id='pid'>97, <a href='https://arxiv.org/pdf/2502.05869.pdf' target='_blank'>https://arxiv.org/pdf/2502.05869.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Li, Haoxuan Qu, Mengyuan Liu, Jun Liu, Yujun Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.05869">HyLiFormer: Hyperbolic Linear Attention for Skeleton-based Human Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transformers have demonstrated remarkable performance in skeleton-based human action recognition, yet their quadratic computational complexity remains a bottleneck for real-world applications. To mitigate this, linear attention mechanisms have been explored but struggle to capture the hierarchical structure of skeleton data. Meanwhile, the PoincarÃ© model, as a typical hyperbolic geometry, offers a powerful framework for modeling hierarchical structures but lacks well-defined operations for existing mainstream linear attention. In this paper, we propose HyLiFormer, a novel hyperbolic linear attention Transformer tailored for skeleton-based action recognition. Our approach incorporates a Hyperbolic Transformation with Curvatures (HTC) module to map skeleton data into hyperbolic space and a Hyperbolic Linear Attention (HLA) module for efficient long-range dependency modeling. Theoretical analysis and extensive experiments on NTU RGB+D and NTU RGB+D 120 datasets demonstrate that HyLiFormer significantly reduces computational complexity while preserving model accuracy, making it a promising solution for efficiency-critical applications.
<div id='section'>Paperid: <span id='pid'>98, <a href='https://arxiv.org/pdf/2404.00532.pdf' target='_blank'>https://arxiv.org/pdf/2404.00532.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoxuan Qu, Yujun Cai, Jun Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00532">LLMs are Good Action Recognizers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeleton-based action recognition has attracted lots of research attention. Recently, to build an accurate skeleton-based action recognizer, a variety of works have been proposed. Among them, some works use large model architectures as backbones of their recognizers to boost the skeleton data representation capability, while some other works pre-train their recognizers on external data to enrich the knowledge. In this work, we observe that large language models which have been extensively used in various natural language processing tasks generally hold both large model architectures and rich implicit knowledge. Motivated by this, we propose a novel LLM-AR framework, in which we investigate treating the Large Language Model as an Action Recognizer. In our framework, we propose a linguistic projection process to project each input action signal (i.e., each skeleton sequence) into its ``sentence format'' (i.e., an ``action sentence''). Moreover, we also incorporate our framework with several designs to further facilitate this linguistic projection process. Extensive experiments demonstrate the efficacy of our proposed framework.
<div id='section'>Paperid: <span id='pid'>99, <a href='https://arxiv.org/pdf/2304.03532.pdf' target='_blank'>https://arxiv.org/pdf/2304.03532.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinshun Wang, Qiongjie Cui, Chen Chen, Shen Zhao, Mengyuan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.03532">Graph-Guided MLP-Mixer for Skeleton-Based Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, Graph Convolutional Networks (GCNs) have been widely used in human motion prediction, but their performance remains unsatisfactory. Recently, MLP-Mixer, initially developed for vision tasks, has been leveraged into human motion prediction as a promising alternative to GCNs, which achieves both better performance and better efficiency than GCNs. Unlike GCNs, which can explicitly capture human skeleton's bone-joint structure by representing it as a graph with edges and nodes, MLP-Mixer relies on fully connected layers and thus cannot explicitly model such graph-like structure of human's. To break this limitation of MLP-Mixer's, we propose \textit{Graph-Guided Mixer}, a novel approach that equips the original MLP-Mixer architecture with the capability to model graph structure. By incorporating graph guidance, our \textit{Graph-Guided Mixer} can effectively capture and utilize the specific connectivity patterns within human skeleton's graph representation. In this paper, first we uncover a theoretical connection between MLP-Mixer and GCN that is unexplored in existing research. Building on this theoretical connection, next we present our proposed \textit{Graph-Guided Mixer}, explaining how the original MLP-Mixer architecture is reinvented to incorporate guidance from graph structure. Then we conduct an extensive evaluation on the Human3.6M, AMASS, and 3DPW datasets, which shows that our method achieves state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>100, <a href='https://arxiv.org/pdf/2401.00921.pdf' target='_blank'>https://arxiv.org/pdf/2401.00921.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruizhuo Xu, Linzhi Huang, Mei Wang, Jiani Hu, Weihong Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.00921">Skeleton2vec: A Self-supervised Learning Framework with Contextualized Target Representations for Skeleton Sequence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-supervised pre-training paradigms have been extensively explored in the field of skeleton-based action recognition. In particular, methods based on masked prediction have pushed the performance of pre-training to a new height. However, these methods take low-level features, such as raw joint coordinates or temporal motion, as prediction targets for the masked regions, which is suboptimal. In this paper, we show that using high-level contextualized features as prediction targets can achieve superior performance. Specifically, we propose Skeleton2vec, a simple and efficient self-supervised 3D action representation learning framework, which utilizes a transformer-based teacher encoder taking unmasked training samples as input to create latent contextualized representations as prediction targets. Benefiting from the self-attention mechanism, the latent representations generated by the teacher encoder can incorporate the global context of the entire training samples, leading to a richer training task. Additionally, considering the high temporal correlations in skeleton sequences, we propose a motion-aware tube masking strategy which divides the skeleton sequence into several tubes and performs persistent masking within each tube based on motion priors, thus forcing the model to build long-range spatio-temporal connections and focus on action-semantic richer regions. Extensive experiments on NTU-60, NTU-120, and PKU-MMD datasets demonstrate that our proposed Skeleton2vec outperforms previous methods and achieves state-of-the-art results.
<div id='section'>Paperid: <span id='pid'>101, <a href='https://arxiv.org/pdf/2303.14474.pdf' target='_blank'>https://arxiv.org/pdf/2303.14474.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Wang, Piotr Koniusz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.14474">3Mformer: Multi-order Multi-mode Transformer for Skeletal Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many skeletal action recognition models use GCNs to represent the human body by 3D body joints connected body parts. GCNs aggregate one- or few-hop graph neighbourhoods, and ignore the dependency between not linked body joints. We propose to form hypergraph to model hyper-edges between graph nodes (e.g., third- and fourth-order hyper-edges capture three and four nodes) which help capture higher-order motion patterns of groups of body joints. We split action sequences into temporal blocks, Higher-order Transformer (HoT) produces embeddings of each temporal block based on (i) the body joints, (ii) pairwise links of body joints and (iii) higher-order hyper-edges of skeleton body joints. We combine such HoT embeddings of hyper-edges of orders 1, ..., r by a novel Multi-order Multi-mode Transformer (3Mformer) with two modules whose order can be exchanged to achieve coupled-mode attention on coupled-mode tokens based on 'channel-temporal block', 'order-channel-body joint', 'channel-hyper-edge (any order)' and 'channel-only' pairs. The first module, called Multi-order Pooling (MP), additionally learns weighted aggregation along the hyper-edge mode, whereas the second module, Temporal block Pooling (TP), aggregates along the temporal block mode. Our end-to-end trainable network yields state-of-the-art results compared to GCN-, transformer- and hypergraph-based counterparts.
<div id='section'>Paperid: <span id='pid'>102, <a href='https://arxiv.org/pdf/2402.03019.pdf' target='_blank'>https://arxiv.org/pdf/2402.03019.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Wang, Xiuyuan Yuan, Tom Gedeon, Liang Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.03019">Taylor Videos for Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effectively extracting motions from video is a critical and long-standing problem for action recognition. This problem is very challenging because motions (i) do not have an explicit form, (ii) have various concepts such as displacement, velocity, and acceleration, and (iii) often contain noise caused by unstable pixels. Addressing these challenges, we propose the Taylor video, a new video format that highlights the dominate motions (e.g., a waving hand) in each of its frames named the Taylor frame. Taylor video is named after Taylor series, which approximates a function at a given point using important terms. In the scenario of videos, we define an implicit motion-extraction function which aims to extract motions from video temporal block. In this block, using the frames, the difference frames, and higher-order difference frames, we perform Taylor expansion to approximate this function at the starting frame. We show the summation of the higher-order terms in the Taylor series gives us dominant motion patterns, where static objects, small and unstable motions are removed. Experimentally we show that Taylor videos are effective inputs to popular architectures including 2D CNNs, 3D CNNs, and transformers. When used individually, Taylor videos yield competitive action recognition accuracy compared to RGB videos and optical flow. When fused with RGB or optical flow videos, further accuracy improvement is achieved. Additionally, we apply Taylor video computation to human skeleton sequences, resulting in Taylor skeleton sequences that outperform the use of original skeletons for skeleton-based action recognition.
<div id='section'>Paperid: <span id='pid'>103, <a href='https://arxiv.org/pdf/2503.06938.pdf' target='_blank'>https://arxiv.org/pdf/2503.06938.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sania Zahan, Ghulam Mubashar Hassan, Ajmal Mian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06938">Modeling Human Skeleton Joint Dynamics for Fall Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing pace of population aging calls for better care and support systems. Falling is a frequent and critical problem for elderly people causing serious long-term health issues. Fall detection from video streams is not an attractive option for real-life applications due to privacy issues. Existing methods try to resolve this issue by using very low-resolution cameras or video encryption. However, privacy cannot be ensured completely with such approaches. Key points on the body, such as skeleton joints, can convey significant information about motion dynamics and successive posture changes which are crucial for fall detection. Skeleton joints have been explored for feature extraction but with image recognition models that ignore joint dependency across frames which is important for the classification of actions. Moreover, existing models are over-parameterized or evaluated on small datasets with very few activity classes. We propose an efficient graph convolution network model that exploits spatio-temporal joint dependencies and dynamics of human skeleton joints for accurate fall detection. Our method leverages dynamic representation with robust concurrent spatio-temporal characteristics of skeleton joints. We performed extensive experiments on three large-scale datasets. With a significantly smaller model size than most existing methods, our proposed method achieves state-of-the-art results on the large scale NTU datasets.
<div id='section'>Paperid: <span id='pid'>104, <a href='https://arxiv.org/pdf/2308.10623.pdf' target='_blank'>https://arxiv.org/pdf/2308.10623.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andy Catruna, Adrian Cosma, Emilian Radoi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.10623">GaitPT: Skeletons Are All You Need For Gait Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The analysis of patterns of walking is an important area of research that has numerous applications in security, healthcare, sports and human-computer interaction. Lately, walking patterns have been regarded as a unique fingerprinting method for automatic person identification at a distance. In this work, we propose a novel gait recognition architecture called Gait Pyramid Transformer (GaitPT) that leverages pose estimation skeletons to capture unique walking patterns, without relying on appearance information. GaitPT adopts a hierarchical transformer architecture that effectively extracts both spatial and temporal features of movement in an anatomically consistent manner, guided by the structure of the human skeleton. Our results show that GaitPT achieves state-of-the-art performance compared to other skeleton-based gait recognition works, in both controlled and in-the-wild scenarios. GaitPT obtains 82.6% average accuracy on CASIA-B, surpassing other works by a margin of 6%. Moreover, it obtains 52.16% Rank-1 accuracy on GREW, outperforming both skeleton-based and appearance-based approaches.
<div id='section'>Paperid: <span id='pid'>105, <a href='https://arxiv.org/pdf/2506.03481.pdf' target='_blank'>https://arxiv.org/pdf/2506.03481.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongsong Wang, Xiaoyan Ma, Jidong Kuang, Jie Gui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03481">Heterogeneous Skeleton-Based Action Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeleton-based human action recognition has received widespread attention in recent years due to its diverse range of application scenarios. Due to the different sources of human skeletons, skeleton data naturally exhibit heterogeneity. The previous works, however, overlook the heterogeneity of human skeletons and solely construct models tailored for homogeneous skeletons. This work addresses the challenge of heterogeneous skeleton-based action representation learning, specifically focusing on processing skeleton data that varies in joint dimensions and topological structures. The proposed framework comprises two primary components: heterogeneous skeleton processing and unified representation learning. The former first converts two-dimensional skeleton data into three-dimensional skeleton via an auxiliary network, and then constructs a prompted unified skeleton using skeleton-specific prompts. We also design an additional modality named semantic motion encoding to harness the semantic information within skeletons. The latter module learns a unified action representation using a shared backbone network that processes different heterogeneous skeletons. Extensive experiments on the NTU-60, NTU-120, and PKU-MMD II datasets demonstrate the effectiveness of our method in various tasks of action understanding. Our approach can be applied to action recognition in robots with different humanoid structures.
<div id='section'>Paperid: <span id='pid'>106, <a href='https://arxiv.org/pdf/2504.16655.pdf' target='_blank'>https://arxiv.org/pdf/2504.16655.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Younggeol Cho, Elisa Motta, Olivia Nocentini, Marta Lagomarsino, Andrea Merello, Marco Crepaldi, Arash Ajoudani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.16655">WiFi based Human Fall and Activity Recognition using Transformer based Encoder Decoder and Graph Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human pose estimation and action recognition have received attention due to their critical roles in healthcare monitoring, rehabilitation, and assistive technologies. In this study, we proposed a novel architecture named Transformer based Encoder Decoder Network (TED Net) designed for estimating human skeleton poses from WiFi Channel State Information (CSI). TED Net integrates convolutional encoders with transformer based attention mechanisms to capture spatiotemporal features from CSI signals. The estimated skeleton poses were used as input to a customized Directed Graph Neural Network (DGNN) for action recognition. We validated our model on two datasets: a publicly available multi modal dataset for assessing general pose estimation, and a newly collected dataset focused on fall related scenarios involving 20 participants. Experimental results demonstrated that TED Net outperformed existing approaches in pose estimation, and that the DGNN achieves reliable action classification using CSI based skeletons, with performance comparable to RGB based systems. Notably, TED Net maintains robust performance across both fall and non fall cases. These findings highlight the potential of CSI driven human skeleton estimation for effective action recognition, particularly in home environments such as elderly fall detection. In such settings, WiFi signals are often readily available, offering a privacy preserving alternative to vision based methods, which may raise concerns about continuous camera monitoring.
<div id='section'>Paperid: <span id='pid'>107, <a href='https://arxiv.org/pdf/2501.06035.pdf' target='_blank'>https://arxiv.org/pdf/2501.06035.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cecilia Curreli, Dominik Muhle, Abhishek Saroha, Zhenzhang Ye, Riccardo Marin, Daniel Cremers
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.06035">Nonisotropic Gaussian Diffusion for Realistic 3D Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Probabilistic human motion prediction aims to forecast multiple possible future movements from past observations. While current approaches report high diversity and realism, they often generate motions with undetected limb stretching and jitter. To address this, we introduce SkeletonDiffusion, a latent diffusion model that embeds an explicit inductive bias on the human body within its architecture and training. Our model is trained with a novel nonisotropic Gaussian diffusion formulation that aligns with the natural kinematic structure of the human skeleton. Results show that our approach outperforms conventional isotropic alternatives, consistently generating realistic predictions while avoiding artifacts such as limb distortion. Additionally, we identify a limitation in commonly used diversity metrics, which may inadvertently favor models that produce inconsistent limb lengths within the same sequence. SkeletonDiffusion sets a new benchmark on real-world datasets, outperforming various baselines across multiple evaluation metrics. Visit our project page at https://ceveloper.github.io/publications/skeletondiffusion/ .
<div id='section'>Paperid: <span id='pid'>108, <a href='https://arxiv.org/pdf/2409.10473.pdf' target='_blank'>https://arxiv.org/pdf/2409.10473.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lehong Wu, Lilang Lin, Jiahang Zhang, Yiyang Ma, Jiaying Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.10473">MacDiff: Unified Skeleton Modeling with Masked Conditional Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-supervised learning has proved effective for skeleton-based human action understanding. However, previous works either rely on contrastive learning that suffers false negative problems or are based on reconstruction that learns too much unessential low-level clues, leading to limited representations for downstream tasks. Recently, great advances have been made in generative learning, which is naturally a challenging yet meaningful pretext task to model the general underlying data distributions. However, the representation learning capacity of generative models is under-explored, especially for the skeletons with spacial sparsity and temporal redundancy. To this end, we propose Masked Conditional Diffusion (MacDiff) as a unified framework for human skeleton modeling. For the first time, we leverage diffusion models as effective skeleton representation learners. Specifically, we train a diffusion decoder conditioned on the representations extracted by a semantic encoder. Random masking is applied to encoder inputs to introduce a information bottleneck and remove redundancy of skeletons. Furthermore, we theoretically demonstrate that our generative objective involves the contrastive learning objective which aligns the masked and noisy views. Meanwhile, it also enforces the representation to complement for the noisy view, leading to better generalization performance. MacDiff achieves state-of-the-art performance on representation learning benchmarks while maintaining the competence for generative tasks. Moreover, we leverage the diffusion model for data augmentation, significantly enhancing the fine-tuning performance in scenarios with scarce labeled data. Our project is available at https://lehongwu.github.io/ECCV24MacDiff/.
<div id='section'>Paperid: <span id='pid'>109, <a href='https://arxiv.org/pdf/2407.12312.pdf' target='_blank'>https://arxiv.org/pdf/2407.12312.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahang Zhang, Lilang Lin, Jiaying Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.12312">Shap-Mix: Shapley Value Guided Mixing for Long-Tailed Skeleton Based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In real-world scenarios, human actions often fall into a long-tailed distribution. It makes the existing skeleton-based action recognition works, which are mostly designed based on balanced datasets, suffer from a sharp performance degradation. Recently, many efforts have been madeto image/video long-tailed learning. However, directly applying them to skeleton data can be sub-optimal due to the lack of consideration of the crucial spatial-temporal motion patterns, especially for some modality-specific methodologies such as data augmentation. To this end, considering the crucial role of the body parts in the spatially concentrated human actions, we attend to the mixing augmentations and propose a novel method, Shap-Mix, which improves long-tailed learning by mining representative motion patterns for tail categories. Specifically, we first develop an effective spatial-temporal mixing strategy for the skeleton to boost representation quality. Then, the employed saliency guidance method is presented, consisting of the saliency estimation based on Shapley value and a tail-aware mixing policy. It preserves the salient motion parts of minority classes in mixed data, explicitly establishing the relationships between crucial body structure cues and high-level semantics. Extensive experiments on three large-scale skeleton datasets show our remarkable performance improvement under both long-tailed and balanced settings. Our project is publicly available at: https://jhang2020.github.io/Projects/Shap-Mix/Shap-Mix.html.
<div id='section'>Paperid: <span id='pid'>110, <a href='https://arxiv.org/pdf/2407.10935.pdf' target='_blank'>https://arxiv.org/pdf/2407.10935.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soroush Mehraban, Mohammad Javad Rajabi, Babak Taati
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10935">STARS: Self-supervised Tuning for 3D Action Recognition in Skeleton Sequences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-supervised pretraining methods with masked prediction demonstrate remarkable within-dataset performance in skeleton-based action recognition. However, we show that, unlike contrastive learning approaches, they do not produce well-separated clusters. Additionally, these methods struggle with generalization in few-shot settings. To address these issues, we propose Self-supervised Tuning for 3D Action Recognition in Skeleton sequences (STARS). Specifically, STARS first uses a masked prediction stage using an encoder-decoder architecture. It then employs nearest-neighbor contrastive learning to partially tune the weights of the encoder, enhancing the formation of semantic clusters for different actions. By tuning the encoder for a few epochs, and without using hand-crafted data augmentations, STARS achieves state-of-the-art self-supervised results in various benchmarks, including NTU-60, NTU-120, and PKU-MMD. In addition, STARS exhibits significantly better results than masked prediction models in few-shot settings, where the model has not seen the actions throughout pretraining. Project page: https://soroushmehraban.github.io/stars/
<div id='section'>Paperid: <span id='pid'>111, <a href='https://arxiv.org/pdf/2406.02978.pdf' target='_blank'>https://arxiv.org/pdf/2406.02978.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahang Zhang, Lilang Lin, Shuai Yang, Jiaying Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.02978">Self-Supervised Skeleton-Based Action Representation Learning: A Benchmark and Beyond</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-supervised learning (SSL), which aims to learn meaningful prior representations from unlabeled data, has been proven effective for skeleton-based action understanding. Different from the image domain, skeleton data possesses sparser spatial structures and diverse representation forms, with the absence of background clues and the additional temporal dimension, presenting new challenges for spatial-temporal motion pretext task design. Recently, many endeavors have been made for skeleton-based SSL, achieving remarkable progress. However, a systematic and thorough review is still lacking. In this paper, we conduct, for the first time, a comprehensive survey on self-supervised skeleton-based action representation learning. Following the taxonomy of context-based, generative learning, and contrastive learning approaches, we make a thorough review and benchmark of existing works and shed light on the future possible directions. Remarkably, our investigation demonstrates that most SSL works rely on the single paradigm, learning representations of a single level, and are evaluated on the action recognition task solely, which leaves the generalization power of skeleton SSL models under-explored. To this end, a novel and effective SSL method for skeleton is further proposed, which integrates versatile representation learning objectives of different granularity, substantially boosting the generalization capacity for multiple skeleton downstream tasks. Extensive experiments under three large-scale datasets demonstrate our method achieves superior generalization performance on various downstream tasks, including recognition, retrieval, detection, and few-shot learning.
<div id='section'>Paperid: <span id='pid'>112, <a href='https://arxiv.org/pdf/2308.14500.pdf' target='_blank'>https://arxiv.org/pdf/2308.14500.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Di Yang, Yaohui Wang, Antitza Dantcheva, Quan Kong, Lorenzo Garattoni, Gianpiero Francesca, Francois Bremond
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.14500">LAC: Latent Action Composition for Skeleton-based Action Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeleton-based action segmentation requires recognizing composable actions in untrimmed videos. Current approaches decouple this problem by first extracting local visual features from skeleton sequences and then processing them by a temporal model to classify frame-wise actions. However, their performances remain limited as the visual features cannot sufficiently express composable actions. In this context, we propose Latent Action Composition (LAC), a novel self-supervised framework aiming at learning from synthesized composable motions for skeleton-based action segmentation. LAC is composed of a novel generation module towards synthesizing new sequences. Specifically, we design a linear latent space in the generator to represent primitive motion. New composed motions can be synthesized by simply performing arithmetic operations on latent representations of multiple input skeleton sequences. LAC leverages such synthesized sequences, which have large diversity and complexity, for learning visual representations of skeletons in both sequence and frame spaces via contrastive learning. The resulting visual encoder has a high expressive power and can be effectively transferred onto action segmentation tasks by end-to-end fine-tuning without the need for additional temporal models. We conduct a study focusing on transfer-learning and we show that representations learned from pre-trained LAC outperform the state-of-the-art by a large margin on TSU, Charades, PKU-MMD datasets.
<div id='section'>Paperid: <span id='pid'>113, <a href='https://arxiv.org/pdf/2307.07286.pdf' target='_blank'>https://arxiv.org/pdf/2307.07286.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyuan Yang, Jun Liu, Shijian Lu, Er Meng Hwa, Alex C. Kot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.07286">One-Shot Action Recognition via Multi-Scale Spatial-Temporal Skeleton Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One-shot skeleton action recognition, which aims to learn a skeleton action recognition model with a single training sample, has attracted increasing interest due to the challenge of collecting and annotating large-scale skeleton action data. However, most existing studies match skeleton sequences by comparing their feature vectors directly which neglects spatial structures and temporal orders of skeleton data. This paper presents a novel one-shot skeleton action recognition technique that handles skeleton action recognition via multi-scale spatial-temporal feature matching. We represent skeleton data at multiple spatial and temporal scales and achieve optimal feature matching from two perspectives. The first is multi-scale matching which captures the scale-wise semantic relevance of skeleton data at multiple spatial and temporal scales simultaneously. The second is cross-scale matching which handles different motion magnitudes and speeds by capturing sample-wise relevance across multiple scales. Extensive experiments over three large-scale datasets (NTU RGB+D, NTU RGB+D 120, and PKU-MMD) show that our method achieves superior one-shot skeleton action recognition, and it outperforms the state-of-the-art consistently by large margins.
<div id='section'>Paperid: <span id='pid'>114, <a href='https://arxiv.org/pdf/2305.20091.pdf' target='_blank'>https://arxiv.org/pdf/2305.20091.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran, Angjoo Kanazawa, Jitendra Malik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.20091">Humans in 4D: Reconstructing and Tracking Humans with Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present an approach to reconstruct humans and track them over time. At the core of our approach, we propose a fully "transformerized" version of a network for human mesh recovery. This network, HMR 2.0, advances the state of the art and shows the capability to analyze unusual poses that have in the past been difficult to reconstruct from single images. To analyze video, we use 3D reconstructions from HMR 2.0 as input to a tracking system that operates in 3D. This enables us to deal with multiple people and maintain identities through occlusion events. Our complete approach, 4DHumans, achieves state-of-the-art results for tracking people from monocular video. Furthermore, we demonstrate the effectiveness of HMR 2.0 on the downstream task of action recognition, achieving significant improvements over previous pose-based action recognition approaches. Our code and models are available on the project website: https://shubham-goel.github.io/4dhumans/.
<div id='section'>Paperid: <span id='pid'>115, <a href='https://arxiv.org/pdf/2304.08799.pdf' target='_blank'>https://arxiv.org/pdf/2304.08799.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyuan Yang, Jun Liu, Shijian Lu, Er Meng Hwa, Yongjian Hu, Alex C. Kot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.08799">Self-Supervised 3D Action Representation Learning with Skeleton Cloud Colorization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Skeleton-based human action recognition has attracted increasing attention in recent years. Most of the existing work focuses on supervised learning which requires a large number of labeled action sequences that are often expensive and time-consuming to annotate. In this paper, we address self-supervised 3D action representation learning for skeleton-based action recognition. We investigate self-supervised representation learning and design a novel skeleton cloud colorization technique that is capable of learning spatial and temporal skeleton representations from unlabeled skeleton sequence data. We represent a skeleton action sequence as a 3D skeleton cloud and colorize each point in the cloud according to its temporal and spatial orders in the original (unannotated) skeleton sequence. Leveraging the colorized skeleton point cloud, we design an auto-encoder framework that can learn spatial-temporal features from the artificial color labels of skeleton joints effectively. Specifically, we design a two-steam pretraining network that leverages fine-grained and coarse-grained colorization to learn multi-scale spatial-temporal features. In addition, we design a Masked Skeleton Cloud Repainting task that can pretrain the designed auto-encoder framework to learn informative representations. We evaluate our skeleton cloud colorization approach with linear classifiers trained under different configurations, including unsupervised, semi-supervised, fully-supervised, and transfer learning settings. Extensive experiments on NTU RGB+D, NTU RGB+D 120, PKU-MMD, NW-UCLA, and UWA3D datasets show that the proposed method outperforms existing unsupervised and semi-supervised 3D action recognition methods by large margins and achieves competitive performance in supervised 3D action recognition as well.
<div id='section'>Paperid: <span id='pid'>116, <a href='https://arxiv.org/pdf/2303.10904.pdf' target='_blank'>https://arxiv.org/pdf/2303.10904.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lilang Lin, Jiahang Zhang, Jiaying Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.10904">Actionlet-Dependent Contrastive Learning for Unsupervised Skeleton-Based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The self-supervised pretraining paradigm has achieved great success in skeleton-based action recognition. However, these methods treat the motion and static parts equally, and lack an adaptive design for different parts, which has a negative impact on the accuracy of action recognition. To realize the adaptive action modeling of both parts, we propose an Actionlet-Dependent Contrastive Learning method (ActCLR). The actionlet, defined as the discriminative subset of the human skeleton, effectively decomposes motion regions for better action modeling. In detail, by contrasting with the static anchor without motion, we extract the motion region of the skeleton data, which serves as the actionlet, in an unsupervised manner. Then, centering on actionlet, a motion-adaptive data transformation method is built. Different data transformations are applied to actionlet and non-actionlet regions to introduce more diversity while maintaining their own characteristics. Meanwhile, we propose a semantic-aware feature pooling method to build feature representations among motion and static regions in a distinguished manner. Extensive experiments on NTU RGB+D and PKUMMD show that the proposed method achieves remarkable action recognition performance. More visualization and quantitative experiments demonstrate the effectiveness of our method. Our project website is available at https://langlandslin.github.io/projects/ActCLR/
<div id='section'>Paperid: <span id='pid'>117, <a href='https://arxiv.org/pdf/2301.13360.pdf' target='_blank'>https://arxiv.org/pdf/2301.13360.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ayman Ali, Ekkasit Pinyoanuntapong, Pu Wang, Mohsen Dorodchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.13360">Skeleton-based Human Action Recognition via Convolutional Neural Networks (CNN)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, there has been a remarkable increase in the interest towards skeleton-based action recognition within the research community, owing to its various advantageous features, including computational efficiency, representative features, and illumination invariance. Despite this, researchers continue to explore and investigate the most optimal way to represent human actions through skeleton representation and the extracted features. As a result, the growth and availability of human action recognition datasets have risen substantially. In addition, deep learning-based algorithms have gained widespread popularity due to the remarkable advancements in various computer vision tasks. Most state-of-the-art contributions in skeleton-based action recognition incorporate a Graph Neural Network (GCN) architecture for representing the human body and extracting features. Our research demonstrates that Convolutional Neural Networks (CNNs) can attain comparable results to GCN, provided that the proper training techniques, augmentations, and optimizers are applied. Our approach has been rigorously validated, and we have achieved a score of 95% on the NTU-60 dataset
<div id='section'>Paperid: <span id='pid'>118, <a href='https://arxiv.org/pdf/2211.13466.pdf' target='_blank'>https://arxiv.org/pdf/2211.13466.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahang Zhang, Lilang Lin, Jiaying Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.13466">Hierarchical Consistent Contrastive Learning for Skeleton-Based Action Recognition with Growing Augmentations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Contrastive learning has been proven beneficial for self-supervised skeleton-based action recognition. Most contrastive learning methods utilize carefully designed augmentations to generate different movement patterns of skeletons for the same semantics. However, it is still a pending issue to apply strong augmentations, which distort the images/skeletons' structures and cause semantic loss, due to their resulting unstable training. In this paper, we investigate the potential of adopting strong augmentations and propose a general hierarchical consistent contrastive learning framework (HiCLR) for skeleton-based action recognition. Specifically, we first design a gradual growing augmentation policy to generate multiple ordered positive pairs, which guide to achieve the consistency of the learned representation from different views. Then, an asymmetric loss is proposed to enforce the hierarchical consistency via a directional clustering operation in the feature space, pulling the representations from strongly augmented views closer to those from weakly augmented views for better generalizability. Meanwhile, we propose and evaluate three kinds of strong augmentations for 3D skeletons to demonstrate the effectiveness of our method. Extensive experiments show that HiCLR outperforms the state-of-the-art methods notably on three large-scale datasets, i.e., NTU60, NTU120, and PKUMMD.
<div id='section'>Paperid: <span id='pid'>119, <a href='https://arxiv.org/pdf/2508.15767.pdf' target='_blank'>https://arxiv.org/pdf/2508.15767.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinhyung Park, Javier Romero, Shunsuke Saito, Fabian Prada, Takaaki Shiratori, Yichen Xu, Federica Bogo, Shoou-I Yu, Kris Kitani, Rawal Khirodkar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15767">ATLAS: Decoupling Skeletal and Shape Parameters for Expressive Parametric Human Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Parametric body models offer expressive 3D representation of humans across a wide range of poses, shapes, and facial expressions, typically derived by learning a basis over registered 3D meshes. However, existing human mesh modeling approaches struggle to capture detailed variations across diverse body poses and shapes, largely due to limited training data diversity and restrictive modeling assumptions. Moreover, the common paradigm first optimizes the external body surface using a linear basis, then regresses internal skeletal joints from surface vertices. This approach introduces problematic dependencies between internal skeleton and outer soft tissue, limiting direct control over body height and bone lengths. To address these issues, we present ATLAS, a high-fidelity body model learned from 600k high-resolution scans captured using 240 synchronized cameras. Unlike previous methods, we explicitly decouple the shape and skeleton bases by grounding our mesh representation in the human skeleton. This decoupling enables enhanced shape expressivity, fine-grained customization of body attributes, and keypoint fitting independent of external soft-tissue characteristics. ATLAS outperforms existing methods by fitting unseen subjects in diverse poses more accurately, and quantitative evaluations show that our non-linear pose correctives more effectively capture complex poses compared to linear models.
<div id='section'>Paperid: <span id='pid'>120, <a href='https://arxiv.org/pdf/2503.20218.pdf' target='_blank'>https://arxiv.org/pdf/2503.20218.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haiyang Liu, Zhan Xu, Fa-Ting Hong, Hsin-Ping Huang, Yi Zhou, Yang Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20218">Video Motion Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Video Motion Graphs, a system designed to generate realistic human motion videos. Using a reference video and conditional signals such as music or motion tags, the system synthesizes new videos by first retrieving video clips with gestures matching the conditions and then generating interpolation frames to seamlessly connect clip boundaries. The core of our approach is HMInterp, a robust Video Frame Interpolation (VFI) model that enables seamless interpolation of discontinuous frames, even for complex motion scenarios like dancing. HMInterp i) employs a dual-branch interpolation approach, combining a Motion Diffusion Model for human skeleton motion interpolation with a diffusion-based video frame interpolation model for final frame generation. ii) adopts condition progressive training to effectively leverage identity strong and weak conditions, such as images and pose. These designs ensure both high video texture quality and accurate motion trajectory. Results show that our Video Motion Graphs outperforms existing generative- and retrieval-based methods for multi-modal conditioned human motion video generation. Project page can be found at https://h-liu1997.github.io/Video-Motion-Graphs/
<div id='section'>Paperid: <span id='pid'>121, <a href='https://arxiv.org/pdf/2207.09644.pdf' target='_blank'>https://arxiv.org/pdf/2207.09644.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxiao Chen, Long Zhao, Jianbo Yuan, Yu Tian, Zhaoyang Xia, Shijie Geng, Ligong Han, Dimitris N. Metaxas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.09644">Hierarchically Self-Supervised Transformer for Human Skeleton Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the success of fully-supervised human skeleton sequence modeling, utilizing self-supervised pre-training for skeleton sequence representation learning has been an active field because acquiring task-specific skeleton annotations at large scales is difficult. Recent studies focus on learning video-level temporal and discriminative information using contrastive learning, but overlook the hierarchical spatial-temporal nature of human skeletons. Different from such superficial supervision at the video level, we propose a self-supervised hierarchical pre-training scheme incorporated into a hierarchical Transformer-based skeleton sequence encoder (Hi-TRS), to explicitly capture spatial, short-term, and long-term temporal dependencies at frame, clip, and video levels, respectively. To evaluate the proposed self-supervised pre-training scheme with Hi-TRS, we conduct extensive experiments covering three skeleton-based downstream tasks including action recognition, action detection, and motion prediction. Under both supervised and semi-supervised evaluation protocols, our method achieves the state-of-the-art performance. Additionally, we demonstrate that the prior knowledge learned by our model in the pre-training stage has strong transfer capability for different downstream tasks.
<div id='section'>Paperid: <span id='pid'>122, <a href='https://arxiv.org/pdf/2205.10636.pdf' target='_blank'>https://arxiv.org/pdf/2205.10636.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingzhe He, Bastian Wandt, Helge Rhodin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2205.10636">AutoLink: Self-supervised Learning of Human Skeletons and Object Outlines by Linking Keypoints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Structured representations such as keypoints are widely used in pose transfer, conditional image generation, animation, and 3D reconstruction. However, their supervised learning requires expensive annotation for each target domain. We propose a self-supervised method that learns to disentangle object structure from the appearance with a graph of 2D keypoints linked by straight edges. Both the keypoint location and their pairwise edge weights are learned, given only a collection of images depicting the same object class. The resulting graph is interpretable, for example, AutoLink recovers the human skeleton topology when applied to images showing people. Our key ingredients are i) an encoder that predicts keypoint locations in an input image, ii) a shared graph as a latent variable that links the same pairs of keypoints in every image, iii) an intermediate edge map that combines the latent graph edge weights and keypoint locations in a soft, differentiable manner, and iv) an inpainting objective on randomly masked images. Although simpler, AutoLink outperforms existing self-supervised methods on the established keypoint and pose estimation benchmarks and paves the way for structure-conditioned generative models on more diverse datasets. Project website: https://xingzhehe.github.io/autolink/.
<div id='section'>Paperid: <span id='pid'>123, <a href='https://arxiv.org/pdf/2508.08944.pdf' target='_blank'>https://arxiv.org/pdf/2508.08944.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenhan Wu, Zhishuai Guo, Chen Chen, Aidong Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08944">UniSTFormer: Unified Spatio-Temporal Lightweight Transformer for Efficient Skeleton-Based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeleton-based action recognition (SAR) has achieved impressive progress with transformer architectures. However, existing methods often rely on complex module compositions and heavy designs, leading to increased parameter counts, high computational costs, and limited scalability. In this paper, we propose a unified spatio-temporal lightweight transformer framework that integrates spatial and temporal modeling within a single attention module, eliminating the need for separate temporal modeling blocks. This approach reduces redundant computations while preserving temporal awareness within the spatial modeling process. Furthermore, we introduce a simplified multi-scale pooling fusion module that combines local and global pooling pathways to enhance the model's ability to capture fine-grained local movements and overarching global motion patterns. Extensive experiments on benchmark datasets demonstrate that our lightweight model achieves a superior balance between accuracy and efficiency, reducing parameter complexity by over 58% and lowering computational cost by over 60% compared to state-of-the-art transformer-based baselines, while maintaining competitive recognition performance.
<div id='section'>Paperid: <span id='pid'>124, <a href='https://arxiv.org/pdf/2506.22179.pdf' target='_blank'>https://arxiv.org/pdf/2506.22179.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenhan Wu, Zhishuai Guo, Chen Chen, Hongfei Xue, Aidong Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.22179">Frequency-Semantic Enhanced Variational Autoencoder for Zero-Shot Skeleton-based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Zero-shot skeleton-based action recognition aims to develop models capable of identifying actions beyond the categories encountered during training. Previous approaches have primarily focused on aligning visual and semantic representations but often overlooked the importance of fine-grained action patterns in the semantic space (e.g., the hand movements in drinking water and brushing teeth). To address these limitations, we propose a Frequency-Semantic Enhanced Variational Autoencoder (FS-VAE) to explore the skeleton semantic representation learning with frequency decomposition. FS-VAE consists of three key components: 1) a frequency-based enhancement module with high- and low-frequency adjustments to enrich the skeletal semantics learning and improve the robustness of zero-shot action recognition; 2) a semantic-based action description with multilevel alignment to capture both local details and global correspondence, effectively bridging the semantic gap and compensating for the inherent loss of information in skeleton sequences; 3) a calibrated cross-alignment loss that enables valid skeleton-text pairs to counterbalance ambiguous ones, mitigating discrepancies and ambiguities in skeleton and text features, thereby ensuring robust alignment. Evaluations on the benchmarks demonstrate the effectiveness of our approach, validating that frequency-enhanced semantic features enable robust differentiation of visually and semantically similar action clusters, improving zero-shot action recognition.
<div id='section'>Paperid: <span id='pid'>125, <a href='https://arxiv.org/pdf/2503.20436.pdf' target='_blank'>https://arxiv.org/pdf/2503.20436.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muxin Pu, Mei Kuan Lim, Chun Yong Chong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20436">Siformer: Feature-isolated Transformer for Efficient Skeleton-based Sign Language Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sign language recognition (SLR) refers to interpreting sign language glosses from given videos automatically. This research area presents a complex challenge in computer vision because of the rapid and intricate movements inherent in sign languages, which encompass hand gestures, body postures, and even facial expressions. Recently, skeleton-based action recognition has attracted increasing attention due to its ability to handle variations in subjects and backgrounds independently. However, current skeleton-based SLR methods exhibit three limitations: 1) they often neglect the importance of realistic hand poses, where most studies train SLR models on non-realistic skeletal representations; 2) they tend to assume complete data availability in both training or inference phases, and capture intricate relationships among different body parts collectively; 3) these methods treat all sign glosses uniformly, failing to account for differences in complexity levels regarding skeletal representations. To enhance the realism of hand skeletal representations, we present a kinematic hand pose rectification method for enforcing constraints. Mitigating the impact of missing data, we propose a feature-isolated mechanism to focus on capturing local spatial-temporal context. This method captures the context concurrently and independently from individual features, thus enhancing the robustness of the SLR model. Additionally, to adapt to varying complexity levels of sign glosses, we develop an input-adaptive inference approach to optimise computational efficiency and accuracy. Experimental results demonstrate the effectiveness of our approach, as evidenced by achieving a new state-of-the-art (SOTA) performance on WLASL100 and LSA64. For WLASL100, we achieve a top-1 accuracy of 86.50\%, marking a relative improvement of 2.39% over the previous SOTA. For LSA64, we achieve a top-1 accuracy of 99.84%.
<div id='section'>Paperid: <span id='pid'>126, <a href='https://arxiv.org/pdf/2412.20621.pdf' target='_blank'>https://arxiv.org/pdf/2412.20621.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenhan Wu, Pengfei Wang, Chen Chen, Aidong Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20621">FreqMixFormerV2: Lightweight Frequency-aware Mixed Transformer for Human Skeleton Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transformer-based human skeleton action recognition has been developed for years. However, the complexity and high parameter count demands of these models hinder their practical applications, especially in resource-constrained environments. In this work, we propose FreqMixForemrV2, which was built upon the Frequency-aware Mixed Transformer (FreqMixFormer) for identifying subtle and discriminative actions with pioneered frequency-domain analysis. We design a lightweight architecture that maintains robust performance while significantly reducing the model complexity. This is achieved through a redesigned frequency operator that optimizes high-frequency and low-frequency parameter adjustments, and a simplified frequency-aware attention module. These improvements result in a substantial reduction in model parameters, enabling efficient deployment with only a minimal sacrifice in accuracy. Comprehensive evaluations of standard datasets (NTU RGB+D, NTU RGB+D 120, and NW-UCLA datasets) demonstrate that the proposed model achieves a superior balance between efficiency and accuracy, outperforming state-of-the-art methods with only 60% of the parameters.
<div id='section'>Paperid: <span id='pid'>127, <a href='https://arxiv.org/pdf/2404.09499.pdf' target='_blank'>https://arxiv.org/pdf/2404.09499.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuaiying Hou, Hongyu Tao, Junheng Fang, Changqing Zou, Hujun Bao, Weiwei Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.09499">Learning Human Motion from Monocular Videos via Cross-Modal Manifold Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning 3D human motion from 2D inputs is a fundamental task in the realms of computer vision and computer graphics. Many previous methods grapple with this inherently ambiguous task by introducing motion priors into the learning process. However, these approaches face difficulties in defining the complete configurations of such priors or training a robust model. In this paper, we present the Video-to-Motion Generator (VTM), which leverages motion priors through cross-modal latent feature space alignment between 3D human motion and 2D inputs, namely videos and 2D keypoints. To reduce the complexity of modeling motion priors, we model the motion data separately for the upper and lower body parts. Additionally, we align the motion data with a scale-invariant virtual skeleton to mitigate the interference of human skeleton variations to the motion priors. Evaluated on AIST++, the VTM showcases state-of-the-art performance in reconstructing 3D human motion from monocular videos. Notably, our VTM exhibits the capabilities for generalization to unseen view angles and in-the-wild videos.
<div id='section'>Paperid: <span id='pid'>128, <a href='https://arxiv.org/pdf/2402.01313.pdf' target='_blank'>https://arxiv.org/pdf/2402.01313.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Felix Tempel, Inga StrÃ¼mke, Espen Alexander F. Ihlen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.01313">AutoGCN -- Towards Generic Human Activity Recognition with Neural Architecture Search</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces AutoGCN, a generic Neural Architecture Search (NAS) algorithm for Human Activity Recognition (HAR) using Graph Convolution Networks (GCNs). HAR has gained attention due to advances in deep learning, increased data availability, and enhanced computational capabilities. At the same time, GCNs have shown promising results in modeling relationships between body key points in a skeletal graph. While domain experts often craft dataset-specific GCN-based methods, their applicability beyond this specific context is severely limited. AutoGCN seeks to address this limitation by simultaneously searching for the ideal hyperparameters and architecture combination within a versatile search space using a reinforcement controller while balancing optimal exploration and exploitation behavior with a knowledge reservoir during the search process. We conduct extensive experiments on two large-scale datasets focused on skeleton-based action recognition to assess the proposed algorithm's performance. Our experimental results underscore the effectiveness of AutoGCN in constructing optimal GCN architectures for HAR, outperforming conventional NAS and GCN methods, as well as random search. These findings highlight the significance of a diverse search space and an expressive input representation to enhance the network performance and generalizability.
<div id='section'>Paperid: <span id='pid'>129, <a href='https://arxiv.org/pdf/2309.06462.pdf' target='_blank'>https://arxiv.org/pdf/2309.06462.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Syed Waleed Hyder, Muhammad Usama, Anas Zafar, Muhammad Naufil, Fawad Javed Fateh, Andrey Konin, M. Zeeshan Zia, Quoc-Huy Tran
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.06462">Action Segmentation Using 2D Skeleton Heatmaps and Multi-Modality Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a 2D skeleton-based action segmentation method with applications in fine-grained human activity recognition. In contrast with state-of-the-art methods which directly take sequences of 3D skeleton coordinates as inputs and apply Graph Convolutional Networks (GCNs) for spatiotemporal feature learning, our main idea is to use sequences of 2D skeleton heatmaps as inputs and employ Temporal Convolutional Networks (TCNs) to extract spatiotemporal features. Despite lacking 3D information, our approach yields comparable/superior performances and better robustness against missing keypoints than previous methods on action segmentation datasets. Moreover, we improve the performances further by using both 2D skeleton heatmaps and RGB videos as inputs. To our best knowledge, this is the first work to utilize 2D skeleton heatmap inputs and the first work to explore 2D skeleton+RGB fusion for action segmentation.
<div id='section'>Paperid: <span id='pid'>130, <a href='https://arxiv.org/pdf/2305.02324.pdf' target='_blank'>https://arxiv.org/pdf/2305.02324.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ding Li, Yongqiang Tang, Zhizhong Zhang, Wensheng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.02324">Cross-Stream Contrastive Learning for Self-Supervised Skeleton-Based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-supervised skeleton-based action recognition enjoys a rapid growth along with the development of contrastive learning. The existing methods rely on imposing invariance to augmentations of 3D skeleton within a single data stream, which merely leverages the easy positive pairs and limits the ability to explore the complicated movement patterns. In this paper, we advocate that the defect of single-stream contrast and the lack of necessary feature transformation are responsible for easy positives, and therefore propose a Cross-Stream Contrastive Learning framework for skeleton-based action Representation learning (CSCLR). Specifically, the proposed CSCLR not only utilizes intra-stream contrast pairs, but introduces inter-stream contrast pairs as hard samples to formulate a better representation learning. Besides, to further exploit the potential of positive pairs and increase the robustness of self-supervised representation learning, we propose a Positive Feature Transformation (PFT) strategy which adopts feature-level manipulation to increase the variance of positive pairs. To validate the effectiveness of our method, we conduct extensive experiments on three benchmark datasets NTU-RGB+D 60, NTU-RGB+D 120 and PKU-MMD. Experimental results show that our proposed CSCLR exceeds the state-of-the-art methods on a diverse range of evaluation protocols.
<div id='section'>Paperid: <span id='pid'>131, <a href='https://arxiv.org/pdf/2304.00858.pdf' target='_blank'>https://arxiv.org/pdf/2304.00858.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qianhui Men, Edmond S. L. Ho, Hubert P. H. Shum, Howard Leung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.00858">Focalized Contrastive View-invariant Learning for Self-supervised Skeleton-based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning view-invariant representation is a key to improving feature discrimination power for skeleton-based action recognition. Existing approaches cannot effectively remove the impact of viewpoint due to the implicit view-dependent representations. In this work, we propose a self-supervised framework called Focalized Contrastive View-invariant Learning (FoCoViL), which significantly suppresses the view-specific information on the representation space where the viewpoints are coarsely aligned. By maximizing mutual information with an effective contrastive loss between multi-view sample pairs, FoCoViL associates actions with common view-invariant properties and simultaneously separates the dissimilar ones. We further propose an adaptive focalization method based on pairwise similarity to enhance contrastive learning for a clearer cluster boundary in the learned space. Different from many existing self-supervised representation learning work that rely heavily on supervised classifiers, FoCoViL performs well on both unsupervised and supervised classifiers with superior recognition performance. Extensive experiments also show that the proposed contrastive-based focalization generates a more discriminative latent representation.
<div id='section'>Paperid: <span id='pid'>132, <a href='https://arxiv.org/pdf/2506.19747.pdf' target='_blank'>https://arxiv.org/pdf/2506.19747.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stephanie KÃ¤s, Sven Peter, Henrik Thillmann, Anton Burenko, David Benjamin Adrian, Dennis Mack, Timm Linder, Bastian Leibe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.19747">Systematic Comparison of Projection Methods for Monocular 3D Human Pose Estimation on Fisheye Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fisheye cameras offer robots the ability to capture human movements across a wider field of view (FOV) than standard pinhole cameras, making them particularly useful for applications in human-robot interaction and automotive contexts. However, accurately detecting human poses in fisheye images is challenging due to the curved distortions inherent to fisheye optics. While various methods for undistorting fisheye images have been proposed, their effectiveness and limitations for poses that cover a wide FOV has not been systematically evaluated in the context of absolute human pose estimation from monocular fisheye images. To address this gap, we evaluate the impact of pinhole, equidistant and double sphere camera models, as well as cylindrical projection methods, on 3D human pose estimation accuracy. We find that in close-up scenarios, pinhole projection is inadequate, and the optimal projection method varies with the FOV covered by the human pose. The usage of advanced fisheye models like the double sphere model significantly enhances 3D human pose estimation accuracy. We propose a heuristic for selecting the appropriate projection model based on the detection bounding box to enhance prediction quality. Additionally, we introduce and evaluate on our novel dataset FISHnCHIPS, which features 3D human skeleton annotations in fisheye images, including images from unconventional angles, such as extreme close-ups, ground-mounted cameras, and wide-FOV poses, available at: https://www.vision.rwth-aachen.de/fishnchips
<div id='section'>Paperid: <span id='pid'>133, <a href='https://arxiv.org/pdf/2506.13897.pdf' target='_blank'>https://arxiv.org/pdf/2506.13897.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thomas Kreutz, Max MÃ¼hlhÃ¤user, Alejandro Sanchez Guinea
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13897">DeSPITE: Exploring Contrastive Deep Skeleton-Pointcloud-IMU-Text Embeddings for Advanced Point Cloud Human Activity Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite LiDAR (Light Detection and Ranging) being an effective privacy-preserving alternative to RGB cameras to perceive human activities, it remains largely underexplored in the context of multi-modal contrastive pre-training for human activity understanding (e.g., human activity recognition (HAR), retrieval, or person re-identification (RE-ID)). To close this gap, our work explores learning the correspondence between LiDAR point clouds, human skeleton poses, IMU data, and text in a joint embedding space. More specifically, we present DeSPITE, a Deep Skeleton-Pointcloud-IMU-Text Embedding model, which effectively learns a joint embedding space across these four modalities. At the heart of our empirical exploration, we have combined the existing LIPD and Babel datasets, which enabled us to synchronize data of all four modalities, allowing us to explore the learning of a new joint embedding space. Our experiments demonstrate novel human activity understanding tasks for point cloud sequences enabled through DeSPITE, including Skeleton<->Pointcloud<->IMU matching, retrieval, and temporal moment retrieval. Furthermore, we show that DeSPITE is an effective pre-training strategy for point cloud HAR through experiments in MSR-Action3D and HMPEAR.
<div id='section'>Paperid: <span id='pid'>134, <a href='https://arxiv.org/pdf/2504.13140.pdf' target='_blank'>https://arxiv.org/pdf/2504.13140.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jongseo Lee, Wooil Lee, Gyeong-Moon Park, Seong Tae Kim, Jinwoo Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13140">PCBEAR: Pose Concept Bottleneck for Explainable Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human action recognition (HAR) has achieved impressive results with deep learning models, but their decision-making process remains opaque due to their black-box nature. Ensuring interpretability is crucial, especially for real-world applications requiring transparency and accountability. Existing video XAI methods primarily rely on feature attribution or static textual concepts, both of which struggle to capture motion dynamics and temporal dependencies essential for action understanding. To address these challenges, we propose Pose Concept Bottleneck for Explainable Action Recognition (PCBEAR), a novel concept bottleneck framework that introduces human pose sequences as motion-aware, structured concepts for video action recognition. Unlike methods based on pixel-level features or static textual descriptions, PCBEAR leverages human skeleton poses, which focus solely on body movements, providing robust and interpretable explanations of motion dynamics. We define two types of pose-based concepts: static pose concepts for spatial configurations at individual frames, and dynamic pose concepts for motion patterns across multiple frames. To construct these concepts, PCBEAR applies clustering to video pose sequences, allowing for automatic discovery of meaningful concepts without manual annotation. We validate PCBEAR on KTH, Penn-Action, and HAA500, showing that it achieves high classification performance while offering interpretable, motion-driven explanations. Our method provides both strong predictive performance and human-understandable insights into the model's reasoning process, enabling test-time interventions for debugging and improving model behavior.
<div id='section'>Paperid: <span id='pid'>135, <a href='https://arxiv.org/pdf/2411.19544.pdf' target='_blank'>https://arxiv.org/pdf/2411.19544.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Niki Martinel, Mariano Serrao, Christian Micheloni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19544">SkelMamba: A State Space Model for Efficient Skeleton Action Recognition of Neurological Disorders</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel state-space model (SSM)-based framework for skeleton-based human action recognition, with an anatomically-guided architecture that improves state-of-the-art performance in both clinical diagnostics and general action recognition tasks. Our approach decomposes skeletal motion analysis into spatial, temporal, and spatio-temporal streams, using channel partitioning to capture distinct movement characteristics efficiently. By implementing a structured, multi-directional scanning strategy within SSMs, our model captures local joint interactions and global motion patterns across multiple anatomical body parts. This anatomically-aware decomposition enhances the ability to identify subtle motion patterns critical in medical diagnosis, such as gait anomalies associated with neurological conditions. On public action recognition benchmarks, i.e., NTU RGB+D, NTU RGB+D 120, and NW-UCLA, our model outperforms current state-of-the-art methods, achieving accuracy improvements up to $3.2\%$ with lower computational complexity than previous leading transformer-based models. We also introduce a novel medical dataset for motion-based patient neurological disorder analysis to validate our method's potential in automated disease diagnosis.
<div id='section'>Paperid: <span id='pid'>136, <a href='https://arxiv.org/pdf/2408.08671.pdf' target='_blank'>https://arxiv.org/pdf/2408.08671.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qichen Zheng, Yi Yu, Siyuan Yang, Jun Liu, Kwok-Yan Lam, Alex Kot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.08671">Towards Physical World Backdoor Attacks against Skeleton Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeleton Action Recognition (SAR) has attracted significant interest for its efficient representation of the human skeletal structure. Despite its advancements, recent studies have raised security concerns in SAR models, particularly their vulnerability to adversarial attacks. However, such strategies are limited to digital scenarios and ineffective in physical attacks, limiting their real-world applicability. To investigate the vulnerabilities of SAR in the physical world, we introduce the Physical Skeleton Backdoor Attacks (PSBA), the first exploration of physical backdoor attacks against SAR. Considering the practicalities of physical execution, we introduce a novel trigger implantation method that integrates infrequent and imperceivable actions as triggers into the original skeleton data. By incorporating a minimal amount of this manipulated data into the training set, PSBA enables the system misclassify any skeleton sequences into the target class when the trigger action is present. We examine the resilience of PSBA in both poisoned and clean-label scenarios, demonstrating its efficacy across a range of datasets, poisoning ratios, and model architectures. Additionally, we introduce a trigger-enhancing strategy to strengthen attack performance in the clean label setting. The robustness of PSBA is tested against three distinct backdoor defenses, and the stealthiness of PSBA is evaluated using two quantitative metrics. Furthermore, by employing a Kinect V2 camera, we compile a dataset of human actions from the real world to mimic physical attack situations, with our findings confirming the effectiveness of our proposed attacks. Our project website can be found at https://qichenzheng.github.io/psba-website.
<div id='section'>Paperid: <span id='pid'>137, <a href='https://arxiv.org/pdf/2404.07487.pdf' target='_blank'>https://arxiv.org/pdf/2404.07487.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Chen, Jingcai Guo, Tian He, Ling Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.07487">Fine-Grained Side Information Guided Dual-Prompts for Zero-Shot Skeleton Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeleton-based zero-shot action recognition aims to recognize unknown human actions based on the learned priors of the known skeleton-based actions and a semantic descriptor space shared by both known and unknown categories. However, previous works focus on establishing the bridges between the known skeleton representation space and semantic descriptions space at the coarse-grained level for recognizing unknown action categories, ignoring the fine-grained alignment of these two spaces, resulting in suboptimal performance in distinguishing high-similarity action categories. To address these challenges, we propose a novel method via Side information and dual-prompts learning for skeleton-based zero-shot action recognition (STAR) at the fine-grained level. Specifically, 1) we decompose the skeleton into several parts based on its topology structure and introduce the side information concerning multi-part descriptions of human body movements for alignment between the skeleton and the semantic space at the fine-grained level; 2) we design the visual-attribute and semantic-part prompts to improve the intra-class compactness within the skeleton space and inter-class separability within the semantic space, respectively, to distinguish the high-similarity actions. Extensive experiments show that our method achieves state-of-the-art performance in ZSL and GZSL settings on NTU RGB+D, NTU RGB+D 120, and PKU-MMD datasets.
<div id='section'>Paperid: <span id='pid'>138, <a href='https://arxiv.org/pdf/2308.13866.pdf' target='_blank'>https://arxiv.org/pdf/2308.13866.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yukun Su, Guosheng Lin, Qingyao Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.13866">Improving Video Violence Recognition with Human Interaction Learning on 3D Skeleton Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning has proved to be very effective in video action recognition. Video violence recognition attempts to learn the human multi-dynamic behaviours in more complex scenarios. In this work, we develop a method for video violence recognition from a new perspective of skeleton points. Unlike the previous works, we first formulate 3D skeleton point clouds from human skeleton sequences extracted from videos and then perform interaction learning on these 3D skeleton point clouds. Specifically, we propose two types of Skeleton Points Interaction Learning (SPIL) strategies: (i) Local-SPIL: by constructing a specific weight distribution strategy between local regional points, Local-SPIL aims to selectively focus on the most relevant parts of them based on their features and spatial-temporal position information. In order to capture diverse types of relation information, a multi-head mechanism is designed to aggregate different features from independent heads to jointly handle different types of relationships between points. (ii) Global-SPIL: to better learn and refine the features of the unordered and unstructured skeleton points, Global-SPIL employs the self-attention layer that operates directly on the sampled points, which can help to make the output more permutation-invariant and well-suited for our task. Extensive experimental results validate the effectiveness of our approach and show that our model outperforms the existing networks and achieves new state-of-the-art performance on video violence datasets.
<div id='section'>Paperid: <span id='pid'>139, <a href='https://arxiv.org/pdf/2509.03609.pdf' target='_blank'>https://arxiv.org/pdf/2509.03609.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengkai Sun, Zefan Zhang, Jianfeng Dong, Zhiyong Cheng, Xiaojun Chang, Meng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03609">Towards Efficient General Feature Prediction in Masked Skeleton Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in the masked autoencoder (MAE) paradigm have significantly propelled self-supervised skeleton-based action recognition. However, most existing approaches limit reconstruction targets to raw joint coordinates or their simple variants, resulting in computational redundancy and limited semantic representation. To address this, we propose a novel General Feature Prediction framework (GFP) for efficient mask skeleton modeling. Our key innovation is replacing conventional low-level reconstruction with high-level feature prediction that spans from local motion patterns to global semantic representations. Specifically, we introduce a collaborative learning framework where a lightweight target generation network dynamically produces diversified supervision signals across spatial-temporal hierarchies, avoiding reliance on pre-computed offline features. The framework incorporates constrained optimization to ensure feature diversity while preventing model collapse. Experiments on NTU RGB+D 60, NTU RGB+D 120 and PKU-MMD demonstrate the benefits of our approach: Computational efficiency (with 6.2$\times$ faster training than standard masked skeleton modeling methods) and superior representation quality, achieving state-of-the-art performance in various downstream tasks.
<div id='section'>Paperid: <span id='pid'>140, <a href='https://arxiv.org/pdf/2506.18721.pdf' target='_blank'>https://arxiv.org/pdf/2506.18721.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dustin Aganian, Erik Franze, Markus Eisenbach, Horst-Michael Gross
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18721">Including Semantic Information via Word Embeddings for Skeleton-based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective human action recognition is widely used for cobots in Industry 4.0 to assist in assembly tasks. However, conventional skeleton-based methods often lose keypoint semantics, limiting their effectiveness in complex interactions. In this work, we introduce a novel approach to skeleton-based action recognition that enriches input representations by leveraging word embeddings to encode semantic information. Our method replaces one-hot encodings with semantic volumes, enabling the model to capture meaningful relationships between joints and objects. Through extensive experiments on multiple assembly datasets, we demonstrate that our approach significantly improves classification performance, and enhances generalization capabilities by simultaneously supporting different skeleton types and object classes. Our findings highlight the potential of incorporating semantic information to enhance skeleton-based action recognition in dynamic and diverse environments.
<div id='section'>Paperid: <span id='pid'>141, <a href='https://arxiv.org/pdf/2504.21266.pdf' target='_blank'>https://arxiv.org/pdf/2504.21266.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhifu Zhao, Hanyang Hua, Jianan Li, Shaoxin Wu, Fu Li, Yangtao Zhou, Yang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21266">CoCoDiff: Diversifying Skeleton Action Features via Coarse-Fine Text-Co-Guided Latent Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In action recognition tasks, feature diversity is essential for enhancing model generalization and performance. Existing methods typically promote feature diversity by expanding the training data in the sample space, which often leads to inefficiencies and semantic inconsistencies. To overcome these problems, we propose a novel Coarse-fine text co-guidance Diffusion model (CoCoDiff). CoCoDiff generates diverse yet semantically consistent features in the latent space by leveraging diffusion and multi-granularity textual guidance. Specifically, our approach feeds spatio-temporal features extracted from skeleton sequences into a latent diffusion model to generate diverse action representations. Meanwhile, we introduce a coarse-fine text co-guided strategy that leverages textual information from large language models (LLMs) to ensure semantic consistency between the generated features and the original inputs. It is noted that CoCoDiff operates as a plug-and-play auxiliary module during training, incurring no additional inference cost. Extensive experiments demonstrate that CoCoDiff achieves SOTA performance on skeleton-based action recognition benchmarks, including NTU RGB+D, NTU RGB+D 120 and Kinetics-Skeleton.
<div id='section'>Paperid: <span id='pid'>142, <a href='https://arxiv.org/pdf/2504.08344.pdf' target='_blank'>https://arxiv.org/pdf/2504.08344.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Renda Li, Xiaohua Qi, Qiang Ling, Jun Yu, Ziyi Chen, Peng Chang, Mei HanJing Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08344">EasyGenNet: An Efficient Framework for Audio-Driven Gesture Video Generation Based on Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-driven cospeech video generation typically involves two stages: speech-to-gesture and gesture-to-video. While significant advances have been made in speech-to-gesture generation, synthesizing natural expressions and gestures remains challenging in gesture-to-video systems. In order to improve the generation effect, previous works adopted complex input and training strategies and required a large amount of data sets for pre-training, which brought inconvenience to practical applications. We propose a simple one-stage training method and a temporal inference method based on a diffusion model to synthesize realistic and continuous gesture videos without the need for additional training of temporal modules.The entire model makes use of existing pre-trained weights, and only a few thousand frames of data are needed for each character at a time to complete fine-tuning. Built upon the video generator, we introduce a new audio-to-video pipeline to synthesize co-speech videos, using 2D human skeleton as the intermediate motion representation. Our experiments show that our method outperforms existing GAN-based and diffusion-based methods.
<div id='section'>Paperid: <span id='pid'>143, <a href='https://arxiv.org/pdf/2503.15126.pdf' target='_blank'>https://arxiv.org/pdf/2503.15126.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyu Ji, Bowen Chen, Weihong Ren, Wenze Huang, Zhihao Yang, Zhiyong Wang, Honghai Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15126">Text-Derived Relational Graph-Enhanced Network for Skeleton-Based Action Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeleton-based Temporal Action Segmentation (STAS) aims to segment and recognize various actions from long, untrimmed sequences of human skeletal movements. Current STAS methods typically employ spatio-temporal modeling to establish dependencies among joints as well as frames, and utilize one-hot encoding with cross-entropy loss for frame-wise classification supervision. However, these methods overlook the intrinsic correlations among joints and actions within skeletal features, leading to a limited understanding of human movements. To address this, we propose a Text-Derived Relational Graph-Enhanced Network (TRG-Net) that leverages prior graphs generated by Large Language Models (LLM) to enhance both modeling and supervision. For modeling, the Dynamic Spatio-Temporal Fusion Modeling (DSFM) method incorporates Text-Derived Joint Graphs (TJG) with channel- and frame-level dynamic adaptation to effectively model spatial relations, while integrating spatio-temporal core features during temporal modeling. For supervision, the Absolute-Relative Inter-Class Supervision (ARIS) method employs contrastive learning between action features and text embeddings to regularize the absolute class distributions, and utilizes Text-Derived Action Graphs (TAG) to capture the relative inter-class relationships among action features. Additionally, we propose a Spatial-Aware Enhancement Processing (SAEP) method, which incorporates random joint occlusion and axial rotation to enhance spatial generalization. Performance evaluations on four public datasets demonstrate that TRG-Net achieves state-of-the-art results.
<div id='section'>Paperid: <span id='pid'>144, <a href='https://arxiv.org/pdf/2501.07104.pdf' target='_blank'>https://arxiv.org/pdf/2501.07104.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sen Peng, Weixing Xie, Zilong Wang, Xiaohu Guo, Zhonggui Chen, Baorong Yang, Xiao Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07104">RMAvatar: Photorealistic Human Avatar Reconstruction from Monocular Video Based on Rectified Mesh-embedded Gaussians</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce RMAvatar, a novel human avatar representation with Gaussian splatting embedded on mesh to learn clothed avatar from a monocular video. We utilize the explicit mesh geometry to represent motion and shape of a virtual human and implicit appearance rendering with Gaussian Splatting. Our method consists of two main modules: Gaussian initialization module and Gaussian rectification module. We embed Gaussians into triangular faces and control their motion through the mesh, which ensures low-frequency motion and surface deformation of the avatar. Due to the limitations of LBS formula, the human skeleton is hard to control complex non-rigid transformations. We then design a pose-related Gaussian rectification module to learn fine-detailed non-rigid deformations, further improving the realism and expressiveness of the avatar. We conduct extensive experiments on public datasets, RMAvatar shows state-of-the-art performance on both rendering quality and quantitative evaluations. Please see our project page at https://rm-avatar.github.io.
<div id='section'>Paperid: <span id='pid'>145, <a href='https://arxiv.org/pdf/2411.10745.pdf' target='_blank'>https://arxiv.org/pdf/2411.10745.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jeonghyeok Do, Munchurl Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10745">Bridging the Skeleton-Text Modality Gap: Diffusion-Powered Modality Alignment for Zero-shot Skeleton-based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In zero-shot skeleton-based action recognition (ZSAR), aligning skeleton features with the text features of action labels is essential for accurately predicting unseen actions. ZSAR faces a fundamental challenge in bridging the modality gap between the two-kind features, which severely limits generalization to unseen actions. Previous methods focus on direct alignment between skeleton and text latent spaces, but the modality gaps between these spaces hinder robust generalization learning. Motivated by the success of diffusion models in multi-modal alignment (e.g., text-to-image, text-to-video), we firstly present a diffusion-based skeleton-text alignment framework for ZSAR. Our approach, Triplet Diffusion for Skeleton-Text Matching (TDSM), focuses on cross-alignment power of diffusion models rather than their generative capability. Specifically, TDSM aligns skeleton features with text prompts by incorporating text features into the reverse diffusion process, where skeleton features are denoised under text guidance, forming a unified skeleton-text latent space for robust matching. To enhance discriminative power, we introduce a triplet diffusion (TD) loss that encourages our TDSM to correct skeleton-text matches while pushing them apart for different action classes. Our TDSM significantly outperforms very recent state-of-the-art methods with significantly large margins of 2.36%-point to 13.05%-point, demonstrating superior accuracy and scalability in zero-shot settings through effective skeleton-text matching.
<div id='section'>Paperid: <span id='pid'>146, <a href='https://arxiv.org/pdf/2404.19383.pdf' target='_blank'>https://arxiv.org/pdf/2404.19383.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhendong Liu, Haifeng Xia, Tong Guo, Libo Sun, Ming Shao, Siyu Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.19383">Cross-Block Fine-Grained Semantic Cascade for Skeleton-Based Sports Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human action video recognition has recently attracted more attention in applications such as video security and sports posture correction. Popular solutions, including graph convolutional networks (GCNs) that model the human skeleton as a spatiotemporal graph, have proven very effective. GCNs-based methods with stacked blocks usually utilize top-layer semantics for classification/annotation purposes. Although the global features learned through the procedure are suitable for the general classification, they have difficulty capturing fine-grained action change across adjacent frames -- decisive factors in sports actions. In this paper, we propose a novel ``Cross-block Fine-grained Semantic Cascade (CFSC)'' module to overcome this challenge. In summary, the proposed CFSC progressively integrates shallow visual knowledge into high-level blocks to allow networks to focus on action details. In particular, the CFSC module utilizes the GCN feature maps produced at different levels, as well as aggregated features from proceeding levels to consolidate fine-grained features. In addition, a dedicated temporal convolution is applied at each level to learn short-term temporal features, which will be carried over from shallow to deep layers to maximize the leverage of low-level details. This cross-block feature aggregation methodology, capable of mitigating the loss of fine-grained information, has resulted in improved performance. Last, FD-7, a new action recognition dataset for fencing sports, was collected and will be made publicly available. Experimental results and empirical analysis on public benchmarks (FSD-10) and self-collected (FD-7) demonstrate the advantage of our CFSC module on learning discriminative patterns for action classification over others.
<div id='section'>Paperid: <span id='pid'>147, <a href='https://arxiv.org/pdf/2404.06152.pdf' target='_blank'>https://arxiv.org/pdf/2404.06152.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arnab Dey, Di Yang, Antitza Dantcheva, Jean Martinet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.06152">HFNeRF: Learning Human Biomechanic Features with Neural Radiance Fields</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent advancements in novel view synthesis, generalizable Neural Radiance Fields (NeRF) based methods applied to human subjects have shown remarkable results in generating novel views from few images. However, this generalization ability cannot capture the underlying structural features of the skeleton shared across all instances. Building upon this, we introduce HFNeRF: a novel generalizable human feature NeRF aimed at generating human biomechanic features using a pre-trained image encoder. While previous human NeRF methods have shown promising results in the generation of photorealistic virtual avatars, such methods lack underlying human structure or biomechanic features such as skeleton or joint information that are crucial for downstream applications including Augmented Reality (AR)/Virtual Reality (VR). HFNeRF leverages 2D pre-trained foundation models toward learning human features in 3D using neural rendering, and then volume rendering towards generating 2D feature maps. We evaluate HFNeRF in the skeleton estimation task by predicting heatmaps as features. The proposed method is fully differentiable, allowing to successfully learn color, geometry, and human skeleton in a simultaneous manner. This paper presents preliminary results of HFNeRF, illustrating its potential in generating realistic virtual avatars with biomechanic features using NeRF.
<div id='section'>Paperid: <span id='pid'>148, <a href='https://arxiv.org/pdf/2403.09508.pdf' target='_blank'>https://arxiv.org/pdf/2403.09508.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jeonghyeok Do, Munchurl Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.09508">SkateFormer: Skeletal-Temporal Transformer for Human Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeleton-based action recognition, which classifies human actions based on the coordinates of joints and their connectivity within skeleton data, is widely utilized in various scenarios. While Graph Convolutional Networks (GCNs) have been proposed for skeleton data represented as graphs, they suffer from limited receptive fields constrained by joint connectivity. To address this limitation, recent advancements have introduced transformer-based methods. However, capturing correlations between all joints in all frames requires substantial memory resources. To alleviate this, we propose a novel approach called Skeletal-Temporal Transformer (SkateFormer) that partitions joints and frames based on different types of skeletal-temporal relation (Skate-Type) and performs skeletal-temporal self-attention (Skate-MSA) within each partition. We categorize the key skeletal-temporal relations for action recognition into a total of four distinct types. These types combine (i) two skeletal relation types based on physically neighboring and distant joints, and (ii) two temporal relation types based on neighboring and distant frames. Through this partition-specific attention strategy, our SkateFormer can selectively focus on key joints and frames crucial for action recognition in an action-adaptive manner with efficient computation. Extensive experiments on various benchmark datasets validate that our SkateFormer outperforms recent state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>149, <a href='https://arxiv.org/pdf/2401.18054.pdf' target='_blank'>https://arxiv.org/pdf/2401.18054.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Wei, Tom De Schepper, Kevin Mets
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.18054">Benchmarking Sensitivity of Continual Graph Learning for Skeleton-Based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Continual learning (CL) is the research field that aims to build machine learning models that can accumulate knowledge continuously over different tasks without retraining from scratch. Previous studies have shown that pre-training graph neural networks (GNN) may lead to negative transfer (Hu et al., 2020) after fine-tuning, a setting which is closely related to CL. Thus, we focus on studying GNN in the continual graph learning (CGL) setting. We propose the first continual graph learning benchmark for spatio-temporal graphs and use it to benchmark well-known CGL methods in this novel setting. The benchmark is based on the N-UCLA and NTU-RGB+D datasets for skeleton-based action recognition. Beyond benchmarking for standard performance metrics, we study the class and task-order sensitivity of CGL methods, i.e., the impact of learning order on each class/task's performance, and the architectural sensitivity of CGL methods with backbone GNN at various widths and depths. We reveal that task-order robust methods can still be class-order sensitive and observe results that contradict previous empirical observations on architectural sensitivity in CL.
<div id='section'>Paperid: <span id='pid'>150, <a href='https://arxiv.org/pdf/2309.10001.pdf' target='_blank'>https://arxiv.org/pdf/2309.10001.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junan Lin, Zhichao Sun, Enjie Cao, Taein Kwon, Mahdi Rad, Marc Pollefeys
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.10001">CaSAR: Contact-aware Skeletal Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeletal Action recognition from an egocentric view is important for applications such as interfaces in AR/VR glasses and human-robot interaction, where the device has limited resources. Most of the existing skeletal action recognition approaches use 3D coordinates of hand joints and 8-corner rectangular bounding boxes of objects as inputs, but they do not capture how the hands and objects interact with each other within the spatial context. In this paper, we present a new framework called Contact-aware Skeletal Action Recognition (CaSAR). It uses novel representations of hand-object interaction that encompass spatial information: 1) contact points where the hand joints meet the objects, 2) distant points where the hand joints are far away from the object and nearly not involved in the current action. Our framework is able to learn how the hands touch or stay away from the objects for each frame of the action sequence, and use this information to predict the action class. We demonstrate that our approach achieves the state-of-the-art accuracy of 91.3% and 98.4% on two public datasets, H2O and FPHA, respectively.
<div id='section'>Paperid: <span id='pid'>151, <a href='https://arxiv.org/pdf/2306.05844.pdf' target='_blank'>https://arxiv.org/pdf/2306.05844.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dustin Aganian, Mona KÃ¶hler, Sebastian Baake, Markus Eisenbach, Horst-Michael Gross
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.05844">How Object Information Improves Skeleton-based Human Action Recognition in Assembly Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As the use of collaborative robots (cobots) in industrial manufacturing continues to grow, human action recognition for effective human-robot collaboration becomes increasingly important. This ability is crucial for cobots to act autonomously and assist in assembly tasks. Recently, skeleton-based approaches are often used as they tend to generalize better to different people and environments. However, when processing skeletons alone, information about the objects a human interacts with is lost. Therefore, we present a novel approach of integrating object information into skeleton-based action recognition. We enhance two state-of-the-art methods by treating object centers as further skeleton joints. Our experiments on the assembly dataset IKEA ASM show that our approach improves the performance of these state-of-the-art methods to a large extent when combining skeleton joints with objects predicted by a state-of-the-art instance segmentation model. Our research sheds light on the benefits of combining skeleton joints with object information for human action recognition in assembly tasks. We analyze the effect of the object detector on the combination for action classification and discuss the important factors that must be taken into account.
<div id='section'>Paperid: <span id='pid'>152, <a href='https://arxiv.org/pdf/2501.11007.pdf' target='_blank'>https://arxiv.org/pdf/2501.11007.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengcheng Dong, Wenbo Wan, Huaxiang Zhang, Shuai Li, Sujuan Hou, Jiande Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.11007">HFGCN:Hypergraph Fusion Graph Convolutional Networks for Skeleton-Based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, action recognition has received much attention and wide application due to its important role in video understanding. Most of the researches on action recognition methods focused on improving the performance via various deep learning methods rather than the classification of skeleton points. The topological modeling between skeleton points and body parts was seldom considered. Although some studies have used a data-driven approach to classify the topology of the skeleton point, the nature of the skeleton point in terms of kinematics has not been taken into consideration. Therefore, in this paper, we draw on the theory of kinematics to adapt the topological relations of the skeleton point and propose a topological relation classification based on body parts and distance from core of body. To synthesize these topological relations for action recognition, we propose a novel Hypergraph Fusion Graph Convolutional Network (HFGCN). In particular, the proposed model is able to focus on the human skeleton points and the different body parts simultaneously, and thus construct the topology, which improves the recognition accuracy obviously. We use a hypergraph to represent the categorical relationships of these skeleton points and incorporate the hypergraph into a graph convolution network to model the higher-order relationships among the skeleton points and enhance the feature representation of the network. In addition, our proposed hypergraph attention module and hypergraph graph convolution module optimize topology modeling in temporal and channel dimensions, respectively, to further enhance the feature representation of the network. We conducted extensive experiments on three widely used datasets.The results validate that our proposed method can achieve the best performance when compared with the state-of-the-art skeleton-based methods.
<div id='section'>Paperid: <span id='pid'>153, <a href='https://arxiv.org/pdf/2411.12560.pdf' target='_blank'>https://arxiv.org/pdf/2411.12560.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyu Liang, Hailun Xia, Naichuan Zheng, Huan Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.12560">Topological Symmetry Enhanced Graph Convolution for Skeleton-Based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeleton-based action recognition has achieved remarkable performance with the development of graph convolutional networks (GCNs). However, most of these methods tend to construct complex topology learning mechanisms while neglecting the inherent symmetry of the human body. Additionally, the use of temporal convolutions with certain fixed receptive fields limits their capacity to effectively capture dependencies in time sequences. To address the issues, we (1) propose a novel Topological Symmetry Enhanced Graph Convolution (TSE-GC) to enable distinct topology learning across different channel partitions while incorporating topological symmetry awareness and (2) construct a Multi-Branch Deformable Temporal Convolution (MBDTC) for skeleton-based action recognition. The proposed TSE-GC emphasizes the inherent symmetry of the human body while enabling efficient learning of dynamic topologies. Meanwhile, the design of MBDTC introduces the concept of deformable modeling, leading to more flexible receptive fields and stronger modeling capacity of temporal dependencies. Combining TSE-GC with MBDTC, our final model, TSE-GCN, achieves competitive performance with fewer parameters compared with state-of-the-art methods on three large datasets, NTU RGB+D, NTU RGB+D 120, and NW-UCLA. On the cross-subject and cross-set evaluations of NTU RGB+D 120, the accuracies of our model reach 90.0\% and 91.1\%, with 1.1M parameters and 1.38 GFLOPS for one stream.
<div id='section'>Paperid: <span id='pid'>154, <a href='https://arxiv.org/pdf/2410.23641.pdf' target='_blank'>https://arxiv.org/pdf/2410.23641.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanchao Liu, Yujiang Li, Tai-Jiang Mu, Shi-Min Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.23641">Recovering Complete Actions for Cross-dataset Skeleton Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite huge progress in skeleton-based action recognition, its generalizability to different domains remains a challenging issue. In this paper, to solve the skeleton action generalization problem, we present a recover-and-resample augmentation framework based on a novel complete action prior. We observe that human daily actions are confronted with temporal mismatch across different datasets, as they are usually partial observations of their complete action sequences. By recovering complete actions and resampling from these full sequences, we can generate strong augmentations for unseen domains. At the same time, we discover the nature of general action completeness within large datasets, indicated by the per-frame diversity over time. This allows us to exploit two assets of transferable knowledge that can be shared across action samples and be helpful for action completion: boundary poses for determining the action start, and linear temporal transforms for capturing global action patterns. Therefore, we formulate the recovering stage as a two-step stochastic action completion with boundary pose-conditioned extrapolation followed by smooth linear transforms. Both the boundary poses and linear transforms can be efficiently learned from the whole dataset via clustering. We validate our approach on a cross-dataset setting with three skeleton action datasets, outperforming other domain generalization approaches by a considerable margin.
<div id='section'>Paperid: <span id='pid'>155, <a href='https://arxiv.org/pdf/2408.01701.pdf' target='_blank'>https://arxiv.org/pdf/2408.01701.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Naichuan Zheng, Yuchen Du, Hailun Xia, Zeyu Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.01701">Signal-SGN: A Spiking Graph Convolutional Network for Skeletal Action Recognition via Learning Temporal-Frequency Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For multimodal skeleton-based action recognition, Graph Convolutional Networks (GCNs) are effective models. Still, their reliance on floating-point computations leads to high energy consumption, limiting their applicability in battery-powered devices. While energy-efficient, Spiking Neural Networks (SNNs) struggle to model skeleton dynamics, leading to suboptimal solutions. We propose Signal-SGN (Spiking Graph Convolutional Network), which utilizes the temporal dimension of skeleton sequences as the spike time steps and represents features as multi-dimensional discrete stochastic signals for temporal-frequency domain feature extraction. It combines the 1D Spiking Graph Convolution (1D-SGC) module and the Frequency Spiking Convolution (FSC) module to extract features from the skeleton represented as spiking form. Additionally, the Multi-Scale Wavelet Transform Feature Fusion (MWTF) module is proposed to extract dynamic spiking features and capture frequency-specific characteristics, enhancing classification performance. Experiments across three large-scale datasets reveal Signal-SGN exceeding state-of-the-art SNN-based methods in accuracy and computational efficiency while attaining comparable performance with GCN methods and significantly reducing theoretical energy consumption.
<div id='section'>Paperid: <span id='pid'>156, <a href='https://arxiv.org/pdf/2404.10210.pdf' target='_blank'>https://arxiv.org/pdf/2404.10210.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Naichuan Zheng, Hailun Xia, Zeyu Liang, Yuchen Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.10210">MK-SGN: A Spiking Graph Convolutional Network with Multimodal Fusion and Knowledge Distillation for Skeleton-based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, multimodal Graph Convolutional Networks (GCNs) have achieved remarkable performance in skeleton-based action recognition. The reliance on high-energy-consuming continuous floating-point operations inherent in GCN-based methods poses significant challenges for deployment in energy-constrained, battery-powered edge devices. To address these limitations, MK-SGN, a Spiking Graph Convolutional Network with Multimodal Fusion and Knowledge Distillation, is proposed to leverage the energy efficiency of Spiking Neural Networks (SNNs) for skeleton-based action recognition for the first time. By integrating the energy-saving properties of SNNs with the graph representation capabilities of GCNs, MK-SGN achieves significant reductions in energy consumption while maintaining competitive recognition accuracy. Firstly, we formulate a Spiking Multimodal Fusion (SMF) module to effectively fuse multimodal skeleton data represented as spike-form features. Secondly, we propose the Self-Attention Spiking Graph Convolution (SA-SGC) module and the Spiking Temporal Convolution (STC) module, to capture spatial relationships and temporal dynamics of spike-form features. Finally, we propose an integrated knowledge distillation strategy to transfer information from the multimodal GCN to the SGN, incorporating both intermediate-layer distillation and soft-label distillation to enhance the performance of the SGN. MK-SGN exhibits substantial advantages, surpassing state-of-the-art GCN frameworks in energy efficiency and outperforming state-of-the-art SNN frameworks in recognition accuracy. The proposed method achieves a remarkable reduction in energy consumption, exceeding 98\% compared to conventional GCN-based approaches. This research establishes a robust baseline for developing high-performance, energy-efficient SNN-based models for skeleton-based action recognition
<div id='section'>Paperid: <span id='pid'>157, <a href='https://arxiv.org/pdf/2403.10082.pdf' target='_blank'>https://arxiv.org/pdf/2403.10082.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tingbing Yan, Wenzheng Zeng, Yang Xiao, Xingyu Tong, Bo Tan, Zhiwen Fang, Zhiguo Cao, Joey Tianyi Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.10082">CrossGLG: LLM Guides One-shot Skeleton-based 3D Action Recognition in a Cross-level Manner</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most existing one-shot skeleton-based action recognition focuses on raw low-level information (e.g., joint location), and may suffer from local information loss and low generalization ability. To alleviate these, we propose to leverage text description generated from large language models (LLM) that contain high-level human knowledge, to guide feature learning, in a global-local-global way. Particularly, during training, we design $2$ prompts to gain global and local text descriptions of each action from an LLM. We first utilize the global text description to guide the skeleton encoder focus on informative joints (i.e.,global-to-local). Then we build non-local interaction between local text and joint features, to form the final global representation (i.e., local-to-global). To mitigate the asymmetry issue between the training and inference phases, we further design a dual-branch architecture that allows the model to perform novel class inference without any text input, also making the additional inference cost neglectable compared with the base skeleton encoder. Extensive experiments on three different benchmarks show that CrossGLG consistently outperforms the existing SOTA methods with large margins, and the inference cost (model size) is only $2.8$\% than the previous SOTA. CrossGLG can also serve as a plug-and-play module that can substantially enhance the performance of different SOTA skeleton encoders with a neglectable cost during inference. The source code will be released soon.
<div id='section'>Paperid: <span id='pid'>158, <a href='https://arxiv.org/pdf/2401.14034.pdf' target='_blank'>https://arxiv.org/pdf/2401.14034.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuankun Li, Shuai Li, Yanbo Gao, Ping Chen, Jian Li, Wanqing Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.14034">Unsupervised Spatial-Temporal Feature Enrichment and Fidelity Preservation Network for Skeleton based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsupervised skeleton based action recognition has achieved remarkable progress recently. Existing unsupervised learning methods suffer from severe overfitting problem, and thus small networks are used, significantly reducing the representation capability. To address this problem, the overfitting mechanism behind the unsupervised learning for skeleton based action recognition is first investigated. It is observed that the skeleton is already a relatively high-level and low-dimension feature, but not in the same manifold as the features for action recognition. Simply applying the existing unsupervised learning method may tend to produce features that discriminate the different samples instead of action classes, resulting in the overfitting problem. To solve this problem, this paper presents an Unsupervised spatial-temporal Feature Enrichment and Fidelity Preservation framework (U-FEFP) to generate rich distributed features that contain all the information of the skeleton sequence. A spatial-temporal feature transformation subnetwork is developed using spatial-temporal graph convolutional network and graph convolutional gate recurrent unit network as the basic feature extraction network. The unsupervised Bootstrap Your Own Latent based learning is used to generate rich distributed features and the unsupervised pretext task based learning is used to preserve the information of the skeleton sequence. The two unsupervised learning ways are collaborated as U-FEFP to produce robust and discriminative representations. Experimental results on three widely used benchmarks, namely NTU-RGB+D-60, NTU-RGB+D-120 and PKU-MMD dataset, demonstrate that the proposed U-FEFP achieves the best performance compared with the state-of-the-art unsupervised learning methods. t-SNE illustrations further validate that U-FEFP can learn more discriminative features for unsupervised skeleton based action recognition.
<div id='section'>Paperid: <span id='pid'>159, <a href='https://arxiv.org/pdf/2309.11445.pdf' target='_blank'>https://arxiv.org/pdf/2309.11445.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haodong Duan, Mingze Xu, Bing Shuai, Davide Modolo, Zhuowen Tu, Joseph Tighe, Alessandro Bergamo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.11445">SkeleTR: Towrads Skeleton-based Action Recognition in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present SkeleTR, a new framework for skeleton-based action recognition. In contrast to prior work, which focuses mainly on controlled environments, we target more general scenarios that typically involve a variable number of people and various forms of interaction between people. SkeleTR works with a two-stage paradigm. It first models the intra-person skeleton dynamics for each skeleton sequence with graph convolutions, and then uses stacked Transformer encoders to capture person interactions that are important for action recognition in general scenarios. To mitigate the negative impact of inaccurate skeleton associations, SkeleTR takes relative short skeleton sequences as input and increases the number of sequences. As a unified solution, SkeleTR can be directly applied to multiple skeleton-based action tasks, including video-level action classification, instance-level action detection, and group-level activity recognition. It also enables transfer learning and joint training across different action tasks and datasets, which result in performance improvement. When evaluated on various skeleton-based action recognition benchmarks, SkeleTR achieves the state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>160, <a href='https://arxiv.org/pdf/2302.02316.pdf' target='_blank'>https://arxiv.org/pdf/2302.02316.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Binqian Xu, Xiangbo Shu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.02316">Spatiotemporal Decouple-and-Squeeze Contrastive Learning for Semi-Supervised Skeleton-based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Contrastive learning has been successfully leveraged to learn action representations for addressing the problem of semi-supervised skeleton-based action recognition. However, most contrastive learning-based methods only contrast global features mixing spatiotemporal information, which confuses the spatial- and temporal-specific information reflecting different semantic at the frame level and joint level. Thus, we propose a novel Spatiotemporal Decouple-and-Squeeze Contrastive Learning (SDS-CL) framework to comprehensively learn more abundant representations of skeleton-based actions by jointly contrasting spatial-squeezing features, temporal-squeezing features, and global features. In SDS-CL, we design a new Spatiotemporal-decoupling Intra-Inter Attention (SIIA) mechanism to obtain the spatiotemporal-decoupling attentive features for capturing spatiotemporal specific information by calculating spatial- and temporal-decoupling intra-attention maps among joint/motion features, as well as spatial- and temporal-decoupling inter-attention maps between joint and motion features. Moreover, we present a new Spatial-squeezing Temporal-contrasting Loss (STL), a new Temporal-squeezing Spatial-contrasting Loss (TSL), and the Global-contrasting Loss (GL) to contrast the spatial-squeezing joint and motion features at the frame level, temporal-squeezing joint and motion features at the joint level, as well as global joint and motion features at the skeleton level. Extensive experimental results on four public datasets show that the proposed SDS-CL achieves performance gains compared with other competitive methods.
<div id='section'>Paperid: <span id='pid'>161, <a href='https://arxiv.org/pdf/2501.12318.pdf' target='_blank'>https://arxiv.org/pdf/2501.12318.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>TamÃ¡s KarÃ¡csony, JoÃ£o Carmona, JoÃ£o Paulo Silva Cunha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.12318">BlanketGen2-Fit3D: Synthetic Blanket Augmentation Towards Improving Real-World In-Bed Blanket Occluded Human Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human Pose Estimation (HPE) from monocular RGB images is crucial for clinical in-bed skeleton-based action recognition, however, it poses unique challenges for HPE models due to the frequent presence of blankets occluding the person, while labeled HPE data in this scenario is scarce. To address this we introduce BlanketGen2-Fit3D (BG2-Fit3D), an augmentation of Fit3D dataset that contains 1,217,312 frames with synthetic photo-realistic blankets. To generate it we used BlanketGen2, our new and improved version of our BlanketGen pipeline that simulates synthetic blankets using ground-truth Skinned Multi-Person Linear model (SMPL) meshes and then renders them as transparent images that can be layered on top of the original frames. This dataset was used in combination with the original Fit3D to finetune the ViTPose-B HPE model, to evaluate synthetic blanket augmentation effectiveness. The trained models were further evaluated on a real-world blanket occluded in-bed HPE dataset (SLP dataset). Comparing architectures trained on only Fit3D with the ones trained with our synthetic blanket augmentation the later improved pose estimation performance on BG2-Fit3D, the synthetic blanket occluded dataset significantly to (0.977 Percentage of Correct Keypoints (PCK), 0.149 Normalized Mean Error (NME)) with an absolute 4.4% PCK increase. Furthermore, the test results on SLP demonstrated the utility of synthetic data augmentation by improving performance by an absolute 2.3% PCK, on real-world images with the poses occluded by real blankets. These results show synthetic blanket augmentation has the potential to improve in-bed blanket occluded HPE from RGB images. The dataset as well as the code will be made available to the public.
<div id='section'>Paperid: <span id='pid'>162, <a href='https://arxiv.org/pdf/2501.05066.pdf' target='_blank'>https://arxiv.org/pdf/2501.05066.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Wen, Ziqian Lu, Fengli Shen, Zhe-Ming Lu, Jialin Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.05066">Improving Skeleton-based Action Recognition with Interactive Object Information</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human skeleton information is important in skeleton-based action recognition, which provides a simple and efficient way to describe human pose. However, existing skeleton-based methods focus more on the skeleton, ignoring the objects interacting with humans, resulting in poor performance in recognizing actions that involve object interactions. We propose a new action recognition framework introducing object nodes to supplement absent interactive object information. We also propose Spatial Temporal Variable Graph Convolutional Networks (ST-VGCN) to effectively model the Variable Graph (VG) containing object nodes. Specifically, in order to validate the role of interactive object information, by leveraging a simple self-training approach, we establish a new dataset, JXGC 24, and an extended dataset, NTU RGB+D+Object 60, including more than 2 million additional object nodes. At the same time, we designe the Variable Graph construction method to accommodate a variable number of nodes for graph structure. Additionally, we are the first to explore the overfitting issue introduced by incorporating additional object information, and we propose a VG-based data augmentation method to address this issue, called Random Node Attack. Finally, regarding the network structure, we introduce two fusion modules, CAF and WNPool, along with a novel Node Balance Loss, to enhance the comprehensive performance by effectively fusing and balancing skeleton and object node information. Our method surpasses the previous state-of-the-art on multiple skeleton-based action recognition benchmarks. The accuracy of our method on NTU RGB+D 60 cross-subject split is 96.7\%, and on cross-view split, it is 99.2\%.
<div id='section'>Paperid: <span id='pid'>163, <a href='https://arxiv.org/pdf/2404.18206.pdf' target='_blank'>https://arxiv.org/pdf/2404.18206.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cuiwei Liu, Youzhi Jiang, Chong Du, Zhaokui Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.18206">Enhancing Action Recognition from Low-Quality Skeleton Data via Part-Level Knowledge Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeleton-based action recognition is vital for comprehending human-centric videos and has applications in diverse domains. One of the challenges of skeleton-based action recognition is dealing with low-quality data, such as skeletons that have missing or inaccurate joints. This paper addresses the issue of enhancing action recognition using low-quality skeletons through a general knowledge distillation framework. The proposed framework employs a teacher-student model setup, where a teacher model trained on high-quality skeletons guides the learning of a student model that handles low-quality skeletons. To bridge the gap between heterogeneous high-quality and lowquality skeletons, we present a novel part-based skeleton matching strategy, which exploits shared body parts to facilitate local action pattern learning. An action-specific part matrix is developed to emphasize critical parts for different actions, enabling the student model to distill discriminative part-level knowledge. A novel part-level multi-sample contrastive loss achieves knowledge transfer from multiple high-quality skeletons to low-quality ones, which enables the proposed knowledge distillation framework to include training low-quality skeletons that lack corresponding high-quality matches. Comprehensive experiments conducted on the NTU-RGB+D, Penn Action, and SYSU 3D HOI datasets demonstrate the effectiveness of the proposed knowledge distillation framework.
<div id='section'>Paperid: <span id='pid'>164, <a href='https://arxiv.org/pdf/2311.12300.pdf' target='_blank'>https://arxiv.org/pdf/2311.12300.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Elaheh Hatamimajoumerd, Pooria Daneshvar Kakhaki, Xiaofei Huang, Lingfei Luan, Somaieh Amraee, Sarah Ostadabbas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.12300">Challenges in Video-Based Infant Action Recognition: A Critical Examination of the State of the Art</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automated human action recognition, a burgeoning field within computer vision, boasts diverse applications spanning surveillance, security, human-computer interaction, tele-health, and sports analysis. Precise action recognition in infants serves a multitude of pivotal purposes, encompassing safety monitoring, developmental milestone tracking, early intervention for developmental delays, fostering parent-infant bonds, advancing computer-aided diagnostics, and contributing to the scientific comprehension of child development. This paper delves into the intricacies of infant action recognition, a domain that has remained relatively uncharted despite the accomplishments in adult action recognition. In this study, we introduce a groundbreaking dataset called ``InfActPrimitive'', encompassing five significant infant milestone action categories, and we incorporate specialized preprocessing for infant data. We conducted an extensive comparative analysis employing cutting-edge skeleton-based action recognition models using this dataset. Our findings reveal that, although the PoseC3D model achieves the highest accuracy at approximately 71%, the remaining models struggle to accurately capture the dynamics of infant actions. This highlights a substantial knowledge gap between infant and adult action recognition domains and the urgent need for data-efficient pipeline models.
<div id='section'>Paperid: <span id='pid'>165, <a href='https://arxiv.org/pdf/2306.11046.pdf' target='_blank'>https://arxiv.org/pdf/2306.11046.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingwen Guo, Hong Liu, Shitong Sun, Tianyu Guo, Min Zhang, Chenyang Si
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.11046">FSAR: Federated Skeleton-based Action Recognition with Adaptive Topology Structure and Knowledge Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing skeleton-based action recognition methods typically follow a centralized learning paradigm, which can pose privacy concerns when exposing human-related videos. Federated Learning (FL) has attracted much attention due to its outstanding advantages in privacy-preserving. However, directly applying FL approaches to skeleton videos suffers from unstable training. In this paper, we investigate and discover that the heterogeneous human topology graph structure is the crucial factor hindering training stability. To address this limitation, we pioneer a novel Federated Skeleton-based Action Recognition (FSAR) paradigm, which enables the construction of a globally generalized model without accessing local sensitive data. Specifically, we introduce an Adaptive Topology Structure (ATS), separating generalization and personalization by learning a domain-invariant topology shared across clients and a domain-specific topology decoupled from global model aggregation.Furthermore, we explore Multi-grain Knowledge Distillation (MKD) to mitigate the discrepancy between clients and server caused by distinct updating patterns through aligning shallow block-wise motion features. Extensive experiments on multiple datasets demonstrate that FSAR outperforms state-of-the-art FL-based methods while inherently protecting privacy.
<div id='section'>Paperid: <span id='pid'>166, <a href='https://arxiv.org/pdf/2305.18710.pdf' target='_blank'>https://arxiv.org/pdf/2305.18710.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junyi Wang, Ziao Li, Bangli Liu, Haibin Cai, Mohamad Saada, Qinggang Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.18710">High-Performance Inference Graph Convolutional Networks for Skeleton-Based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, the significant achievements have been made in skeleton-based human action recognition with the emergence of graph convolutional networks (GCNs). However, the state-of-the-art (SOTA) models used for this task focus on constructing more complex higher-order connections between joint nodes to describe skeleton information, which leads to complex inference processes and high computational costs. To address the slow inference speed caused by overly complex model structures, we introduce re-parameterization and over-parameterization techniques to GCNs and propose two novel high-performance inference GCNs, namely HPI-GCN-RP and HPI-GCN-OP. After the completion of model training, model parameters are fixed. HPI-GCN-RP adopts re-parameterization technique to transform high-performance training model into fast inference model through linear transformations, which achieves a higher inference speed with competitive model performance. HPI-GCN-OP further utilizes over-parameterization technique to achieve higher performance improvement by introducing additional inference parameters, albeit with slightly decreased inference speed. The experimental results on the two skeleton-based action recognition datasets demonstrate the effectiveness of our approach. Our HPI-GCN-OP achieves performance comparable to the current SOTA models, with inference speeds five times faster. Specifically, our HPI-GCN-OP achieves an accuracy of 93\% on the cross-subject split of the NTU-RGB+D 60 dataset, and 90.1\% on the cross-subject benchmark of the NTU-RGB+D 120 dataset. Code is available at github.com/lizaowo/HPI-GCN.
<div id='section'>Paperid: <span id='pid'>167, <a href='https://arxiv.org/pdf/2301.11495.pdf' target='_blank'>https://arxiv.org/pdf/2301.11495.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Pang, Xuequan Lu, Lei Lyu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.11495">Skeleton-based Action Recognition through Contrasting Two-Stream Spatial-Temporal Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For pursuing accurate skeleton-based action recognition, most prior methods use the strategy of combining Graph Convolution Networks (GCNs) with attention-based methods in a serial way. However, they regard the human skeleton as a complete graph, resulting in less variations between different actions (e.g., the connection between the elbow and head in action ``clapping hands''). For this, we propose a novel Contrastive GCN-Transformer Network (ConGT) which fuses the spatial and temporal modules in a parallel way. The ConGT involves two parallel streams: Spatial-Temporal Graph Convolution stream (STG) and Spatial-Temporal Transformer stream (STT). The STG is designed to obtain action representations maintaining the natural topology structure of the human skeleton. The STT is devised to acquire action representations containing the global relationships among joints. Since the action representations produced from these two streams contain different characteristics, and each of them knows little information of the other, we introduce the contrastive learning paradigm to guide their output representations of the same sample to be as close as possible in a self-supervised manner. Through the contrastive learning, they can learn information from each other to enrich the action features by maximizing the mutual information between the two types of action representations. To further improve action recognition accuracy, we introduce the Cyclical Focal Loss (CFL) which can focus on confident training samples in early training epochs, with an increasing focus on hard samples during the middle epochs. We conduct experiments on three benchmark datasets, which demonstrate that our model achieves state-of-the-art performance in action recognition.
<div id='section'>Paperid: <span id='pid'>168, <a href='https://arxiv.org/pdf/2111.11051.pdf' target='_blank'>https://arxiv.org/pdf/2111.11051.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Wang, Jun Wen, Chenyang Si, Yuntao Qian, Liang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2111.11051">Contrast-reconstruction Representation Learning for Self-supervised Skeleton-based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeleton-based action recognition is widely used in varied areas, e.g., surveillance and human-machine interaction. Existing models are mainly learned in a supervised manner, thus heavily depending on large-scale labeled data which could be infeasible when labels are prohibitively expensive. In this paper, we propose a novel Contrast-Reconstruction Representation Learning network (CRRL) that simultaneously captures postures and motion dynamics for unsupervised skeleton-based action recognition. It mainly consists of three parts: Sequence Reconstructor, Contrastive Motion Learner, and Information Fuser. The Sequence Reconstructor learns representation from skeleton coordinate sequence via reconstruction, thus the learned representation tends to focus on trivial postural coordinates and be hesitant in motion learning. To enhance the learning of motions, the Contrastive Motion Learner performs contrastive learning between the representations learned from coordinate sequence and additional velocity sequence, respectively. Finally, in the Information Fuser, we explore varied strategies to combine the Sequence Reconstructor and Contrastive Motion Learner, and propose to capture postures and motions simultaneously via a knowledge-distillation based fusion strategy that transfers the motion learning from the Contrastive Motion Learner to the Sequence Reconstructor. Experimental results on several benchmarks, i.e., NTU RGB+D 60, NTU RGB+D 120, CMU mocap, and NW-UCLA, demonstrate the promise of the proposed CRRL method by far outperforming state-of-the-art approaches.
<div id='section'>Paperid: <span id='pid'>169, <a href='https://arxiv.org/pdf/1712.03084.pdf' target='_blank'>https://arxiv.org/pdf/1712.03084.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dimitrios S. Alexiadis, Anargyros Chatzitofis, Nikolaos Zioulis, Olga Zoidi, Georgios Louizis, Dimitrios Zarpalas, Petros Daras
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1712.03084">An Integrated Platform for Live 3D Human Reconstruction and Motion Capturing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The latest developments in 3D capturing, processing, and rendering provide means to unlock novel 3D application pathways. The main elements of an integrated platform, which target tele-immersion and future 3D applications, are described in this paper, addressing the tasks of real-time capturing, robust 3D human shape/appearance reconstruction, and skeleton-based motion tracking. More specifically, initially, the details of a multiple RGB-depth (RGB-D) capturing system are given, along with a novel sensors' calibration method. A robust, fast reconstruction method from multiple RGB-D streams is then proposed, based on an enhanced variation of the volumetric Fourier transform-based method, parallelized on the Graphics Processing Unit, and accompanied with an appropriate texture-mapping algorithm. On top of that, given the lack of relevant objective evaluation methods, a novel framework is proposed for the quantitative evaluation of real-time 3D reconstruction systems. Finally, a generic, multiple depth stream-based method for accurate real-time human skeleton tracking is proposed. Detailed experimental results with multi-Kinect2 data sets verify the validity of our arguments and the effectiveness of the proposed system and methodologies.
<div id='section'>Paperid: <span id='pid'>170, <a href='https://arxiv.org/pdf/2509.07994.pdf' target='_blank'>https://arxiv.org/pdf/2509.07994.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Robinson, Animesh Gupta, Rizwan Quershi, Qiushi Fu, Mubarak Shah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07994">STROKEVISION-BENCH: A Multimodal Video And 2D Pose Benchmark For Tracking Stroke Recovery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite advancements in rehabilitation protocols, clinical assessment of upper extremity (UE) function after stroke largely remains subjective, relying heavily on therapist observation and coarse scoring systems. This subjectivity limits the sensitivity of assessments to detect subtle motor improvements, which are critical for personalized rehabilitation planning. Recent progress in computer vision offers promising avenues for enabling objective, quantitative, and scalable assessment of UE motor function. Among standardized tests, the Box and Block Test (BBT) is widely utilized for measuring gross manual dexterity and tracking stroke recovery, providing a structured setting that lends itself well to computational analysis. However, existing datasets targeting stroke rehabilitation primarily focus on daily living activities and often fail to capture clinically structured assessments such as block transfer tasks. Furthermore, many available datasets include a mixture of healthy and stroke-affected individuals, limiting their specificity and clinical utility. To address these critical gaps, we introduce StrokeVision-Bench, the first-ever dedicated dataset of stroke patients performing clinically structured block transfer tasks. StrokeVision-Bench comprises 1,000 annotated videos categorized into four clinically meaningful action classes, with each sample represented in two modalities: raw video frames and 2D skeletal keypoints. We benchmark several state-of-the-art video action recognition and skeleton-based action classification methods to establish performance baselines for this domain and facilitate future research in automated stroke rehabilitation assessment.
<div id='section'>Paperid: <span id='pid'>171, <a href='https://arxiv.org/pdf/2508.19647.pdf' target='_blank'>https://arxiv.org/pdf/2508.19647.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bikash Kumar Badatya, Vipul Baghel, Ravi Hegde
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19647">UTAL-GNN: Unsupervised Temporal Action Localization using Graph Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fine-grained action localization in untrimmed sports videos presents a significant challenge due to rapid and subtle motion transitions over short durations. Existing supervised and weakly supervised solutions often rely on extensive annotated datasets and high-capacity models, making them computationally intensive and less adaptable to real-world scenarios. In this work, we introduce a lightweight and unsupervised skeleton-based action localization pipeline that leverages spatio-temporal graph neural representations. Our approach pre-trains an Attention-based Spatio-Temporal Graph Convolutional Network (ASTGCN) on a pose-sequence denoising task with blockwise partitions, enabling it to learn intrinsic motion dynamics without any manual labeling. At inference, we define a novel Action Dynamics Metric (ADM), computed directly from low-dimensional ASTGCN embeddings, which detects motion boundaries by identifying inflection points in its curvature profile. Our method achieves a mean Average Precision (mAP) of 82.66% and average localization latency of 29.09 ms on the DSV Diving dataset, matching state-of-the-art supervised performance while maintaining computational efficiency. Furthermore, it generalizes robustly to unseen, in-the-wild diving footage without retraining, demonstrating its practical applicability for lightweight, real-time action analysis systems in embedded or dynamic environments.
<div id='section'>Paperid: <span id='pid'>172, <a href='https://arxiv.org/pdf/2505.23012.pdf' target='_blank'>https://arxiv.org/pdf/2505.23012.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shanaka Ramesh Gunasekara, Wanqing Li, Philip Ogunbona, Jack Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23012">Spatio-Temporal Joint Density Driven Learning for Skeleton-Based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional approaches in unsupervised or self supervised learning for skeleton-based action classification have concentrated predominantly on the dynamic aspects of skeletal sequences. Yet, the intricate interaction between the moving and static elements of the skeleton presents a rarely tapped discriminative potential for action classification. This paper introduces a novel measurement, referred to as spatial-temporal joint density (STJD), to quantify such interaction. Tracking the evolution of this density throughout an action can effectively identify a subset of discriminative moving and/or static joints termed "prime joints" to steer self-supervised learning. A new contrastive learning strategy named STJD-CL is proposed to align the representation of a skeleton sequence with that of its prime joints while simultaneously contrasting the representations of prime and nonprime joints. In addition, a method called STJD-MP is developed by integrating it with a reconstruction-based framework for more effective learning. Experimental evaluations on the NTU RGB+D 60, NTU RGB+D 120, and PKUMMD datasets in various downstream tasks demonstrate that the proposed STJD-CL and STJD-MP improved performance, particularly by 3.5 and 3.6 percentage points over the state-of-the-art contrastive methods on the NTU RGB+D 120 dataset using X-sub and X-set evaluations, respectively.
<div id='section'>Paperid: <span id='pid'>173, <a href='https://arxiv.org/pdf/2503.09537.pdf' target='_blank'>https://arxiv.org/pdf/2503.09537.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuokang Huang, Julie A. McCann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09537">GenHPE: Generative Counterfactuals for 3D Human Pose Estimation with Radio Frequency Signals</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human pose estimation (HPE) detects the positions of human body joints for various applications. Compared to using cameras, HPE using radio frequency (RF) signals is non-intrusive and more robust to adverse conditions, exploiting the signal variations caused by human interference. However, existing studies focus on single-domain HPE confined by domain-specific confounders, which cannot generalize to new domains and result in diminished HPE performance. Specifically, the signal variations caused by different human body parts are entangled, containing subject-specific confounders. RF signals are also intertwined with environmental noise, involving environment-specific confounders. In this paper, we propose GenHPE, a 3D HPE approach that generates counterfactual RF signals to eliminate domain-specific confounders. GenHPE trains generative models conditioned on human skeleton labels, learning how human body parts and confounders interfere with RF signals. We manipulate skeleton labels (i.e., removing body parts) as counterfactual conditions for generative models to synthesize counterfactual RF signals. The differences between counterfactual signals approximately eliminate domain-specific confounders and regularize an encoder-decoder model to learn domain-independent representations. Such representations help GenHPE generalize to new subjects/environments for cross-domain 3D HPE. We evaluate GenHPE on three public datasets from WiFi, ultra-wideband, and millimeter wave. Experimental results show that GenHPE outperforms state-of-the-art methods and reduces estimation errors by up to 52.2mm for cross-subject HPE and 10.6mm for cross-environment HPE.
<div id='section'>Paperid: <span id='pid'>174, <a href='https://arxiv.org/pdf/2501.12086.pdf' target='_blank'>https://arxiv.org/pdf/2501.12086.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hu Cui, Renjing Huang, Ruoyu Zhang, Tessai Hayama
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.12086">DSTSA-GCN: Advancing Skeleton-Based Gesture Recognition with Semantic-Aware Spatio-Temporal Topology Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph convolutional networks (GCNs) have emerged as a powerful tool for skeleton-based action and gesture recognition, thanks to their ability to model spatial and temporal dependencies in skeleton data. However, existing GCN-based methods face critical limitations: (1) they lack effective spatio-temporal topology modeling that captures dynamic variations in skeletal motion, and (2) they struggle to model multiscale structural relationships beyond local joint connectivity. To address these issues, we propose a novel framework called Dynamic Spatial-Temporal Semantic Awareness Graph Convolutional Network (DSTSA-GCN). DSTSA-GCN introduces three key modules: Group Channel-wise Graph Convolution (GC-GC), Group Temporal-wise Graph Convolution (GT-GC), and Multi-Scale Temporal Convolution (MS-TCN). GC-GC and GT-GC operate in parallel to independently model channel-specific and frame-specific correlations, enabling robust topology learning that accounts for temporal variations. Additionally, both modules employ a grouping strategy to adaptively capture multiscale structural relationships. Complementing this, MS-TCN enhances temporal modeling through group-wise temporal convolutions with diverse receptive fields. Extensive experiments demonstrate that DSTSA-GCN significantly improves the topology modeling capabilities of GCNs, achieving state-of-the-art performance on benchmark datasets for gesture and action recognition, including SHREC17 Track, DHG-14\/28, NTU-RGB+D, and NTU-RGB+D-120.
<div id='section'>Paperid: <span id='pid'>175, <a href='https://arxiv.org/pdf/2412.14988.pdf' target='_blank'>https://arxiv.org/pdf/2412.14988.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haitao Tian, Pierre Payeur
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14988">Stitch Contrast and Segment_Learning a Human Action Segmentation Model Using Trimmed Skeleton Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing skeleton-based human action classification models rely on well-trimmed action-specific skeleton videos for both training and testing, precluding their scalability to real-world applications where untrimmed videos exhibiting concatenated actions are predominant. To overcome this limitation, recently introduced skeleton action segmentation models involve un-trimmed skeleton videos into end-to-end training. The model is optimized to provide frame-wise predictions for any length of testing videos, simultaneously realizing action localization and classification. Yet, achieving such an improvement im-poses frame-wise annotated skeleton videos, which remains time-consuming in practice. This paper features a novel framework for skeleton-based action segmentation trained on short trimmed skeleton videos, but that can run on longer un-trimmed videos. The approach is implemented in three steps: Stitch, Contrast, and Segment. First, Stitch proposes a tem-poral skeleton stitching scheme that treats trimmed skeleton videos as elementary human motions that compose a semantic space and can be sampled to generate multi-action stitched se-quences. Contrast learns contrastive representations from stitched sequences with a novel discrimination pretext task that enables a skeleton encoder to learn meaningful action-temporal contexts to improve action segmentation. Finally, Segment relates the proposed method to action segmentation by learning a segmentation layer while handling particular da-ta availability. Experiments involve a trimmed source dataset and an untrimmed target dataset in an adaptation formulation for real-world skeleton-based human action segmentation to evaluate the effectiveness of the proposed method.
<div id='section'>Paperid: <span id='pid'>176, <a href='https://arxiv.org/pdf/2412.14833.pdf' target='_blank'>https://arxiv.org/pdf/2412.14833.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Huang, Yujie Lin, Siyu Chen, Haiyang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14833">Synchronized and Fine-Grained Head for Skeleton-Based Ambiguous Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeleton-based action recognition using GCNs has achieved remarkable performance, but recognizing ambiguous actions, such as "waving" and "saluting", remains a significant challenge. Existing methods typically rely on a serial combination of GCNs and TCNs, where spatial and temporal features are extracted independently, leading to an unbalanced spatial-temporal information, which hinders accurate action recognition. Moreover, existing methods for ambiguous actions often overemphasize local details, resulting in the loss of crucial global context, which further complicates the task of differentiating ambiguous actions. To address these challenges, we propose a lightweight plug-and-play module called SF-Head, inserted between GCN and TCN layers. SF-Head first conducts SSTE with a Feature Redundancy Loss (F-RL), ensuring a balanced interaction. It then performs AC-FA, with a Feature Consistency Loss (F-CL), which aligns the aggregated feature with their original spatial-temporal feature. Experimental results on NTU RGB+D 60, NTU RGB+D 120, NW-UCLA and PKU-MMD I datasets demonstrate significant improvements in distinguishing ambiguous actions.
<div id='section'>Paperid: <span id='pid'>177, <a href='https://arxiv.org/pdf/2411.14656.pdf' target='_blank'>https://arxiv.org/pdf/2411.14656.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuting Hu, Peggy Ackun, Xiang Zhang, Siyang Cao, Jennifer Barton, Melvin G. Hector, Mindy J. Fain, Nima Toosizadeh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.14656">mmWave Radar for Sit-to-Stand Analysis: A Comparative Study with Wearables and Kinect</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study explores a novel approach for analyzing Sit-to-Stand (STS) movements using millimeter-wave (mmWave) radar technology. The goal is to develop a non-contact sensing, privacy-preserving, and all-day operational method for healthcare applications, including fall risk assessment. We used a 60GHz mmWave radar system to collect radar point cloud data, capturing STS motions from 45 participants. By employing a deep learning pose estimation model, we learned the human skeleton from Kinect built-in body tracking and applied Inverse Kinematics (IK) to calculate joint angles, segment STS motions, and extract commonly used features in fall risk assessment. Radar extracted features were then compared with those obtained from Kinect and wearable sensors. The results demonstrated the effectiveness of mmWave radar in capturing general motion patterns and large joint movements (e.g., trunk). Additionally, the study highlights the advantages and disadvantages of individual sensors and suggests the potential of integrated sensor technologies to improve the accuracy and reliability of motion analysis in clinical and biomedical research settings.
<div id='section'>Paperid: <span id='pid'>178, <a href='https://arxiv.org/pdf/2408.09356.pdf' target='_blank'>https://arxiv.org/pdf/2408.09356.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shanaka Ramesh Gunasekara, Wanqing Li, Jack Yang, Philip Ogunbona
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09356">Joint Temporal Pooling for Improving Skeleton-based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In skeleton-based human action recognition, temporal pooling is a critical step for capturing spatiotemporal relationship of joint dynamics. Conventional pooling methods overlook the preservation of motion information and treat each frame equally. However, in an action sequence, only a few segments of frames carry discriminative information related to the action. This paper presents a novel Joint Motion Adaptive Temporal Pooling (JMAP) method for improving skeleton-based action recognition. Two variants of JMAP, frame-wise pooling and joint-wise pooling, are introduced. The efficacy of JMAP has been validated through experiments on the popular NTU RGB+D 120 and PKU-MMD datasets.
<div id='section'>Paperid: <span id='pid'>179, <a href='https://arxiv.org/pdf/2407.14655.pdf' target='_blank'>https://arxiv.org/pdf/2407.14655.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soroush Oraki, Harry Zhuang, Jie Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.14655">LORTSAR: Low-Rank Transformer for Skeleton-based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The complexity of state-of-the-art Transformer-based models for skeleton-based action recognition poses significant challenges in terms of computational efficiency and resource utilization. In this paper, we explore the application of Singular Value Decomposition (SVD) to effectively reduce the model sizes of these pre-trained models, aiming to minimize their resource consumption while preserving accuracy. Our method, LORTSAR (LOw-Rank Transformer for Skeleton-based Action Recognition), also includes a fine-tuning step to compensate for any potential accuracy degradation caused by model compression, and is applied to two leading Transformer-based models, "Hyperformer" and "STEP-CATFormer". Experimental results on the "NTU RGB+D" and "NTU RGB+D 120" datasets show that our method can reduce the number of model parameters substantially with negligible degradation or even performance increase in recognition accuracy. This confirms that SVD combined with post-compression fine-tuning can boost model efficiency, paving the way for more sustainable, lightweight, and high-performance technologies in human action recognition.
<div id='section'>Paperid: <span id='pid'>180, <a href='https://arxiv.org/pdf/2404.17837.pdf' target='_blank'>https://arxiv.org/pdf/2404.17837.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Bao, Xu Zhao, Dahong Qian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.17837">Hybrid 3D Human Pose Estimation with Monocular Video and Sparse IMUs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Temporal 3D human pose estimation from monocular videos is a challenging task in human-centered computer vision due to the depth ambiguity of 2D-to-3D lifting. To improve accuracy and address occlusion issues, inertial sensor has been introduced to provide complementary source of information. However, it remains challenging to integrate heterogeneous sensor data for producing physically rational 3D human poses. In this paper, we propose a novel framework, Real-time Optimization and Fusion (RTOF), to address this issue. We first incorporate sparse inertial orientations into a parametric human skeleton to refine 3D poses in kinematics. The poses are then optimized by energy functions built on both visual and inertial observations to reduce the temporal jitters. Our framework outputs smooth and biomechanically plausible human motion. Comprehensive experiments with ablation studies demonstrate its rationality and efficiency. On Total Capture dataset, the pose estimation error is significantly decreased compared to the baseline method.
<div id='section'>Paperid: <span id='pid'>181, <a href='https://arxiv.org/pdf/2404.07645.pdf' target='_blank'>https://arxiv.org/pdf/2404.07645.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soumyabrata Chaudhuri, Saumik Bhattacharya
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.07645">Simba: Mamba augmented U-ShiftGCN for Skeletal Action Recognition in Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeleton Action Recognition (SAR) involves identifying human actions using skeletal joint coordinates and their interconnections. While plain Transformers have been attempted for this task, they still fall short compared to the current leading methods, which are rooted in Graph Convolutional Networks (GCNs) due to the absence of structural priors. Recently, a novel selective state space model, Mamba, has surfaced as a compelling alternative to the attention mechanism in Transformers, offering efficient modeling of long sequences. In this work, to the utmost extent of our awareness, we present the first SAR framework incorporating Mamba. Each fundamental block of our model adopts a novel U-ShiftGCN architecture with Mamba as its core component. The encoder segment of the U-ShiftGCN is devised to extract spatial features from the skeletal data using downsampling vanilla Shift S-GCN blocks. These spatial features then undergo intermediate temporal modeling facilitated by the Mamba block before progressing to the encoder section, which comprises vanilla upsampling Shift S-GCN blocks. Additionally, a Shift T-GCN (ShiftTCN) temporal modeling unit is employed before the exit of each fundamental block to refine temporal representations. This particular integration of downsampling spatial, intermediate temporal, upsampling spatial, and ultimate temporal subunits yields promising results for skeleton action recognition. We dub the resulting model \textbf{Simba}, which attains state-of-the-art performance across three well-known benchmark skeleton action recognition datasets: NTU RGB+D, NTU RGB+D 120, and Northwestern-UCLA. Interestingly, U-ShiftGCN (Simba without Intermediate Mamba Block) by itself is capable of performing reasonably well and surpasses our baseline.
<div id='section'>Paperid: <span id='pid'>182, <a href='https://arxiv.org/pdf/2402.02210.pdf' target='_blank'>https://arxiv.org/pdf/2402.02210.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haochen Chang, Jing Chen, Yilin Li, Jixiang Chen, Xiaofeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.02210">Wavelet-Decoupling Contrastive Enhancement Network for Fine-Grained Skeleton-Based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeleton-based action recognition has attracted much attention, benefiting from its succinctness and robustness. However, the minimal inter-class variation in similar action sequences often leads to confusion. The inherent spatiotemporal coupling characteristics make it challenging to mine the subtle differences in joint motion trajectories, which is critical for distinguishing confusing fine-grained actions. To alleviate this problem, we propose a Wavelet-Attention Decoupling (WAD) module that utilizes discrete wavelet transform to effectively disentangle salient and subtle motion features in the time-frequency domain. Then, the decoupling attention adaptively recalibrates their temporal responses. To further amplify the discrepancies in these subtle motion features, we propose a Fine-grained Contrastive Enhancement (FCE) module to enhance attention towards trajectory features by contrastive learning. Extensive experiments are conducted on the coarse-grained dataset NTU RGB+D and the fine-grained dataset FineGYM. Our methods perform competitively compared to state-of-the-art methods and can discriminate confusing fine-grained actions well.
<div id='section'>Paperid: <span id='pid'>183, <a href='https://arxiv.org/pdf/2310.06854.pdf' target='_blank'>https://arxiv.org/pdf/2310.06854.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leiyu Xie, Yang Sun, Syed Mohsen Naqvi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.06854">Learning with Noisy Labels for Human Fall Events Classification: Joint Cooperative Training with Trinity Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the increasing ageing population, fall events classification has drawn much research attention. In the development of deep learning, the quality of data labels is crucial. Most of the datasets are labelled automatically or semi-automatically, and the samples may be mislabeled, which constrains the performance of Deep Neural Networks (DNNs). Recent research on noisy label learning confirms that neural networks first focus on the clean and simple instances and then follow the noisy and hard instances in the training stage. To address the learning with noisy label problem and protect the human subjects' privacy, we propose a simple but effective approach named Joint Cooperative training with Trinity Networks (JoCoT). To mitigate the privacy issue, human skeleton data are used. The robustness and performance of the noisy label learning framework is improved by using the two teacher modules and one student module in the proposed JoCoT. To mitigate the incorrect selections, the predictions from the teacher modules are applied with the consensus-based method to guide the student module training. The performance evaluation on the widely used UP-Fall dataset and comparison with the state-of-the-art, confirms the effectiveness of the proposed JoCoT in high noise rates. Precisely, JoCoT outperforms the state-of-the-art by 5.17% and 3.35% with the averaged pairflip and symmetric noises, respectively.
<div id='section'>Paperid: <span id='pid'>184, <a href='https://arxiv.org/pdf/2307.16074.pdf' target='_blank'>https://arxiv.org/pdf/2307.16074.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zaedul Islam, A. Ben Hamza
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.16074">Iterative Graph Filtering Network for 3D Human Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph convolutional networks (GCNs) have proven to be an effective approach for 3D human pose estimation. By naturally modeling the skeleton structure of the human body as a graph, GCNs are able to capture the spatial relationships between joints and learn an efficient representation of the underlying pose. However, most GCN-based methods use a shared weight matrix, making it challenging to accurately capture the different and complex relationships between joints. In this paper, we introduce an iterative graph filtering framework for 3D human pose estimation, which aims to predict the 3D joint positions given a set of 2D joint locations in images. Our approach builds upon the idea of iteratively solving graph filtering with Laplacian regularization via the Gauss-Seidel iterative method. Motivated by this iterative solution, we design a Gauss-Seidel network (GS-Net) architecture, which makes use of weight and adjacency modulation, skip connection, and a pure convolutional block with layer normalization. Adjacency modulation facilitates the learning of edges that go beyond the inherent connections of body joints, resulting in an adjusted graph structure that reflects the human skeleton, while skip connections help maintain crucial information from the input layer's initial features as the network depth increases. We evaluate our proposed model on two standard benchmark datasets, and compare it with a comprehensive set of strong baseline methods for 3D human pose estimation. Our experimental results demonstrate that our approach outperforms the baseline methods on both datasets, achieving state-of-the-art performance. Furthermore, we conduct ablation studies to analyze the contributions of different components of our model architecture and show that the skip connection and adjacency modulation help improve the model performance.
<div id='section'>Paperid: <span id='pid'>185, <a href='https://arxiv.org/pdf/2306.15321.pdf' target='_blank'>https://arxiv.org/pdf/2306.15321.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sheng-Lan Liu, Yu-Ning Ding, Jin-Rong Zhang, Kai-Yuan Liu, Si-Fan Zhang, Fei-Long Wang, Gao Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.15321">Multi-Dimensional Refinement Graph Convolutional Network with Robust Decouple Loss for Fine-Grained Skeleton-Based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph convolutional networks have been widely used in skeleton-based action recognition. However, existing approaches are limited in fine-grained action recognition due to the similarity of inter-class data. Moreover, the noisy data from pose extraction increases the challenge of fine-grained recognition. In this work, we propose a flexible attention block called Channel-Variable Spatial-Temporal Attention (CVSTA) to enhance the discriminative power of spatial-temporal joints and obtain a more compact intra-class feature distribution. Based on CVSTA, we construct a Multi-Dimensional Refinement Graph Convolutional Network (MDR-GCN), which can improve the discrimination among channel-, joint- and frame-level features for fine-grained actions. Furthermore, we propose a Robust Decouple Loss (RDL), which significantly boosts the effect of the CVSTA and reduces the impact of noise. The proposed method combining MDR-GCN with RDL outperforms the known state-of-the-art skeleton-based approaches on fine-grained datasets, FineGym99 and FSD-10, and also on the coarse dataset NTU-RGB+D X-view version.
<div id='section'>Paperid: <span id='pid'>186, <a href='https://arxiv.org/pdf/2305.05785.pdf' target='_blank'>https://arxiv.org/pdf/2305.05785.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tanvir Hassan, A. Ben Hamza
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.05785">Regular Splitting Graph Network for 3D Human Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In human pose estimation methods based on graph convolutional architectures, the human skeleton is usually modeled as an undirected graph whose nodes are body joints and edges are connections between neighboring joints. However, most of these methods tend to focus on learning relationships between body joints of the skeleton using first-order neighbors, ignoring higher-order neighbors and hence limiting their ability to exploit relationships between distant joints. In this paper, we introduce a higher-order regular splitting graph network (RS-Net) for 2D-to-3D human pose estimation using matrix splitting in conjunction with weight and adjacency modulation. The core idea is to capture long-range dependencies between body joints using multi-hop neighborhoods and also to learn different modulation vectors for different body joints as well as a modulation matrix added to the adjacency matrix associated to the skeleton. This learnable modulation matrix helps adjust the graph structure by adding extra graph edges in an effort to learn additional connections between body joints. Instead of using a shared weight matrix for all neighboring body joints, the proposed RS-Net model applies weight unsharing before aggregating the feature vectors associated to the joints in order to capture the different relations between them. Experiments and ablations studies performed on two benchmark datasets demonstrate the effectiveness of our model, achieving superior performance over recent state-of-the-art methods for 3D human pose estimation.
<div id='section'>Paperid: <span id='pid'>187, <a href='https://arxiv.org/pdf/2304.09751.pdf' target='_blank'>https://arxiv.org/pdf/2304.09751.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yichun Li, Yi Li, Rajesh Nair, Syed Mohsen Naqvi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.09751">Skeleton-based action analysis for ADHD diagnosis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Attention Deficit Hyperactivity Disorder (ADHD) is a common neurobehavioral disorder worldwide. While extensive research has focused on machine learning methods for ADHD diagnosis, most research relies on high-cost equipment, e.g., MRI machine and EEG patch. Therefore, low-cost diagnostic methods based on the action characteristics of ADHD are desired. Skeleton-based action recognition has gained attention due to the action-focused nature and robustness. In this work, we propose a novel ADHD diagnosis system with a skeleton-based action recognition framework, utilizing a real multi-modal ADHD dataset and state-of-the-art detection algorithms. Compared to conventional methods, the proposed method shows cost-efficiency and significant performance improvement, making it more accessible for a broad range of initial ADHD diagnoses. Through the experiment results, the proposed method outperforms the conventional methods in accuracy and AUC. Meanwhile, our method is widely applicable for mass screening.
<div id='section'>Paperid: <span id='pid'>188, <a href='https://arxiv.org/pdf/2303.15270.pdf' target='_blank'>https://arxiv.org/pdf/2303.15270.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ryo Hachiuma, Fumiaki Sato, Taiki Sekii
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.15270">Unified Keypoint-based Action Recognition Framework via Structured Keypoint Pooling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper simultaneously addresses three limitations associated with conventional skeleton-based action recognition; skeleton detection and tracking errors, poor variety of the targeted actions, as well as person-wise and frame-wise action recognition. A point cloud deep-learning paradigm is introduced to the action recognition, and a unified framework along with a novel deep neural network architecture called Structured Keypoint Pooling is proposed. The proposed method sparsely aggregates keypoint features in a cascaded manner based on prior knowledge of the data structure (which is inherent in skeletons), such as the instances and frames to which each keypoint belongs, and achieves robustness against input errors. Its less constrained and tracking-free architecture enables time-series keypoints consisting of human skeletons and nonhuman object contours to be efficiently treated as an input 3D point cloud and extends the variety of the targeted action. Furthermore, we propose a Pooling-Switching Trick inspired by Structured Keypoint Pooling. This trick switches the pooling kernels between the training and inference phases to detect person-wise and frame-wise actions in a weakly supervised manner using only video-level action labels. This trick enables our training scheme to naturally introduce novel data augmentation, which mixes multiple point clouds extracted from different videos. In the experiments, we comprehensively verify the effectiveness of the proposed method against the limitations, and the method outperforms state-of-the-art skeleton-based action recognition and spatio-temporal action localization methods.
<div id='section'>Paperid: <span id='pid'>189, <a href='https://arxiv.org/pdf/2302.08689.pdf' target='_blank'>https://arxiv.org/pdf/2302.08689.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengqin Wang, Yongji Zhang, Hong Qi, Minghao Zhao, Yu Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.08689">Dynamic Spatial-temporal Hypergraph Convolutional Network for Skeleton-based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeleton-based action recognition relies on the extraction of spatial-temporal topological information. Hypergraphs can establish prior unnatural dependencies for the skeleton. However, the existing methods only focus on the construction of spatial topology and ignore the time-point dependence. This paper proposes a dynamic spatial-temporal hypergraph convolutional network (DST-HCN) to capture spatial-temporal information for skeleton-based action recognition. DST-HCN introduces a time-point hypergraph (TPH) to learn relationships at time points. With multiple spatial static hypergraphs and dynamic TPH, our network can learn more complete spatial-temporal features. In addition, we use the high-order information fusion module (HIF) to fuse spatial-temporal information synchronously. Extensive experiments on NTU RGB+D, NTU RGB+D 120, and NW-UCLA datasets show that our model achieves state-of-the-art, especially compared with hypergraph methods.
<div id='section'>Paperid: <span id='pid'>190, <a href='https://arxiv.org/pdf/2209.04288.pdf' target='_blank'>https://arxiv.org/pdf/2209.04288.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stefano Berti, Andrea Rosasco, Michele Colledanchise, Lorenzo Natale
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.04288">One-Shot Open-Set Skeleton-Based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Action recognition is a fundamental capability for humanoid robots to interact and cooperate with humans. This application requires the action recognition system to be designed so that new actions can be easily added, while unknown actions are identified and ignored. In recent years, deep-learning approaches represented the principal solution to the Action Recognition problem. However, most models often require a large dataset of manually-labeled samples. In this work we target One-Shot deep-learning models, because they can deal with just a single instance for class. Unfortunately, One-Shot models assume that, at inference time, the action to recognize falls into the support set and they fail when the action lies outside the support set. Few-Shot Open-Set Recognition (FSOSR) solutions attempt to address that flaw, but current solutions consider only static images and not sequences of images. Static images remain insufficient to discriminate actions such as sitting-down and standing-up. In this paper we propose a novel model that addresses the FSOSR problem with a One-Shot model that is augmented with a discriminator that rejects unknown actions. This model is useful for applications in humanoid robotics, because it allows to easily add new classes and determine whether an input sequence is among the ones that are known to the system. We show how to train the whole model in an end-to-end fashion and we perform quantitative and qualitative analyses. Finally, we provide real-world examples.
<div id='section'>Paperid: <span id='pid'>191, <a href='https://arxiv.org/pdf/1901.06882.pdf' target='_blank'>https://arxiv.org/pdf/1901.06882.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sunoh Kim, Kimin Yun, Jongyoul Park, Jin Young Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1901.06882">Skeleton-based Action Recognition of People Handling Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In visual surveillance systems, it is necessary to recognize the behavior of people handling objects such as a phone, a cup, or a plastic bag. In this paper, to address this problem, we propose a new framework for recognizing object-related human actions by graph convolutional networks using human and object poses. In this framework, we construct skeletal graphs of reliable human poses by selectively sampling the informative frames in a video, which include human joints with high confidence scores obtained in pose estimation. The skeletal graphs generated from the sampled frames represent human poses related to the object position in both the spatial and temporal domains, and these graphs are used as inputs to the graph convolutional networks. Through experiments over an open benchmark and our own data sets, we verify the validity of our framework in that our method outperforms the state-of-the-art method for skeleton-based action recognition.
<div id='section'>Paperid: <span id='pid'>192, <a href='https://arxiv.org/pdf/2509.09067.pdf' target='_blank'>https://arxiv.org/pdf/2509.09067.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hesham M. Shehata, Mohammad Abdolrahmani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09067">Improvement of Human-Object Interaction Action Recognition Using Scene Information and Multi-Task Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent graph convolutional neural networks (GCNs) have shown high performance in the field of human action recognition by using human skeleton poses. However, it fails to detect human-object interaction cases successfully due to the lack of effective representation of the scene information and appropriate learning architectures. In this context, we propose a methodology to utilize human action recognition performance by considering fixed object information in the environment and following a multi-task learning approach. In order to evaluate the proposed method, we collected real data from public environments and prepared our data set, which includes interaction classes of hands-on fixed objects (e.g., ATM ticketing machines, check-in/out machines, etc.) and non-interaction classes of walking and standing. The multi-task learning approach, along with interaction area information, succeeds in recognizing the studied interaction and non-interaction actions with an accuracy of 99.25%, outperforming the accuracy of the base model using only human skeleton poses by 2.75%.
<div id='section'>Paperid: <span id='pid'>193, <a href='https://arxiv.org/pdf/2507.03705.pdf' target='_blank'>https://arxiv.org/pdf/2507.03705.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Praveen Jesudhas, Raghuveera T, Shiney Jeyaraj
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03705">Computationally efficient non-Intrusive pre-impact fall detection system</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing pre-impact fall detection systems have high accuracy, however they are either intrusive to the subject or require heavy computational resources for fall detection, resulting in prohibitive deployment costs. These factors limit the global adoption of existing fall detection systems. In this work we present a Pre-impact fall detection system that is both non-intrusive and computationally efficient at deployment. Our system utilizes video data of the locality available through cameras, thereby requiring no specialized equipment to be worn by the subject. Further, the fall detection system utilizes minimal fall specific features and simplistic neural network models, designed to reduce the computational cost of the system. A minimal set of fall specific features are derived from the skeletal data, post observing the relative position of human skeleton during fall. These features are shown to have different distributions for Fall and non-fall scenarios proving their discriminative capability. A Long Short Term Memory (LSTM) based network is selected and the network architecture and training parameters are designed after evaluation of performance on standard datasets. In the Pre-impact fall detection system the computation requirement is about 18 times lesser than existing modules with a comparable accuracy of 88%. Given the low computation requirements and higher accuracy levels, the proposed system is suitable for wider adoption in engineering systems related to industrial and residential safety.
<div id='section'>Paperid: <span id='pid'>194, <a href='https://arxiv.org/pdf/2503.14960.pdf' target='_blank'>https://arxiv.org/pdf/2503.14960.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seungyeon Cho, Tae-Kyun Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14960">Body-Hand Modality Expertized Networks with Cross-attention for Fine-grained Skeleton Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeleton-based Human Action Recognition (HAR) is a vital technology in robotics and human-robot interaction. However, most existing methods concentrate primarily on full-body movements and often overlook subtle hand motions that are critical for distinguishing fine-grained actions. Recent work leverages a unified graph representation that combines body, hand, and foot keypoints to capture detailed body dynamics. Yet, these models often blur fine hand details due to the disparity between body and hand action characteristics and the loss of subtle features during the spatial-pooling. In this paper, we propose BHaRNet (Body-Hand action Recognition Network), a novel framework that augments a typical body-expert model with a hand-expert model. Our model jointly trains both streams with an ensemble loss that fosters cooperative specialization, functioning in a manner reminiscent of a Mixture-of-Experts (MoE). Moreover, cross-attention is employed via an expertized branch method and a pooling-attention module to enable feature-level interactions and selectively fuse complementary information. Inspired by MMNet, we also demonstrate the applicability of our approach to multi-modal tasks by leveraging RGB information, where body features guide RGB learning to capture richer contextual cues. Experiments on large-scale benchmarks (NTU RGB+D 60, NTU RGB+D 120, PKU-MMD, and Northwestern-UCLA) demonstrate that BHaRNet achieves SOTA accuracies -- improving from 86.4\% to 93.0\% in hand-intensive actions -- while maintaining fewer GFLOPs and parameters than the relevant unified methods.
<div id='section'>Paperid: <span id='pid'>195, <a href='https://arxiv.org/pdf/2501.18729.pdf' target='_blank'>https://arxiv.org/pdf/2501.18729.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anthony Richardson, Felix Putze
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.18729">Motion Diffusion Autoencoders: Enabling Attribute Manipulation in Human Motion Demonstrated on Karate Techniques</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Attribute manipulation deals with the problem of changing individual attributes of a data point or a time series, while leaving all other aspects unaffected. This work focuses on the domain of human motion, more precisely karate movement patterns. To the best of our knowledge, it presents the first success at manipulating attributes of human motion data. One of the key requirements for achieving attribute manipulation on human motion is a suitable pose representation. Therefore, we design a novel continuous, rotation-based pose representation that enables the disentanglement of the human skeleton and the motion trajectory, while still allowing an accurate reconstruction of the original anatomy. The core idea of the manipulation approach is to use a transformer encoder for discovering high-level semantics, and a diffusion probabilistic model for modeling the remaining stochastic variations. We show that the embedding space obtained from the transformer encoder is semantically meaningful and linear. This enables the manipulation of high-level attributes, by discovering their linear direction of change in the semantic embedding space and moving the embedding along said direction. All code and data is made publicly available.
<div id='section'>Paperid: <span id='pid'>196, <a href='https://arxiv.org/pdf/2501.02593.pdf' target='_blank'>https://arxiv.org/pdf/2501.02593.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jushang Qiu, Lei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.02593">Evolving Skeletons: Motion Dynamics in Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeleton-based action recognition has gained significant attention for its ability to efficiently represent spatiotemporal information in a lightweight format. Most existing approaches use graph-based models to process skeleton sequences, where each pose is represented as a skeletal graph structured around human physical connectivity. Among these, the Spatiotemporal Graph Convolutional Network (ST-GCN) has become a widely used framework. Alternatively, hypergraph-based models, such as the Hyperformer, capture higher-order correlations, offering a more expressive representation of complex joint interactions. A recent advancement, termed Taylor Videos, introduces motion-enhanced skeleton sequences by embedding motion concepts, providing a fresh perspective on interpreting human actions in skeleton-based action recognition. In this paper, we conduct a comprehensive evaluation of both traditional skeleton sequences and Taylor-transformed skeletons using ST-GCN and Hyperformer models on the NTU-60 and NTU-120 datasets. We compare skeletal graph and hypergraph representations, analyzing static poses against motion-injected poses. Our findings highlight the strengths and limitations of Taylor-transformed skeletons, demonstrating their potential to enhance motion dynamics while exposing current challenges in fully using their benefits. This study underscores the need for innovative skeletal modelling techniques to effectively handle motion-rich data and advance the field of action recognition.
<div id='section'>Paperid: <span id='pid'>197, <a href='https://arxiv.org/pdf/2412.05386.pdf' target='_blank'>https://arxiv.org/pdf/2412.05386.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Himanshu Mittal, Suvramalya Basak, Anjali Gautam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05386">DIFEM: Key-points Interaction based Feature Extraction Module for Violence Recognition in Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Violence detection in surveillance videos is a critical task for ensuring public safety. As a result, there is increasing need for efficient and lightweight systems for automatic detection of violent behaviours. In this work, we propose an effective method which leverages human skeleton key-points to capture inherent properties of violence, such as rapid movement of specific joints and their close proximity. At the heart of our method is our novel Dynamic Interaction Feature Extraction Module (DIFEM) which captures features such as velocity, and joint intersections, effectively capturing the dynamics of violent behavior. With the features extracted by our DIFEM, we use various classification algorithms such as Random Forest, Decision tree, AdaBoost and k-Nearest Neighbor. Our approach has substantially lesser amount of parameter expense than the existing state-of-the-art (SOTA) methods employing deep learning techniques. We perform extensive experiments on three standard violence recognition datasets, showing promising performance in all three datasets. Our proposed method surpasses several SOTA violence recognition methods.
<div id='section'>Paperid: <span id='pid'>198, <a href='https://arxiv.org/pdf/2411.01769.pdf' target='_blank'>https://arxiv.org/pdf/2411.01769.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuanchuan Wang, Ahmad Sufril Azlan Mohmamed, Mohd Halim Bin Mohd Noor, Xiao Yang, Feifan Yi, Xiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.01769">ARN-LSTM: A Multi-Stream Fusion Model for Skeleton-based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents the ARN-LSTM architecture, a novel multi-stream action recognition model designed to address the challenge of simultaneously capturing spatial motion and temporal dynamics in action sequences. Traditional methods often focus solely on spatial or temporal features, limiting their ability to comprehend complex human activities fully. Our proposed model integrates joint, motion, and temporal information through a multi-stream fusion architecture. Specifically, it comprises a jointstream for extracting skeleton features, a temporal stream for capturing dynamic temporal features, and an ARN-LSTM block that utilizes Time-Distributed Long Short-Term Memory (TD-LSTM) layers followed by an Attention Relation Network (ARN) to model temporal relations. The outputs from these streams are fused in a fully connected layer to provide the final action prediction. Evaluations on the NTU RGB+D 60 and NTU RGB+D 120 datasets outperform the superior performance of our model, particularly in group activity recognition.
<div id='section'>Paperid: <span id='pid'>199, <a href='https://arxiv.org/pdf/2409.11689.pdf' target='_blank'>https://arxiv.org/pdf/2409.11689.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuowen Liang, Sisi Li, Qingyun Wang, Cen Zhang, Kaiquan Zhu, Tian Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11689">GUNet: A Graph Convolutional Network United Diffusion Model for Stable and Diversity Pose Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pose skeleton images are an important reference in pose-controllable image generation. In order to enrich the source of skeleton images, recent works have investigated the generation of pose skeletons based on natural language. These methods are based on GANs. However, it remains challenging to perform diverse, structurally correct and aesthetically pleasing human pose skeleton generation with various textual inputs. To address this problem, we propose a framework with GUNet as the main model, PoseDiffusion. It is the first generative framework based on a diffusion model and also contains a series of variants fine-tuned based on a stable diffusion model. PoseDiffusion demonstrates several desired properties that outperform existing methods. 1) Correct Skeletons. GUNet, a denoising model of PoseDiffusion, is designed to incorporate graphical convolutional neural networks. It is able to learn the spatial relationships of the human skeleton by introducing skeletal information during the training process. 2) Diversity. We decouple the key points of the skeleton and characterise them separately, and use cross-attention to introduce textual conditions. Experimental results show that PoseDiffusion outperforms existing SoTA algorithms in terms of stability and diversity of text-driven pose skeleton generation. Qualitative analyses further demonstrate its superiority for controllable generation in Stable Diffusion.
<div id='section'>Paperid: <span id='pid'>200, <a href='https://arxiv.org/pdf/2407.21525.pdf' target='_blank'>https://arxiv.org/pdf/2407.21525.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingyao Wang, Emmanuel Bergeret, Issam Falih
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.21525">Skeleton-Based Action Recognition with Spatial-Structural Graph Convolution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human Activity Recognition (HAR) is a field of study that focuses on identifying and classifying human activities. Skeleton-based Human Activity Recognition has received much attention in recent years, where Graph Convolutional Network (GCN) based method is widely used and has achieved remarkable results. However, the representation of skeleton data and the issue of over-smoothing in GCN still need to be studied. 1). Compared to central nodes, edge nodes can only aggregate limited neighbor information, and different edge nodes of the human body are always structurally related. However, the information from edge nodes is crucial for fine-grained activity recognition. 2). The Graph Convolutional Network suffers from a significant over-smoothing issue, causing nodes to become increasingly similar as the number of network layers increases. Based on these two ideas, we propose a two-stream graph convolution method called Spatial-Structural GCN (SpSt-GCN). Spatial GCN performs information aggregation based on the topological structure of the human body, and structural GCN performs differentiation based on the similarity of edge node sequences. The spatial connection is fixed, and the human skeleton naturally maintains this topology regardless of the actions performed by humans. However, the structural connection is dynamic and depends on the type of movement the human body is performing. Based on this idea, we also propose an entirely data-driven structural connection, which greatly increases flexibility. We evaluate our method on two large-scale datasets, i.e., NTU RGB+D and NTU RGB+D 120. The proposed method achieves good results while being efficient.
<div id='section'>Paperid: <span id='pid'>201, <a href='https://arxiv.org/pdf/2407.14224.pdf' target='_blank'>https://arxiv.org/pdf/2407.14224.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Suvajit Patra, Arkadip Maitra, Megha Tiwari, K. Kumaran, Swathy Prabhu, Swami Punyeshwarananda, Soumitra Samanta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.14224">Hierarchical Windowed Graph Attention Network and a Large Scale Dataset for Isolated Indian Sign Language Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic Sign Language (SL) recognition is an important task in the computer vision community. To build a robust SL recognition system, we need a considerable amount of data which is lacking particularly in Indian sign language (ISL). In this paper, we introduce a large-scale isolated ISL dataset and a novel SL recognition model based on skeleton graph structure. The dataset covers 2002 daily used common words in the deaf community recorded by 20 (10 male and 10 female) deaf adult signers (contains 40033 videos). We propose a SL recognition model namely Hierarchical Windowed Graph Attention Network (HWGAT) by utilizing the human upper body skeleton graph. The HWGAT tries to capture distinctive motions by giving attention to different body parts induced by the human skeleton graph. The utility of the proposed dataset and the usefulness of our model are evaluated through extensive experiments. We pre-trained the proposed model on the presented dataset and fine-tuned it across different sign language datasets further boosting the performance of 1.10, 0.46, 0.78, and 6.84 percentage points on INCLUDE, LSA64, AUTSL and WLASL respectively compared to the existing state-of-the-art keypoints-based models.
<div id='section'>Paperid: <span id='pid'>202, <a href='https://arxiv.org/pdf/2407.03817.pdf' target='_blank'>https://arxiv.org/pdf/2407.03817.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ana Filipa Rodrigues Nogueira, HÃ©lder P. Oliveira, LuÃ­s F. Teixeira
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.03817">Markerless Multi-view 3D Human Pose Estimation: a survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D human pose estimation involves reconstructing the human skeleton by detecting the body joints. Accurate and efficient solutions are required for several real-world applications including animation, human-robot interaction, surveillance, and sports. However, challenges such as occlusions, 2D pose mismatches, random camera perspectives, and limited 3D labelled data have been hampering the models' performance and limiting their deployment in real-world scenarios. The higher availability of cameras has led researchers to explore multi-view solutions to take advantage of the different perspectives to reconstruct the pose.
  Most existing reviews have mainly focused on monocular 3D human pose estimation, so a comprehensive survey on multi-view approaches has been missing since 2012. According to the reviewed articles, the majority of the existing methods are fully-supervised approaches based on geometric constraints, which are often limited by 2D pose mismatches. To mitigate this, researchers have proposed incorporating temporal consistency or depth information. Alternatively, working directly with 3D features has been shown to completely overcome this issue, albeit at the cost of increased computational complexity. Additionally, models with lower levels of supervision have been identified to help address challenges such as annotated data scarcity and generalisation to new setups. Therefore, no method currently addresses all challenges associated with 3D pose reconstruction, and a trade-off between complexity and performance exists. Further research is needed to develop approaches capable of quickly inferring a highly accurate 3D pose with bearable computation cost. Techniques such as active learning, low-supervision methods, temporal consistency, view selection, depth information estimation, and multi-modal approaches are strategies to consider when developing a new method for this task.
<div id='section'>Paperid: <span id='pid'>203, <a href='https://arxiv.org/pdf/2401.17086.pdf' target='_blank'>https://arxiv.org/pdf/2401.17086.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Long Liu, Xin Wang, Fangming Li, Jiayu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.17086">Active Generation Network of Human Skeleton for Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data generation is a data augmentation technique for enhancing the generalization ability for skeleton-based human action recognition. Most existing data generation methods face challenges to ensure the temporal consistency of the dynamic information for action. In addition, the data generated by these methods lack diversity when only a few training samples are available. To solve those problems, We propose a novel active generative network (AGN), which can adaptively learn various action categories by motion style transfer to generate new actions when the data for a particular action is only a single sample or few samples. The AGN consists of an action generation network and an uncertainty metric network. The former, with ST-GCN as the Backbone, can implicitly learn the morphological features of the target action while preserving the category features of the source action. The latter guides generating actions. Specifically, an action recognition model generates prediction vectors for each action, which is then scored using an uncertainty metric. Finally, UMN provides the uncertainty sampling basis for the generated actions.
<div id='section'>Paperid: <span id='pid'>204, <a href='https://arxiv.org/pdf/2310.13039.pdf' target='_blank'>https://arxiv.org/pdf/2310.13039.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lijuan Zhou, Xiang Meng, Zhihuan Liu, Mengqi Wu, Zhimin Gao, Pichao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.13039">Human Pose-based Estimation, Tracking and Action Recognition with Deep Learning: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human pose analysis has garnered significant attention within both the research community and practical applications, owing to its expanding array of uses, including gaming, video surveillance, sports performance analysis, and human-computer interactions, among others. The advent of deep learning has significantly improved the accuracy of pose capture, making pose-based applications increasingly practical. This paper presents a comprehensive survey of pose-based applications utilizing deep learning, encompassing pose estimation, pose tracking, and action recognition.Pose estimation involves the determination of human joint positions from images or image sequences. Pose tracking is an emerging research direction aimed at generating consistent human pose trajectories over time. Action recognition, on the other hand, targets the identification of action types using pose estimation or tracking data. These three tasks are intricately interconnected, with the latter often reliant on the former. In this survey, we comprehensively review related works, spanning from single-person pose estimation to multi-person pose estimation, from 2D pose estimation to 3D pose estimation, from single image to video, from mining temporal context gradually to pose tracking, and lastly from tracking to pose-based action recognition. As a survey centered on the application of deep learning to pose analysis, we explicitly discuss both the strengths and limitations of existing techniques. Notably, we emphasize methodologies for integrating these three tasks into a unified framework within video sequences. Additionally, we explore the challenges involved and outline potential directions for future research.
<div id='section'>Paperid: <span id='pid'>205, <a href='https://arxiv.org/pdf/2310.08451.pdf' target='_blank'>https://arxiv.org/pdf/2310.08451.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marlin Berger, Frederik Cloppenburg, Jens Eufinger, Thomas Gries
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.08451">Proving the Potential of Skeleton Based Action Recognition to Automate the Analysis of Manual Processes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In manufacturing sectors such as textiles and electronics, manual processes are a fundamental part of production. The analysis and monitoring of the processes is necessary for efficient production design. Traditional methods for analyzing manual processes are complex, expensive, and inflexible. Compared to established approaches such as Methods-Time-Measurement (MTM), machine learning (ML) methods promise: Higher flexibility, self-sufficient & permanent use, lower costs. In this work, based on a video stream, the current motion class in a manual assembly process is detected. With information on the current motion, Key-Performance-Indicators (KPIs) can be derived easily. A skeleton-based action recognition approach is taken, as this field recently shows major success in machine vision tasks. For skeleton-based action recognition in manual assembly, no sufficient pre-work could be found. Therefore, a ML pipeline is developed, to enable extensive research on different (pre-) processing methods and neural nets. Suitable well generalizing approaches are found, proving the potential of ML to enhance analyzation of manual processes. Models detect the current motion, performed by an operator in manual assembly, but the results can be transferred to all kinds of manual processes.
<div id='section'>Paperid: <span id='pid'>206, <a href='https://arxiv.org/pdf/2304.11631.pdf' target='_blank'>https://arxiv.org/pdf/2304.11631.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongjingdin Liu, Pengpeng Chen, Miao Yao, Yijing Lu, Zijie Cai, Yuxin Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.11631">TSGCNeXt: Dynamic-Static Multi-Graph Convolution for Efficient Skeleton-Based Action Recognition with Long-term Learning Potential</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeleton-based action recognition has achieved remarkable results in human action recognition with the development of graph convolutional networks (GCNs). However, the recent works tend to construct complex learning mechanisms with redundant training and exist a bottleneck for long time-series. To solve these problems, we propose the Temporal-Spatio Graph ConvNeXt (TSGCNeXt) to explore efficient learning mechanism of long temporal skeleton sequences. Firstly, a new graph learning mechanism with simple structure, Dynamic-Static Separate Multi-graph Convolution (DS-SMG) is proposed to aggregate features of multiple independent topological graphs and avoid the node information being ignored during dynamic convolution. Next, we construct a graph convolution training acceleration mechanism to optimize the back-propagation computing of dynamic graph learning with 55.08\% speed-up. Finally, the TSGCNeXt restructure the overall structure of GCN with three Spatio-temporal learning modules,efficiently modeling long temporal features. In comparison with existing previous methods on large-scale datasets NTU RGB+D 60 and 120, TSGCNeXt outperforms on single-stream networks. In addition, with the ema model introduced into the multi-stream fusion, TSGCNeXt achieves SOTA levels. On the cross-subject and cross-set of the NTU 120, accuracies reach 90.22% and 91.74%.
<div id='section'>Paperid: <span id='pid'>207, <a href='https://arxiv.org/pdf/2302.13434.pdf' target='_blank'>https://arxiv.org/pdf/2302.13434.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Jiang, Han Chen, Hanseok Ko
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.13434">Spatial-temporal Transformer-guided Diffusion based Data Augmentation for Efficient Skeleton-based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, skeleton-based human action has become a hot research topic because the compact representation of human skeletons brings new blood to this research domain. As a result, researchers began to notice the importance of using RGB or other sensors to analyze human action by extracting skeleton information. Leveraging the rapid development of deep learning (DL), a significant number of skeleton-based human action approaches have been presented with fine-designed DL structures recently. However, a well-trained DL model always demands high-quality and sufficient data, which is hard to obtain without costing high expenses and human labor. In this paper, we introduce a novel data augmentation method for skeleton-based action recognition tasks, which can effectively generate high-quality and diverse sequential actions. In order to obtain natural and realistic action sequences, we propose denoising diffusion probabilistic models (DDPMs) that can generate a series of synthetic action sequences, and their generation process is precisely guided by a spatial-temporal transformer (ST-Trans). Experimental results show that our method outperforms the state-of-the-art (SOTA) motion generation approaches on different naturality and diversity metrics. It proves that its high-quality synthetic data can also be effectively deployed to existing action recognition models with significant performance improvement.
<div id='section'>Paperid: <span id='pid'>208, <a href='https://arxiv.org/pdf/2302.12007.pdf' target='_blank'>https://arxiv.org/pdf/2302.12007.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shannan Guan, Xin Yu, Wei Huang, Gengfa Fang, Haiyan Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.12007">DMMG: Dual Min-Max Games for Self-Supervised Skeleton-Based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we propose a new Dual Min-Max Games (DMMG) based self-supervised skeleton action recognition method by augmenting unlabeled data in a contrastive learning framework. Our DMMG consists of a viewpoint variation min-max game and an edge perturbation min-max game. These two min-max games adopt an adversarial paradigm to perform data augmentation on the skeleton sequences and graph-structured body joints, respectively. Our viewpoint variation min-max game focuses on constructing various hard contrastive pairs by generating skeleton sequences from various viewpoints. These hard contrastive pairs help our model learn representative action features, thus facilitating model transfer to downstream tasks. Moreover, our edge perturbation min-max game specializes in building diverse hard contrastive samples through perturbing connectivity strength among graph-based body joints. The connectivity-strength varying contrastive pairs enable the model to capture minimal sufficient information of different actions, such as representative gestures for an action while preventing the model from overfitting. By fully exploiting the proposed DMMG, we can generate sufficient challenging contrastive pairs and thus achieve discriminative action feature representations from unlabeled skeleton data in a self-supervised manner. Extensive experiments demonstrate that our method achieves superior results under various evaluation protocols on widely-used NTU-RGB+D and NTU120-RGB+D datasets.
<div id='section'>Paperid: <span id='pid'>209, <a href='https://arxiv.org/pdf/2301.13090.pdf' target='_blank'>https://arxiv.org/pdf/2301.13090.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Farajzadeh Bavil, Hamed Damirchi, Hamid D. Taghirad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.13090">Action Capsules: Human Skeleton Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to the compact and rich high-level representations offered, skeleton-based human action recognition has recently become a highly active research topic. Previous studies have demonstrated that investigating joint relationships in spatial and temporal dimensions provides effective information critical to action recognition. However, effectively encoding global dependencies of joints during spatio-temporal feature extraction is still challenging. In this paper, we introduce Action Capsule which identifies action-related key joints by considering the latent correlation of joints in a skeleton sequence. We show that, during inference, our end-to-end network pays attention to a set of joints specific to each action, whose encoded spatio-temporal features are aggregated to recognize the action. Additionally, the use of multiple stages of action capsules enhances the ability of the network to classify similar actions. Consequently, our network outperforms the state-of-the-art approaches on the N-UCLA dataset and obtains competitive results on the NTURGBD dataset. This is while our approach has significantly lower computational requirements based on GFLOPs measurements.
<div id='section'>Paperid: <span id='pid'>210, <a href='https://arxiv.org/pdf/2110.13385.pdf' target='_blank'>https://arxiv.org/pdf/2110.13385.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingtian Wang, Jianlin Peng, Shuze Shi, Tingxi Liu, Jiabin He, Renliang Weng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2110.13385">IIP-Transformer: Intra-Inter-Part Transformer for Skeleton-Based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, Transformer-based networks have shown great promise on skeleton-based action recognition tasks. The ability to capture global and local dependencies is the key to success while it also brings quadratic computation and memory cost. Another problem is that previous studies mainly focus on the relationships among individual joints, which often suffers from the noisy skeleton joints introduced by the noisy inputs of sensors or inaccurate estimations. To address the above issues, we propose a novel Transformer-based network (IIP-Transformer). Instead of exploiting interactions among individual joints, our IIP-Transformer incorporates body joints and parts interactions simultaneously and thus can capture both joint-level (intra-part) and part-level (inter-part) dependencies efficiently and effectively. From the data aspect, we introduce a part-level skeleton data encoding that significantly reduces the computational complexity and is more robust to joint-level skeleton noise. Besides, a new part-level data augmentation is proposed to improve the performance of the model. On two large-scale datasets, NTU-RGB+D 60 and NTU RGB+D 120, the proposed IIP-Transformer achieves the-state-of-art performance with more than 8x less computational complexity than DSTA-Net, which is the SOTA Transformer-based method.
<div id='section'>Paperid: <span id='pid'>211, <a href='https://arxiv.org/pdf/2510.05506.pdf' target='_blank'>https://arxiv.org/pdf/2510.05506.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>James Dickens
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05506">Human Action Recognition from Point Clouds over Time</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent research into human action recognition (HAR) has focused predominantly on skeletal action recognition and video-based methods. With the increasing availability of consumer-grade depth sensors and Lidar instruments, there is a growing opportunity to leverage dense 3D data for action recognition, to develop a third way. This paper presents a novel approach for recognizing actions from 3D videos by introducing a pipeline that segments human point clouds from the background of a scene, tracks individuals over time, and performs body part segmentation. The method supports point clouds from both depth sensors and monocular depth estimation. At the core of the proposed HAR framework is a novel backbone for 3D action recognition, which combines point-based techniques with sparse convolutional networks applied to voxel-mapped point cloud sequences. Experiments incorporate auxiliary point features including surface normals, color, infrared intensity, and body part parsing labels, to enhance recognition accuracy. Evaluation on the NTU RGB- D 120 dataset demonstrates that the method is competitive with existing skeletal action recognition algorithms. Moreover, combining both sensor-based and estimated depth inputs in an ensemble setup, this approach achieves 89.3% accuracy when different human subjects are considered for training and testing, outperforming previous point cloud action recognition methods.
<div id='section'>Paperid: <span id='pid'>212, <a href='https://arxiv.org/pdf/2502.21085.pdf' target='_blank'>https://arxiv.org/pdf/2502.21085.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jing-Yuan Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.21085">BST: Badminton Stroke-type Transformer for Skeleton-based Action Recognition in Racket Sports</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Badminton, known for having the fastest ball speeds among all sports, presents significant challenges to the field of computer vision, including player identification, court line detection, shuttlecock trajectory tracking, and player stroke-type classification. In this paper, we introduce a novel video clipping strategy to extract frames of each player's racket swing in a badminton broadcast match. These clipped frames are then processed by three existing models: one for Human Pose Estimation to obtain human skeletal joints, another for shuttlecock trajectory tracking, and the other for court line detection to determine player positions on the court. Leveraging these data as inputs, we propose Badminton Stroke-type Transformer (BST) to classify player stroke-types in singles. To the best of our knowledge, experimental results demonstrate that our method outperforms the previous state-of-the-art on the largest publicly available badminton video dataset (ShuttleSet), another badminton dataset (BadmintonDB), and a tennis dataset (TenniSet). These results suggest that effectively leveraging ball trajectory is a promising direction for action recognition in racket sports.
<div id='section'>Paperid: <span id='pid'>213, <a href='https://arxiv.org/pdf/2412.18780.pdf' target='_blank'>https://arxiv.org/pdf/2412.18780.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuheng Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.18780">Skeleton-based Action Recognition with Non-linear Dependency Modeling and Hilbert-Schmidt Independence Criterion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human skeleton-based action recognition has long been an indispensable aspect of artificial intelligence. Current state-of-the-art methods tend to consider only the dependencies between connected skeletal joints, limiting their ability to capture non-linear dependencies between physically distant joints. Moreover, most existing approaches distinguish action classes by estimating the probability density of motion representations, yet the high-dimensional nature of human motions invokes inherent difficulties in accomplishing such measurements. In this paper, we seek to tackle these challenges from two directions: (1) We propose a novel dependency refinement approach that explicitly models dependencies between any pair of joints, effectively transcending the limitations imposed by joint distance. (2) We further propose a framework that utilizes the Hilbert-Schmidt Independence Criterion to differentiate action classes without being affected by data dimensionality, and mathematically derive learning objectives guaranteeing precise recognition. Empirically, our approach sets the state-of-the-art performance on NTU RGB+D, NTU RGB+D 120, and Northwestern-UCLA datasets.
<div id='section'>Paperid: <span id='pid'>214, <a href='https://arxiv.org/pdf/2404.02624.pdf' target='_blank'>https://arxiv.org/pdf/2404.02624.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ikuo Nakamura
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.02624">Multi-Scale Spatial-Temporal Self-Attention Graph Convolutional Networks for Skeleton-based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeleton-based gesture recognition methods have achieved high success using Graph Convolutional Network (GCN). In addition, context-dependent adaptive topology as a neighborhood vertex information and attention mechanism leverages a model to better represent actions. In this paper, we propose self-attention GCN hybrid model, Multi-Scale Spatial-Temporal self-attention (MSST)-GCN to effectively improve modeling ability to achieve state-of-the-art results on several datasets. We utilize spatial self-attention module with adaptive topology to understand intra-frame interactions within a frame among different body parts, and temporal self-attention module to examine correlations between frames of a node. These two are followed by multi-scale convolution network with dilations, which not only captures the long-range temporal dependencies of joints but also the long-range spatial dependencies (i.e., long-distance dependencies) of node temporal behaviors. They are combined into high-level spatial-temporal representations and output the predicted action with the softmax classifier.
<div id='section'>Paperid: <span id='pid'>215, <a href='https://arxiv.org/pdf/2311.08094.pdf' target='_blank'>https://arxiv.org/pdf/2311.08094.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ozge Oztimur Karadag
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.08094">SkelVIT: Consensus of Vision Transformers for a Lightweight Skeleton-Based Action Recognition System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeleton-based action recognition receives the attention of many researchers as it is robust to viewpoint and illumination changes, and its processing is much more efficient than the processing of video frames. With the emergence of deep learning models, it has become very popular to represent the skeleton data in pseudo-image form and apply CNN for action recognition. Thereafter, studies concentrated on finding effective methods for forming pseudo-images. Recently, attention networks, more specifically transformers have provided promising results in various vision problems. In this study, the effectiveness of VIT for skeleton-based action recognition is examined and its robustness on the pseudo-image representation scheme is investigated. To this end, a three-level architecture, SkelVit is proposed, which forms a set of pseudo images, applies a classifier on each of the representations, and combines their results to find the final action class. The performance of SkelVit is examined thoroughly via a set of experiments. First, the sensitivity of the system to representation is investigated by comparing it with two of the state-of-the-art pseudo-image representation methods. Then, the classifiers of SkelVit are realized in two experimental setups by CNNs and VITs, and their performances are compared. In the final experimental setup, the contribution of combining classifiers is examined by applying the model with a different number of classifiers. Experimental studies reveal that the proposed system with its lightweight representation scheme achieves better results than the state-of-the-art methods. It is also observed that the vision transformer is less sensitive to the initial pseudo-image representation compared to CNN. Nevertheless, even with the vision transformer, the recognition performance can be further improved by the consensus of classifiers.
<div id='section'>Paperid: <span id='pid'>216, <a href='https://arxiv.org/pdf/2306.17590.pdf' target='_blank'>https://arxiv.org/pdf/2306.17590.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hichem Sahbi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.17590">Miniaturized Graph Convolutional Networks with Topologically Consistent Pruning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Magnitude pruning is one of the mainstream methods in lightweight architecture design whose goal is to extract subnetworks with the largest weight connections. This method is known to be successful, but under very high pruning regimes, it suffers from topological inconsistency which renders the extracted subnetworks disconnected, and this hinders their generalization ability. In this paper, we devise a novel magnitude pruning method that allows extracting subnetworks while guarantying their topological consistency. The latter ensures that only accessible and co-accessible -- impactful -- connections are kept in the resulting lightweight networks. Our solution is based on a novel reparametrization and two supervisory bi-directional networks which implement accessibility/co-accessibility and guarantee that only connected subnetworks will be selected during training. This solution allows enhancing generalization significantly, under very high pruning regimes, as corroborated through extensive experiments, involving graph convolutional networks, on the challenging task of skeleton-based action recognition.
